{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 17 13:00:20 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   37C    P8     7W / 180W |      1MiB / 16278MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import functools\n",
    "import collections\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import torchvision\n",
    "from torchtext.data import Example, Field, Dataset\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Number of CPUs: 8\n"
     ]
    }
   ],
   "source": [
    "SEED = 781\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "N_WORKERS = multiprocessing.cpu_count()\n",
    "print(f'Number of CPUs: {N_WORKERS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 67 µs, sys: 35 µs, total: 102 µs\n",
      "Wall time: 83.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists('./data'):\n",
    "    !mkdir ./data\n",
    "    \n",
    "    !wget --no-check-certificate \\\n",
    "        https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip \\\n",
    "        -O ./data/Flickr8k_Dataset.zip\n",
    "    !unzip -q ./data/Flickr8k_Dataset.zip -d ./data\n",
    "    !rm -r ./data/Flickr8k_Dataset.zip\n",
    "\n",
    "    !wget --no-check-certificate \\\n",
    "        https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip \\\n",
    "        -O ./data/Flickr8k_text.zip\n",
    "    !unzip -q ./data/Flickr8k_text.zip -d ./data\n",
    "    !rm -r ./data/Flickr8k_text.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use this corpus / data:\n",
      "\n",
      "Please cite: M. Hodosh, P. Young and J. Hockenmaier (2013) \"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics\", Journal of Artifical Intellegence Research, Volume 47, pages 853-899\n",
      "http://www.jair.org/papers/paper3994.html\n",
      "\n",
      "\n",
      "Captions, Dataset Splits, and Human Annotations :\n",
      "\n",
      "\n",
      "Flickr8k.token.txt - the raw captions of the Flickr8k Dataset . The first column is the ID of the caption which is \"image address # caption number\"\n",
      "\n",
      "Flickr8k.lemma.txt - the lemmatized version of the above captions \n",
      "\n",
      "Flickr_8k.trainImages.txt - The training images used in our experiments\n",
      "Flickr_8k.devImages.txt - The development/validation images used in our experiments\n",
      "Flickr_8k.testImages.txt - The test images used in our experiments\n",
      "\n",
      "\n",
      "ExpertAnnotations.txt is the expert judgments.  The first two columns are the image and caption IDs.  Caption IDs are <image file name>#<0-4>.  The next three columns are the expert judgments for that image-caption pair.  Scores range from 1 to 4, with a 1 indicating that the caption does not describe the image at all, a 2 indicating the caption describes minor aspects of the image but does not describe the image, a 3 indicating that the caption almost describes the image with minor mistakes, and a 4 indicating that the caption describes the image.\n",
      "\n",
      "\n",
      "CrowdFlowerAnnotations.txt contains the CrowdFlower judgments.  The first two columns are the image and caption IDs.  The third column is the percent of Yeses, the fourth column is the total number of Yeses, the fifth column is the total number of Noes.  A Yes means that the caption describes the image (possibly with minor mistakes), while a No means that the caption does not describe the image.  Each image-caption pair has a minimum of three judgments, but some may have more.\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 8,092\n",
      "Number of train images: 6,000\n",
      "Number of valid images: 1,000\n",
      "Number of test images: 1,000\n"
     ]
    }
   ],
   "source": [
    "train_img_fn = [*map(str.strip, open('./data/Flickr_8k.trainImages.txt').readlines())]\n",
    "valid_img_fn = [*map(str.strip, open('./data/Flickr_8k.devImages.txt').readlines())]\n",
    "test_img_fn = [*map(str.strip, open('./data/Flickr_8k.testImages.txt').readlines())]\n",
    "\n",
    "img_captions = collections.defaultdict(lambda: [])\n",
    "with open('./data/Flickr8k.token.txt') as file:\n",
    "    for line in file.readlines():\n",
    "        img_fn, caption = line.strip().split('\\t')\n",
    "        img_captions[img_fn[:-2]].append(caption)\n",
    "        \n",
    "train_img_captions = dict(filter(lambda x: x[0] in train_img_fn, img_captions.items()))\n",
    "valid_img_captions = dict(filter(lambda x: x[0] in valid_img_fn, img_captions.items()))\n",
    "test_img_captions = dict(filter(lambda x: x[0] in test_img_fn, img_captions.items()))\n",
    "    \n",
    "print(f'Number of images: {len(img_captions):,}')\n",
    "print(f'Number of train images: {len(train_img_captions):,}')\n",
    "print(f'Number of valid images: {len(valid_img_captions):,}')\n",
    "print(f'Number of test images: {len(test_img_captions):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = '<pad>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "\n",
    "def clean(caption):\n",
    "    # Remove non-alphabetical character\n",
    "    caption = re.sub(r'[^a-zA-Z]', r' ', caption)\n",
    "    # Remove one word character\n",
    "    caption = re.sub(r'\\b[a-zA-Z]\\b', r' ', caption)\n",
    "    # Remove multiple spaces\n",
    "    caption = re.sub(r'\\s+', r' ', caption)\n",
    "    return caption.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train image caption length: 33\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), train_img_captions.items()))\n",
    "valid_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), valid_img_captions.items()))\n",
    "test_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), test_img_captions.items()))\n",
    "\n",
    "all_train_captions = [*functools.reduce(lambda x, y: x + y, train_img_captions.values())]\n",
    "print('Max train image caption length:', max(map(len, map(str.split, all_train_captions))))\n",
    "plt.hist([*map(len, map(str.split, all_train_captions))])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:01<00:00, 17951.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary: 2,527\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 5\n",
    "MAX_LEN = 25\n",
    "all_train_captions = [*functools.reduce(lambda x, y: x + y, train_img_captions.values())]\n",
    "EN = Field(init_token=SOS_TOKEN,\n",
    "           eos_token=EOS_TOKEN,\n",
    "           fix_length=MAX_LEN,\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='en',\n",
    "           include_lengths=True)\n",
    "examples = [Example.fromlist(data=[caption], fields=[('caption', EN)])\n",
    "            for caption in tqdm.tqdm(all_train_captions)]\n",
    "captions_data = Dataset(examples, fields={'caption': EN})\n",
    "EN.build_vocab(captions_data,\n",
    "               min_freq=MIN_COUNT,\n",
    "               specials=[SOS_TOKEN, UNK_TOKEN, EOS_TOKEN, PAD_TOKEN])\n",
    "print(f'Length of vocabulary: {len(EN.vocab):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, img_captions, split_set, img_transform, caption_transform, img_shape=(256, 256)):\n",
    "        assert split_set in {'TRAIN', 'VALID', 'TEST'}\n",
    "        self.data_path = data_path\n",
    "        self.img_captions = img_captions\n",
    "        self.split_set = split_set\n",
    "        self.img_transform = img_transform\n",
    "        self.caption_transform = caption_transform\n",
    "        self.img_shape = img_shape\n",
    "        self.ids = list(sorted(self.img_captions.keys()))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "\n",
    "        img = Image.open(os.path.join(self.data_path, img_id)).convert('RGB')\n",
    "        img = self.img_transform(img.resize(self.img_shape))\n",
    "\n",
    "        targets = self.img_captions[img_id]\n",
    "        targets = self.caption_transform(targets)\n",
    "        \n",
    "        idx = np.random.randint(len(targets))\n",
    "        target = (targets[0][:, idx], targets[1][idx])\n",
    "        \n",
    "        if self.split_set is 'TRAIN':\n",
    "            return img, target\n",
    "        else:\n",
    "            return img, target, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Lambda(lambda x: x / 255.),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def caption_transform(caption):\n",
    "    if isinstance(caption, str):\n",
    "        return EN.process([EN.preprocess(caption)])\n",
    "    elif isinstance(caption, list):\n",
    "        return EN.process([*map(EN.preprocess, caption)])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "train_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
    "                                    img_captions=train_img_captions,\n",
    "                                    split_set='TRAIN',\n",
    "                                    img_transform=img_transform,\n",
    "                                    caption_transform=caption_transform)\n",
    "valid_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
    "                                    img_captions=valid_img_captions,\n",
    "                                    split_set='VALID',\n",
    "                                    img_transform=img_transform,\n",
    "                                    caption_transform=caption_transform)\n",
    "test_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
    "                                   img_captions=test_img_captions,\n",
    "                                   split_set='TEST',\n",
    "                                   img_transform=img_transform,\n",
    "                                   caption_transform=caption_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderWithResNet101(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size=2048, encoded_image_size=14):\n",
    "        super(EncoderWithResNet101, self).__init__()\n",
    "        resnet = torchvision.models.resnet101(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "        \n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "    def fine_tuning_resnet(self, fine_tune):\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            images: Tensor[batch_size, 3, img_size, img_size]\n",
    "        :return\n",
    "            out: Tensor[batch_size, 14, 14, 2048]\n",
    "        \"\"\"\n",
    "        out = self.resnet(images)\n",
    "        out = self.adaptive_pool(out)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encoder():\n",
    "    encoder = EncoderWithResNet101()\n",
    "    latent = encoder(torch.rand((10, 3, 256, 256)))\n",
    "    assert latent.size() == torch.Size([10, 14, 14, 2048])\n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = nn.Linear(enc_hidden_size, hidden_size)\n",
    "        self.W2 = nn.Linear(dec_hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, features, h_state):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            features:  Tensor[batch_size, num_pixels, enc_hidden_size]\n",
    "            h_state: Tensor[batch_size, dec_hidden_size]\n",
    "        :return\n",
    "            context_vector: Tensor[batch_size, enc_hidden_size]\n",
    "            attention_weights: Tensor[batch_size, num_pixels]\n",
    "        \"\"\"\n",
    "        h_state = h_state.unsqueeze(1) # [batch_size, 1, dec_hidden_size]\n",
    "        score = F.relu(self.W1(features) + self.W2(h_state)) # [batch_size, num_pixels, hidden_size]\n",
    "        attention_weights = F.softmax(self.V(score), dim=1) # [batch_size, num_pixels, 1]\n",
    "        context_vector = attention_weights * features # [batch_size, num_pixels, enc_hidden_size]\n",
    "        context_vector = torch.sum(context_vector, dim=1) # [batch_size, enc_hidden_size]\n",
    "        return context_vector, attention_weights.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attention():\n",
    "    attention = BahdanauAttention(enc_hidden_size=2048, dec_hidden_size=512, hidden_size=512)\n",
    "    context_vector, attention_weights = attention(torch.rand((10, 14*14, 2048)), torch.rand((10, 512)))\n",
    "    assert context_vector.size() == torch.Size([10, 2048])\n",
    "    assert attention_weights.size() == torch.Size([10, 14*14])\n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithBahdanauAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_hidden_size, attn_hidden_size, hidden_size, embedding_size, vocab_size, dropout):\n",
    "        super(DecoderWithBahdanauAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size + enc_hidden_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(enc_hidden_size, hidden_size, attn_hidden_size)\n",
    "        self.f_beta = nn.Linear(hidden_size, enc_hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "        \n",
    "    def fine_tuning_embeddings(self, fine_tune=True):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "        \n",
    "    def forward(self, input_word_index, h_state, c_state, enc_outputs):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            input_word_index: Tensor[batch_size,]\n",
    "            h_state: Tensor[1, batch_size, hidden_size]\n",
    "            c_state: Tensor[1, batch_size, hidden_size]\n",
    "            enc_outputs: Tensor[batch_size, num_pixels, enc_hidden_size]\n",
    "        :return\n",
    "            logit: Tensor[batch_size, vocab_size]\n",
    "            h_state: Tensor[1, batch_size, hidden_size]\n",
    "            c_state: Tensor[1, batch_size, hidden_size]\n",
    "            attention_weights: Tensor[batch_size, num_pixels]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_word_index)  # [batch_size, embedding_size]\n",
    "        context_vector, attention_weights = self.attention(enc_outputs, h_state.squeeze(0))\n",
    "        # context_vector: Tensor[batch_size, enc_hidden_size]\n",
    "        # attention_weights: Tensor[batch_size, num_pixels]\n",
    "        gate = torch.sigmoid(self.f_beta(h_state))  # [1, batch_size, enc_hidden_size], Gating scalar\n",
    "        context_vector = gate.squeeze(0) * context_vector # [batch_size, enc_hidden_size]\n",
    "        \n",
    "        x = torch.cat((embedded, context_vector), dim=1) # [batch_size, embedding_size + enc_hidden_size]\n",
    "        output, (h_state, c_state) = self.lstm(x.unsqueeze(0), (h_state, c_state))\n",
    "        # output: [1, batch_size, hidden_size]\n",
    "        # h_state: [1, batch_size, hidden_size]\n",
    "        # c_state: [1, batch_size, hidden_size]\n",
    "        logit = self.fc(self.dropout(output)) # [1, batch_size, vocab_size]\n",
    "        logit = logit.squeeze(0) # [batch_size, vocab_size]\n",
    "        return logit, h_state, c_state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decoder():\n",
    "    decoder = DecoderWithBahdanauAttention(enc_hidden_size=2048, attn_hidden_size=512, hidden_size=512, embedding_size=512, vocab_size=1000, dropout=0.5)\n",
    "    logit, h_state, c_state, attention_weights = \\\n",
    "        decoder(torch.randint(low=0, high=1000, size=(10,)), torch.rand((1, 10, 512)), torch.rand((1, 10, 512)), torch.rand((10, 14*14, 2048)))\n",
    "    assert logit.size() == torch.Size([10, 1000])\n",
    "    assert h_state.size() == torch.Size([1, 10, 512])\n",
    "    assert c_state.size() == torch.Size([1, 10, 512])\n",
    "    assert attention_weights.size() == torch.Size([10, 14*14])\n",
    "test_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.init_h0 = nn.Linear(encoder.hidden_size, decoder.hidden_size)\n",
    "        self.init_c0 = nn.Linear(encoder.hidden_size, decoder.hidden_size)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, images, target_sequences, sequence_lengths):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            images: Tensor[batch_size, 3, img_size, img_size]\n",
    "            target_sequences: Tensor[batch_size, seq_len]\n",
    "            sequence_lengths: Tensor[batch_size,]\n",
    "        :return\n",
    "            logits: Tensor[max(decode_lengths), batch_size, vocab_size]\n",
    "            logits: Tensor[batch_size, max(decode_lengths), num_pixels]\n",
    "            sorted_target_sequences: Tensor[seq_len, batch_size]\n",
    "            sorted_decode_lengths: list[seq_len]\n",
    "            sorted_indices: list[batch_size]\n",
    "        \"\"\"\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Encoding\n",
    "        image_features = self.encoder(images) # [batch_size, 14, 14, hidden_size]\n",
    "        image_features = image_features.view(batch_size, -1, self.encoder.hidden_size) # [batch_size, num_pixels, enc_hidden_size]\n",
    "        num_pixels = image_features.size(1)\n",
    "        \n",
    "        # Sort the batch by decreasing lengths\n",
    "        sorted_sequence_lengths, sorted_indices = torch.sort(sequence_lengths, dim=0, descending=True)\n",
    "        sorted_image_features = image_features[sorted_indices] # [batch_size, num_pixels, enc_hidden_size]\n",
    "        sorted_target_sequences = target_sequences[sorted_indices] # [seq_len, batch_size]\n",
    "        \n",
    "        # Init hidden and memory states\n",
    "        mean_image_features = sorted_image_features.mean(dim=1) # [batch_size, enc_hidden_size]\n",
    "        h_state, c_state = self.init_h0(mean_image_features), self.init_c0(mean_image_features) # [batch_size, dec_hidden_size]\n",
    "        h_state, c_state = h_state.unsqueeze(0), c_state.unsqueeze(0) # [1, batch_size, dec_hidden_size]\n",
    "        \n",
    "        # We won't decode at the <eos> position, since we've finished generating as soon as we generate <eos>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        sorted_decode_lengths = (sorted_sequence_lengths - 1).tolist()\n",
    "        \n",
    "        # Decoding\n",
    "        logits = torch.zeros(max(sorted_decode_lengths), batch_size, self.decoder.vocab_size).to(self.device)\n",
    "        alphas = torch.zeros(batch_size, max(sorted_decode_lengths), num_pixels).to(self.device)\n",
    "        for t in range(max(sorted_decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in sorted_decode_lengths])\n",
    "            logit, h_state, c_state, attention_weights = self.decoder(sorted_target_sequences[:batch_size_t, t],\n",
    "                                                                      h_state[:, :batch_size_t, :],\n",
    "                                                                      c_state[:, :batch_size_t, :],\n",
    "                                                                      sorted_image_features[:batch_size_t, :, :])\n",
    "            logits[t, :batch_size_t, :] = logit\n",
    "            alphas[:batch_size_t, t, :] = attention_weights\n",
    "        \n",
    "        return logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_autoencoder():\n",
    "    encoder = EncoderWithResNet101()\n",
    "    decoder = DecoderWithBahdanauAttention(enc_hidden_size=2048, attn_hidden_size=512, hidden_size=512, embedding_size=512, vocab_size=1000, dropout=0.5)\n",
    "    autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, device='cpu')\n",
    "    logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = \\\n",
    "        autoencoder(torch.rand((10, 3, 256, 256)), torch.randint(low=0, high=1000, size=(10, 25)), torch.randint(low=5, high=26, size=(10,)))\n",
    "    assert logits.size() == torch.Size([max(sorted_decode_lengths), 10, 1000])\n",
    "    assert alphas.size() == torch.Size([10, max(sorted_decode_lengths), 14*14])\n",
    "    assert len(sorted_decode_lengths) == 10\n",
    "    assert sorted_target_sequences.size() == torch.Size([10, 25])\n",
    "    assert len(sorted_indices) == 10\n",
    "test_autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def init_embeddings(embeddings):\n",
    "    bias = np.sqrt(3.0 / embeddings.size(1))\n",
    "    torch.nn.init.uniform_(embeddings, -bias, bias)\n",
    "\n",
    "def load_embeddings(nlp, field):\n",
    "    embeddings = torch.FloatTensor(len(field.vocab), 300)\n",
    "    init_embeddings(embeddings)\n",
    "    for token, index in tqdm.tqdm(field.vocab.stoi.items()):\n",
    "        token = nlp(token)\n",
    "        if token.has_vector:\n",
    "            embeddings[index] = torch.tensor(token.vector, dtype=torch.float32)\n",
    "    return embeddings\n",
    "\n",
    "def clip_gradient(optimizer, grad_clip):\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "\n",
    "def save_checkpoint(model, optimizer, data_name, epoch, last_improv, bleu4, is_best):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'bleu-4': bleu4,\n",
    "        'last_improv': last_improv,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    if not os.path.exists('./checkpoint'):\n",
    "        !mkdir ./checkpoint\n",
    "    torch.save(state, './checkpoint/' + data_name + '.pt')\n",
    "    if is_best:\n",
    "        torch.save(state, './checkpoint/' + 'BEST_' + data_name + '.pt')\n",
    "\n",
    "        \n",
    "class AvgMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def adjust_lr(optimizer, shrink_factor):\n",
    "    print(\"\\nDECAYING learning rate.\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
    "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
    "    \n",
    "def accuracy(outputs, target_sequences, k=5):\n",
    "    batch_size = outputs.size(1)\n",
    "    _, indices = outputs.topk(k, dim=1, largest=True, sorted=True)\n",
    "    correct = indices.eq(target_sequences.view(-1, 1).expand_as(indices))\n",
    "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
    "    return correct_total.item() * (100.0 / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, criterion, loader, epoch, grad_clip, alpha_c, device):\n",
    "    loss_tracker, acc_tracker = AvgMeter(), AvgMeter()\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "    for i, (images, (target_sequences, sequence_lengths)) in pbar:\n",
    "        images = images.to(device)\n",
    "        target_sequences = target_sequences.to(device)\n",
    "        sequence_lengths = sequence_lengths.to(device)\n",
    "        # Forward prop.\n",
    "        logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = model(images, target_sequences, sequence_lengths)\n",
    "        # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
    "        sorted_target_sequences = sorted_target_sequences[:, 1:]\n",
    "        # Remove paddings\n",
    "        logits = pack_padded_sequence(logits, sorted_decode_lengths).data\n",
    "        sorted_target_sequences = pack_padded_sequence(sorted_target_sequences, sorted_decode_lengths, batch_first=True).data\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, sorted_target_sequences)\n",
    "        # Add doubly stochastic attention regularization\n",
    "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "        # Back prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Track metrics\n",
    "        loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
    "        acc_tracker.update(accuracy(logits, sorted_target_sequences, 1), sum(sorted_decode_lengths))\n",
    "        # Update progressbar description\n",
    "        pbar.set_description(f'Epoch: {epoch + 1:03d} - loss: {loss_tracker.avg:.3f} - acc: {acc_tracker.avg:.3f}%')\n",
    "    return loss_tracker.avg, acc_tracker.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, loader, field, epoch, alpha_c, device):\n",
    "    references, hypotheses = [], []\n",
    "    loss_tracker, acc_tracker = AvgMeter(), AvgMeter()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "        for i, (images, (target_sequences, sequence_lengths), (all_target_sequences, all_sequence_lengths)) in pbar:\n",
    "            images = images.to(device)\n",
    "            target_sequences = target_sequences.to(device)\n",
    "            sequence_lengths = sequence_lengths.to(device)\n",
    "            all_target_sequences = all_target_sequences.to(device)\n",
    "            all_sequence_lengths = all_sequence_lengths.to(device)\n",
    "            # Forward prop.\n",
    "            logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = model(images, target_sequences, sequence_lengths)\n",
    "            # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
    "            sorted_target_sequences = sorted_target_sequences[:, 1:]\n",
    "            # Remove paddings\n",
    "            logits_copy = logits.clone()\n",
    "            logits = pack_padded_sequence(logits, sorted_decode_lengths).data\n",
    "            sorted_target_sequences = pack_padded_sequence(sorted_target_sequences, sorted_decode_lengths, batch_first=True).data\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, sorted_target_sequences)\n",
    "            # Add doubly stochastic attention regularization\n",
    "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "            # Track metrics\n",
    "            loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
    "            acc_tracker.update(accuracy(logits, sorted_target_sequences, 1), sum(sorted_decode_lengths))\n",
    "            # Update references\n",
    "            all_sorted_target_sequences = all_target_sequences[sorted_indices] # Because images were sorted in the decoder\n",
    "            for j in range(all_sorted_target_sequences.size(0)):\n",
    "                img_caps = all_sorted_target_sequences[j].t().tolist()\n",
    "                # Remove <sos> and <pad> tokens\n",
    "                img_caps = [*map(lambda c: [field.vocab.itos[w] for w in c if w not in (field.vocab.stoi[field.init_token], field.vocab.stoi[field.pad_token])], img_caps)]\n",
    "                references.append(img_caps)\n",
    "            # Update hypotheses\n",
    "            _, preds = torch.max(logits_copy, dim=2)\n",
    "            preds, temp_preds = preds.t().tolist(), []\n",
    "            for j, p in enumerate(preds):\n",
    "                temp_preds.append([*map(lambda w: field.vocab.itos[w], preds[j][:sorted_decode_lengths[j]])]) # Remove padding\n",
    "            hypotheses.extend(temp_preds)\n",
    "            # Update progressbar description\n",
    "            pbar.set_description(f'Epoch: {epoch + 1:03d} - val_loss: {loss_tracker.avg:.3f} - val_acc: {acc_tracker.avg:.3f}%')\n",
    "        # Calculate BLEU-4 scores\n",
    "        bleu1 = bleu_score(hypotheses, references, max_n=1, weights=[1.])\n",
    "        bleu2 = bleu_score(hypotheses, references, max_n=2, weights=[0.5, 0.5])\n",
    "        bleu3 = bleu_score(hypotheses, references, max_n=3, weights=[1./3, 1./3, 1./3])\n",
    "        bleu4 = bleu_score(hypotheses, references, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "    return loss_tracker.avg, acc_tracker.avg, bleu1, bleu2, bleu3, bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, valid_loader, field, alpha_c, start_epoch, n_epochs, grad_clip, device, model_name, last_improv):\n",
    "    history, best_bleu = {\n",
    "        'acc': [],\n",
    "        'loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_loss': [],\n",
    "        'bleu1': [],\n",
    "        'bleu2': [],\n",
    "        'bleu3': [],\n",
    "        'bleu4': [],\n",
    "    }, 0.\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        # Stop training if no improvment since last 4 epochs\n",
    "        if last_improv == 20:\n",
    "            print('Training Finished - The model has stopped improving since last 20 epochs')\n",
    "            break\n",
    "        # Decay LR if no improvment\n",
    "        if last_improv > 0 and last_improv % 8 == 0:\n",
    "            adjust_lr(optimizer, 0.8)\n",
    "        # Train step\n",
    "        loss, acc = train_step(model=model, optimizer=optimizer, criterion=criterion, loader=train_loader, epoch=epoch, grad_clip=grad_clip, alpha_c=alpha_c, device=device)\n",
    "        # Validation step\n",
    "        val_loss, val_acc, bleu1, bleu2, bleu3, bleu4 = validate(model=model, criterion=criterion, loader=valid_loader, field=field, epoch=epoch, alpha_c=alpha_c, device=device)\n",
    "        # Update history dict\n",
    "        history['acc'].append(acc)\n",
    "        history['loss'].append(loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['bleu1'].append(bleu1)\n",
    "        history['bleu2'].append(bleu2)\n",
    "        history['bleu3'].append(bleu3)\n",
    "        history['bleu4'].append(bleu4)\n",
    "        # Print BLEU score\n",
    "        text = f'BLEU-1: {bleu1*100:.3f}% - BLEU-2: {bleu2*100:.3f}% - BLEU-3: {bleu3*100:.3f}% - BLEU-4: {bleu4*100:.3f}%'\n",
    "        if best_bleu > bleu4:\n",
    "            last_improv += 1\n",
    "            text += f' - Last improvement since {last_improv} epoch(s)'\n",
    "        else:\n",
    "            best_bleu, last_improv = bleu4, 0\n",
    "        print(text)\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(model=model, optimizer=optimizer, data_name=model_name, epoch=epoch, last_improv=last_improv, bleu4=bleu4, is_best=bleu4 >= best_bleu)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2527/2527 [00:21<00:00, 116.01it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = load_embeddings(nlp=NLP, field=EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'Flickr_8k'\n",
    "ENCODER_HIDDEN_SIZE = 2048\n",
    "ATTENTION_SIZE = 512\n",
    "DECODER_HIDDEN_SIZE = 512\n",
    "EMBEDDING_SIZE = 300\n",
    "DROPOUT = 0.5\n",
    "\n",
    "N_EPOCHS = 25\n",
    "BATCH_SIZE = 32\n",
    "LR = 4e-4\n",
    "GRAD_CLIP = 5.\n",
    "ALPHA_C = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of the model: 11,618,784\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderWithResNet101()\n",
    "encoder.fine_tuning_resnet(fine_tune=False)\n",
    "decoder = DecoderWithBahdanauAttention(enc_hidden_size=ENCODER_HIDDEN_SIZE,\n",
    "                                       attn_hidden_size=ATTENTION_SIZE,\n",
    "                                       hidden_size=DECODER_HIDDEN_SIZE,\n",
    "                                       embedding_size=EMBEDDING_SIZE,\n",
    "                                       vocab_size=len(EN.vocab),\n",
    "                                       dropout=DROPOUT)\n",
    "decoder.load_pretrained_embeddings(embeddings)\n",
    "decoder.fine_tuning_embeddings(fine_tune=False)\n",
    "autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, device=DEVICE).to(DEVICE)\n",
    "optimizer = optim.Adam(params=autoencoder.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(f'Number of parameters of the model: {count_parameters(autoencoder):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           num_workers=N_WORKERS,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                           num_workers=N_WORKERS,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True, \n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 001 - loss: 6.175 - acc: 2.184%: 100%|██████████| 188/188 [00:52<00:00,  3.56it/s]\n",
      "Epoch: 001 - val_loss: 5.558 - val_acc: 2.881%: 100%|██████████| 32/32 [00:07<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 45.188% - BLEU-2: 20.739% - BLEU-3: 8.729% - BLEU-4: 3.964%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 002 - loss: 5.458 - acc: 3.047%: 100%|██████████| 188/188 [00:50<00:00,  3.70it/s]\n",
      "Epoch: 002 - val_loss: 5.200 - val_acc: 3.243%: 100%|██████████| 32/32 [00:06<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 51.105% - BLEU-2: 25.074% - BLEU-3: 11.947% - BLEU-4: 5.290%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 003 - loss: 5.177 - acc: 3.247%: 100%|██████████| 188/188 [00:50<00:00,  3.70it/s]\n",
      "Epoch: 003 - val_loss: 4.970 - val_acc: 3.486%: 100%|██████████| 32/32 [00:07<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 52.082% - BLEU-2: 26.533% - BLEU-3: 13.283% - BLEU-4: 6.471%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 004 - loss: 4.979 - acc: 3.449%: 100%|██████████| 188/188 [00:50<00:00,  3.72it/s]\n",
      "Epoch: 004 - val_loss: 4.853 - val_acc: 3.604%: 100%|██████████| 32/32 [00:06<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 53.053% - BLEU-2: 27.678% - BLEU-3: 13.964% - BLEU-4: 6.553%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 005 - loss: 4.837 - acc: 3.595%: 100%|██████████| 188/188 [00:50<00:00,  3.73it/s]\n",
      "Epoch: 005 - val_loss: 4.737 - val_acc: 3.693%: 100%|██████████| 32/32 [00:06<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 52.864% - BLEU-2: 27.386% - BLEU-3: 13.716% - BLEU-4: 6.733%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 006 - loss: 4.712 - acc: 3.690%: 100%|██████████| 188/188 [00:50<00:00,  3.72it/s]\n",
      "Epoch: 006 - val_loss: 4.673 - val_acc: 3.782%: 100%|██████████| 32/32 [00:07<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 54.307% - BLEU-2: 29.236% - BLEU-3: 15.140% - BLEU-4: 7.714%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 007 - loss: 4.619 - acc: 3.781%: 100%|██████████| 188/188 [00:50<00:00,  3.71it/s]\n",
      "Epoch: 007 - val_loss: 4.608 - val_acc: 3.783%: 100%|██████████| 32/32 [00:06<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 53.380% - BLEU-2: 28.110% - BLEU-3: 14.599% - BLEU-4: 7.651% - Last improvement since 1 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 008 - loss: 4.519 - acc: 3.914%: 100%|██████████| 188/188 [00:50<00:00,  3.70it/s]\n",
      "Epoch: 008 - val_loss: 4.547 - val_acc: 3.904%: 100%|██████████| 32/32 [00:06<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 55.392% - BLEU-2: 30.293% - BLEU-3: 16.225% - BLEU-4: 8.656%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 009 - loss: 4.441 - acc: 4.005%: 100%|██████████| 188/188 [00:50<00:00,  3.70it/s]\n",
      "Epoch: 009 - val_loss: 4.523 - val_acc: 4.023%: 100%|██████████| 32/32 [00:06<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 54.010% - BLEU-2: 29.407% - BLEU-3: 15.455% - BLEU-4: 7.798% - Last improvement since 1 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 010 - loss: 4.370 - acc: 4.031%: 100%|██████████| 188/188 [00:50<00:00,  3.72it/s]\n",
      "Epoch: 010 - val_loss: 4.483 - val_acc: 4.069%: 100%|██████████| 32/32 [00:06<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 54.783% - BLEU-2: 30.428% - BLEU-3: 16.168% - BLEU-4: 8.447% - Last improvement since 2 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 011 - loss: 4.302 - acc: 4.127%: 100%|██████████| 188/188 [00:50<00:00,  3.71it/s]\n",
      "Epoch: 011 - val_loss: 4.466 - val_acc: 4.054%: 100%|██████████| 32/32 [00:06<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 55.707% - BLEU-2: 31.043% - BLEU-3: 16.778% - BLEU-4: 8.764%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 012 - loss: 4.234 - acc: 4.226%: 100%|██████████| 188/188 [00:51<00:00,  3.69it/s]\n",
      "Epoch: 012 - val_loss: 4.436 - val_acc: 3.969%: 100%|██████████| 32/32 [00:07<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 55.091% - BLEU-2: 29.875% - BLEU-3: 16.118% - BLEU-4: 8.759% - Last improvement since 1 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 013 - loss: 4.172 - acc: 4.295%: 100%|██████████| 188/188 [00:51<00:00,  3.68it/s]\n",
      "Epoch: 013 - val_loss: 4.416 - val_acc: 3.990%: 100%|██████████| 32/32 [00:06<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 55.710% - BLEU-2: 30.821% - BLEU-3: 16.647% - BLEU-4: 8.844%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 014 - loss: 4.106 - acc: 4.349%: 100%|██████████| 188/188 [00:50<00:00,  3.71it/s]\n",
      "Epoch: 014 - val_loss: 4.427 - val_acc: 4.085%: 100%|██████████| 32/32 [00:06<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 55.484% - BLEU-2: 30.987% - BLEU-3: 16.865% - BLEU-4: 8.892%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 015 - loss: 4.052 - acc: 4.414%: 100%|██████████| 188/188 [00:50<00:00,  3.71it/s]\n",
      "Epoch: 015 - val_loss: 4.405 - val_acc: 4.101%: 100%|██████████| 32/32 [00:07<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 55.172% - BLEU-2: 30.827% - BLEU-3: 16.797% - BLEU-4: 8.860% - Last improvement since 1 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 016 - loss: 3.997 - acc: 4.539%: 100%|██████████| 188/188 [00:51<00:00,  3.65it/s]\n",
      "Epoch: 016 - val_loss: 4.405 - val_acc: 4.084%: 100%|██████████| 32/32 [00:07<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 54.608% - BLEU-2: 30.249% - BLEU-3: 16.552% - BLEU-4: 8.958%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 017 - loss: 3.914 - acc: 4.599%:  32%|███▏      | 60/188 [00:16<00:33,  3.79it/s]"
     ]
    }
   ],
   "source": [
    "history = train(model=autoencoder,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                train_loader=train_loader,\n",
    "                valid_loader=valid_loader,\n",
    "                field=EN,\n",
    "                alpha_c=ALPHA_C,\n",
    "                start_epoch=0,\n",
    "                n_epochs=N_EPOCHS,\n",
    "                grad_clip=GRAD_CLIP,\n",
    "                device=DEVICE,\n",
    "                model_name=MODEL_NAME,\n",
    "                last_improv=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(history['loss'], label='train')\n",
    "axes[0].plot(history['val_loss'], label='valid')\n",
    "axes[0].set_title('Loss history')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True)\n",
    "axes[1].plot(history['acc'], label='train accuracy')\n",
    "axes[1].plot(history['val_acc'], label='valid accuracy')\n",
    "axes[1].plot(np.array(history['bleu4']) * 100., label='BLEU-4')\n",
    "axes[1].set_title('Accuracy & BLEU history')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy & BLEU (%)')\n",
    "axes[1].grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_state_dict(torch.load('./checkpoint/BEST_Flickr_8k.pt').get('model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          num_workers=N_WORKERS,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True, \n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, beam_size, field, max_len, device):\n",
    "    references, hypotheses = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "        for i, (images, (target_sequences, sequence_lengths), (all_target_sequences, all_sequence_lengths)) in pbar:\n",
    "            images = images.to(device)\n",
    "            target_sequences = target_sequences.to(device)\n",
    "            sequence_lengths = sequence_lengths.to(device)\n",
    "            all_target_sequences = all_target_sequences.to(device)\n",
    "            all_sequence_lengths = all_sequence_lengths.to(device)\n",
    "            \n",
    "            k = beam_size\n",
    "            \n",
    "            # Encoding\n",
    "            image_features = model.encoder(images) # [1, 14, 14, hidden_size]\n",
    "            image_features = image_features.view(1, -1, model.encoder.hidden_size) # [1, num_pixels, enc_hidden_size]\n",
    "            \n",
    "            # Init hidden and memory states\n",
    "            mean_image_features = image_features.mean(dim=1) # [1, enc_hidden_size]\n",
    "            h_state, c_state = model.init_h0(mean_image_features), model.init_c0(mean_image_features) # [1, dec_hidden_size]\n",
    "            \n",
    "            # Extend image features, hidden and memory states\n",
    "            image_features = image_features.expand(k, -1, -1)  # [k, num_pixels, enc_hidden_size]\n",
    "            h_state = h_state.expand(k, -1) # [k, dec_hidden_size]\n",
    "            c_state = c_state.expand(k, -1) # [k, dec_hidden_size]\n",
    "            \n",
    "            # Top k previous words at each step; now they're just <sos>\n",
    "            topk_prev_tokens = torch.LongTensor([[field.vocab.stoi[field.init_token]]] * k).to(device)  # [k, 1]\n",
    "            \n",
    "            # Top k sequences; now they're just <sos>\n",
    "            topk_sequences = topk_prev_tokens  # [k, 1]\n",
    "            \n",
    "            # Top k sequences' log proba; now they're just 0\n",
    "            topk_logps = torch.zeros(k, 1).to(device)  # [k, 1]\n",
    "            \n",
    "            # Lists to store completed sequences and logps\n",
    "            complete_sequences, complete_sequence_logps = [], []\n",
    "            \n",
    "            # Decoding\n",
    "            step = 1\n",
    "            while True:\n",
    "                embedded = model.decoder.embedding(topk_prev_tokens.squeeze(1))  # [k, embedding_size]\n",
    "                context_vector, _ = model.decoder.attention(image_features, h_state)\n",
    "                # context_vector: Tensor[k, enc_hidden_size]\n",
    "                # _: Tensor[k, num_pixels]\n",
    "                gate = torch.sigmoid(model.decoder.f_beta(h_state))  # [k, enc_hidden_size], Gating scalar\n",
    "                context_vector = gate * context_vector # [k, enc_hidden_size]\n",
    "                x = torch.cat((embedded, context_vector), dim=1) # [k, embedding_size + enc_hidden_size]\n",
    "                h_state, c_state = h_state.unsqueeze(0).contiguous(), c_state.unsqueeze(0).contiguous()\n",
    "                output, (h_state, c_state) = model.decoder.lstm(x.unsqueeze(0), (h_state, c_state))\n",
    "                # output: [1, k, hidden_size]\n",
    "                # h_state: [1, k, hidden_size]\n",
    "                # c_state: [1, k, hidden_size]\n",
    "                logit = model.decoder.fc(output) # [1, k, vocab_size]\n",
    "                logps = F.log_softmax(logit.squeeze(0), dim=1) # [k, vocab_size]\n",
    "                \n",
    "                # Extend logp\n",
    "                logp = topk_logps.expand_as(logps) + logps  # [k, vocab_size]\n",
    "                \n",
    "                if step == 1:\n",
    "                    topk_logps, topk_tokens = logps[0].topk(k, 0, True, True)  # [k,]\n",
    "                else:\n",
    "                    # Unroll and find top scores, and their unrolled indices\n",
    "                    topk_logps, topk_tokens = logps.view(-1).topk(k, 0, True, True)  # [k,]\n",
    "                    \n",
    "                # Convert unrolled indices to actual indices of logps\n",
    "                prev_token_indices = topk_tokens / model.decoder.vocab_size  # [k,]\n",
    "                next_token_indices = topk_tokens % model.decoder.vocab_size  # [k,]\n",
    "                \n",
    "                # Add new tokens to top k sequences\n",
    "                topk_sequences = torch.cat((topk_sequences[prev_token_indices], next_token_indices.unsqueeze(1)), dim=1)  # [k, step + 1]\n",
    "                \n",
    "                # Get complete and incomplete sequences (didn't reach <eos>)?\n",
    "                incomplete_indices = [indice for indice, next_token in enumerate(next_token_indices) if next_token != field.vocab.stoi[field.eos_token]]\n",
    "                complete_indices = [*set(range(len(next_token_indices))) - set(incomplete_indices)]\n",
    "                \n",
    "                # Set aside complete sequences\n",
    "                if len(complete_indices) > 0:\n",
    "                    complete_sequences.extend(topk_sequences[complete_indices].tolist())\n",
    "                    complete_sequence_logps.extend(topk_logps[complete_indices])\n",
    "                \n",
    "                # Reduce beam size accordingly\n",
    "                k -= len(complete_indices)\n",
    "                \n",
    "                # Break if all sequences are complete\n",
    "                if k == 0:\n",
    "                    break\n",
    "                \n",
    "                # Proceed with incomplete sequences\n",
    "                topk_sequences = topk_sequences[incomplete_indices]\n",
    "                h_state = h_state[:, prev_token_indices[incomplete_indices]].squeeze(0).contiguous()\n",
    "                c_state = c_state[:, prev_token_indices[incomplete_indices]].squeeze(0).contiguous()\n",
    "                image_features = image_features[prev_token_indices[incomplete_indices]]\n",
    "                topk_logps = topk_logps[incomplete_indices].unsqueeze(1)\n",
    "                topk_prev_tokens = next_token_indices[incomplete_indices].unsqueeze(1)\n",
    "                \n",
    "                # Break if things have been going on too long\n",
    "                if step > max_len:\n",
    "                    complete_sequences.extend(topk_sequences[incomplete_indices].tolist())\n",
    "                    complete_sequence_logps.extend(topk_logps[incomplete_indices])\n",
    "                    break\n",
    "                    \n",
    "                # Increment step\n",
    "                step += 1\n",
    "                \n",
    "            i = complete_sequence_logps.index(max(complete_sequence_logps))\n",
    "            topk_sequences = complete_sequences[i]\n",
    "            \n",
    "            # References\n",
    "            all_target_sequences = all_target_sequences[0].tolist()\n",
    "            img_captions = [*map(lambda c: [w for w in c if w not in {field.vocab.stoi[field.init_token],\n",
    "                                                                      field.vocab.stoi[field.eos_token],\n",
    "                                                                      field.vocab.stoi[field.pad_token]}], all_target_sequences)]\n",
    "            references.append(img_captions)\n",
    "            # Hypotheses\n",
    "            hypotheses.append([w for w in topk_sequences if w not in {field.vocab.stoi[field.init_token],\n",
    "                                                                      field.vocab.stoi[field.eos_token],\n",
    "                                                                      field.vocab.stoi[field.pad_token]}])\n",
    "            print(' '.join([field.vocab.itos[w] for w in topk_sequences]))\n",
    "            \n",
    "        # Calculate BLEU-4 scores\n",
    "        bleu4 = corpus_bleu(references, hypotheses)\n",
    "        \n",
    "    return bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu4 = evaluate(model=autoencoder.to(DEVICE), loader=test_loader, beam_size=5, field=EN, max_len=MAX_LEN, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
