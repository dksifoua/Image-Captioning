{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dksifoua/Neural-Image-Caption-Generator/blob/master/Neural%20Image%20Caption%20Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKrQqejCZXyE"
   },
   "source": [
    "# 1. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "S8bMWPjTZXyI",
    "outputId": "ae99572f-89d5-4860-dd16-b5bf7e9c8da3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun  4 18:19:02 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 49%   65C    P8     8W / 180W |      1MiB / 16278MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlsnqBPSZXyg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import functools\n",
    "import collections\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import torchvision\n",
    "from torchtext.data import Example, Field, Dataset\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "AO8NwIodZXy5",
    "outputId": "0781ac6b-6e94-47db-d697-3d5c902bd41b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Number of CPUs: 8\n"
     ]
    }
   ],
   "source": [
    "SEED = 781\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "N_WORKERS = multiprocessing.cpu_count()\n",
    "print(f'Number of CPUs: {N_WORKERS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2StBu8PtZXzG"
   },
   "source": [
    "# 2. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v0emNzBNZXzK",
    "outputId": "e836beaf-c0c3-44ce-8955-3ac2294114a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 µs, sys: 0 ns, total: 28 µs\n",
      "Wall time: 32.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists('./data'):\n",
    "    !mkdir ./data\n",
    "    \n",
    "    !wget --no-check-certificate \\\n",
    "        https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip \\\n",
    "        -O ./data/Flickr8k_Dataset.zip\n",
    "    !unzip -q ./data/Flickr8k_Dataset.zip -d ./data\n",
    "    !rm -r ./data/Flickr8k_Dataset.zip\n",
    "\n",
    "    !wget --no-check-certificate \\\n",
    "        https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip \\\n",
    "        -O ./data/Flickr8k_text.zip\n",
    "    !unzip -q ./data/Flickr8k_text.zip -d ./data\n",
    "    !rm -r ./data/Flickr8k_text.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xd62CBG3ZXzV"
   },
   "source": [
    "# 3. Data Preparation\n",
    "\n",
    "## 3.1. Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "Ud-2n9boZXzX",
    "outputId": "8b1b73ab-ea01-4677-cd59-a619c411e432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use this corpus / data:\n",
      "\n",
      "Please cite: M. Hodosh, P. Young and J. Hockenmaier (2013) \"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics\", Journal of Artifical Intellegence Research, Volume 47, pages 853-899\n",
      "http://www.jair.org/papers/paper3994.html\n",
      "\n",
      "\n",
      "Captions, Dataset Splits, and Human Annotations :\n",
      "\n",
      "\n",
      "Flickr8k.token.txt - the raw captions of the Flickr8k Dataset . The first column is the ID of the caption which is \"image address # caption number\"\n",
      "\n",
      "Flickr8k.lemma.txt - the lemmatized version of the above captions \n",
      "\n",
      "Flickr_8k.trainImages.txt - The training images used in our experiments\n",
      "Flickr_8k.devImages.txt - The development/validation images used in our experiments\n",
      "Flickr_8k.testImages.txt - The test images used in our experiments\n",
      "\n",
      "\n",
      "ExpertAnnotations.txt is the expert judgments.  The first two columns are the image and caption IDs.  Caption IDs are <image file name>#<0-4>.  The next three columns are the expert judgments for that image-caption pair.  Scores range from 1 to 4, with a 1 indicating that the caption does not describe the image at all, a 2 indicating the caption describes minor aspects of the image but does not describe the image, a 3 indicating that the caption almost describes the image with minor mistakes, and a 4 indicating that the caption describes the image.\n",
      "\n",
      "\n",
      "CrowdFlowerAnnotations.txt contains the CrowdFlower judgments.  The first two columns are the image and caption IDs.  The third column is the percent of Yeses, the fourth column is the total number of Yeses, the fifth column is the total number of Noes.  A Yes means that the caption describes the image (possibly with minor mistakes), while a No means that the caption does not describe the image.  Each image-caption pair has a minimum of three judgments, but some may have more.\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "F3RCqRt7ZXzn",
    "outputId": "a37ebf9b-c63c-4d8c-d05d-bc810a66985e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 8,092\n",
      "Number of train images: 6,000\n",
      "Number of valid images: 1,000\n",
      "Number of test images: 1,000\n"
     ]
    }
   ],
   "source": [
    "train_img_fn = [*map(str.strip, open('./data/Flickr_8k.trainImages.txt').readlines())]\n",
    "valid_img_fn = [*map(str.strip, open('./data/Flickr_8k.devImages.txt').readlines())]\n",
    "test_img_fn = [*map(str.strip, open('./data/Flickr_8k.testImages.txt').readlines())]\n",
    "\n",
    "img_captions = collections.defaultdict(lambda: [])\n",
    "with open('./data/Flickr8k.token.txt') as file:\n",
    "    for line in file.readlines():\n",
    "        img_fn, caption = line.strip().split('\\t')\n",
    "        img_captions[img_fn[:-2]].append(caption)\n",
    "        \n",
    "train_img_captions = dict(filter(lambda x: x[0] in train_img_fn, img_captions.items()))\n",
    "valid_img_captions = dict(filter(lambda x: x[0] in valid_img_fn, img_captions.items()))\n",
    "test_img_captions = dict(filter(lambda x: x[0] in test_img_fn, img_captions.items()))\n",
    "    \n",
    "print(f'Number of images: {len(img_captions):,}')\n",
    "print(f'Number of train images: {len(train_img_captions):,}')\n",
    "print(f'Number of valid images: {len(valid_img_captions):,}')\n",
    "print(f'Number of test images: {len(test_img_captions):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4c1mm4rIZXzv"
   },
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "PAD_TOKEN = '<pad>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "\n",
    "def clean(caption):\n",
    "    # Remove non-alphabetical character\n",
    "    caption = re.sub(r'[^a-zA-Z]', r' ', caption)\n",
    "    # Remove one word character\n",
    "    caption = re.sub(r'\\b[a-zA-Z]\\b', r' ', caption)\n",
    "    # Remove multiple spaces\n",
    "    caption = re.sub(r'\\s+', r' ', caption)\n",
    "    return caption.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "ZEgswwOSZX0R",
    "outputId": "9af07ee3-7d89-4c57-a469-9bc1a7574aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train image caption length: 38\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean captions\n",
    "# train_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), train_img_captions.items()))\n",
    "# valid_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), valid_img_captions.items()))\n",
    "# test_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), test_img_captions.items()))\n",
    "\n",
    "# Get the length of the longest caption\n",
    "all_train_captions = [*functools.reduce(lambda x, y: x + y, train_img_captions.values())]\n",
    "print('Max train image caption length:', max(map(len, map(str.split, all_train_captions))))\n",
    "plt.hist([*map(len, map(str.split, all_train_captions))])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "TK007mJhZX09",
    "outputId": "8b0c0d35-dc10-42ff-b60d-586e615f55a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:01<00:00, 18556.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary: 2,548\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 5\n",
    "MAX_LEN = 25\n",
    "all_train_captions = [*functools.reduce(lambda x, y: x + y, train_img_captions.values())]\n",
    "EN = Field(init_token=SOS_TOKEN,\n",
    "           eos_token=EOS_TOKEN,\n",
    "           fix_length=MAX_LEN,\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='en',\n",
    "           include_lengths=True)\n",
    "examples = [Example.fromlist(data=[caption], fields=[('caption', EN)])\n",
    "            for caption in tqdm.tqdm(all_train_captions)]\n",
    "captions_data = Dataset(examples, fields={'caption': EN})\n",
    "EN.build_vocab(captions_data,\n",
    "               min_freq=MIN_COUNT,\n",
    "               specials=[SOS_TOKEN, UNK_TOKEN, EOS_TOKEN, PAD_TOKEN])\n",
    "print(f'Length of vocabulary: {len(EN.vocab):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eEgNGFvJZX1I"
   },
   "outputs": [],
   "source": [
    "def caption_transform(caption):\n",
    "    if isinstance(caption, str):\n",
    "        return EN.process([EN.preprocess(caption)])\n",
    "    elif isinstance(caption, list):\n",
    "        return EN.process([*map(EN.preprocess, caption)])\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4DMbwJDZX1d"
   },
   "source": [
    "## 3.2. Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDBS9jH2ZX1g"
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (256, 256)\n",
    "\n",
    "img_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Lambda(lambda x: x / 255.),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bc7y35ZvZX1z"
   },
   "source": [
    "## 3.3. Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xx9cTSYwZX11"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, img_captions, split_set, img_transform, caption_transform, img_shape=(256, 256)):\n",
    "        assert split_set in {'TRAIN', 'VALID', 'TEST'}\n",
    "        self.data_path = data_path\n",
    "        self.img_captions = img_captions\n",
    "        self.split_set = split_set\n",
    "        self.img_transform = img_transform\n",
    "        self.caption_transform = caption_transform\n",
    "        self.img_shape = img_shape\n",
    "        self.ids = list(sorted(self.img_captions.keys()))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "\n",
    "        img = Image.open(os.path.join(self.data_path, img_id)).convert('RGB')\n",
    "        img = self.img_transform(img.resize(self.img_shape))\n",
    "\n",
    "        targets = self.img_captions[img_id]\n",
    "        targets = self.caption_transform(targets)\n",
    "        \n",
    "        idx = np.random.randint(len(targets))\n",
    "        target = (targets[0][:, idx], targets[1][idx])\n",
    "        \n",
    "        if self.split_set is 'TRAIN':\n",
    "            return img, target\n",
    "        else:\n",
    "            return img, target, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1lMLIU4ZX2X"
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
    "                                    img_captions=train_img_captions,\n",
    "                                    split_set='TRAIN',\n",
    "                                    img_transform=img_transform,\n",
    "                                    caption_transform=caption_transform)\n",
    "valid_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
    "                                    img_captions=valid_img_captions,\n",
    "                                    split_set='VALID',\n",
    "                                    img_transform=img_transform,\n",
    "                                    caption_transform=caption_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7AIJjCbZX20"
   },
   "source": [
    "# 4. Modeling\n",
    "\n",
    "## 4.1. Generate image features - ResNet Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fbvd220ZX22"
   },
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size=2048):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        resnet = torchvision.models.wide_resnet101_2(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "    def fine_tuning_resnet(self, fine_tune):\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            images: Tensor[batch_size, 3, img_size, img_size]\n",
    "        :return\n",
    "            out: Tensor[batch_size, 8, 8, hidden_size]\n",
    "        \"\"\"\n",
    "        out = self.resnet(images)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFUilrnBZX3C"
   },
   "outputs": [],
   "source": [
    "def test_encoder():\n",
    "    encoder = ResNetEncoder()\n",
    "    latent = encoder(torch.rand((10, 3, 256, 256)))\n",
    "    assert latent.size() == torch.Size([10, 8, 8, 2048]), latent.size() \n",
    "    \n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LExSos0FZX3U"
   },
   "source": [
    "## 4.2. Badhanau Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2tAPwvLZX3X"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = nn.Linear(enc_hidden_size, hidden_size)\n",
    "        self.W2 = nn.Linear(dec_hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, features, h_state):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            features:  Tensor[batch_size, num_pixels, enc_hidden_size]\n",
    "            h_state: Tensor[batch_size, dec_hidden_size]\n",
    "        :return\n",
    "            context_vector: Tensor[batch_size, enc_hidden_size]\n",
    "            attention_weights: Tensor[batch_size, num_pixels]\n",
    "        \"\"\"\n",
    "        h_state = h_state.unsqueeze(1) # [batch_size, 1, dec_hidden_size]\n",
    "        score = F.elu(self.W1(features) + self.W2(h_state)) # [batch_size, num_pixels, hidden_size]\n",
    "        attention_weights = F.softmax(self.V(score), dim=1) # [batch_size, num_pixels, 1]\n",
    "        context_vector = attention_weights * features # [batch_size, num_pixels, enc_hidden_size]\n",
    "        context_vector = torch.sum(context_vector, dim=1) # [batch_size, enc_hidden_size]\n",
    "        return context_vector, attention_weights.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWzAiYL3ZX3o"
   },
   "outputs": [],
   "source": [
    "def test_attention():\n",
    "    attention = BahdanauAttention(enc_hidden_size=2048, dec_hidden_size=512, hidden_size=512)\n",
    "    context_vector, attention_weights = attention(torch.rand((10, 8*8, 2048)), torch.rand((10, 512)))\n",
    "    assert context_vector.size() == torch.Size([10, 2048])\n",
    "    assert attention_weights.size() == torch.Size([10, 8*8])\n",
    "    \n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R62bBm1OZX32"
   },
   "source": [
    "## 4.3. Generate captions - LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdFH5HsPZX34"
   },
   "outputs": [],
   "source": [
    "class DecoderWithBahdanauAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_hidden_size, attn_hidden_size, hidden_size, embedding_size, vocab_size, dropout):\n",
    "        super(DecoderWithBahdanauAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size + enc_hidden_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(enc_hidden_size, hidden_size, attn_hidden_size)\n",
    "        self.f_beta = nn.Linear(hidden_size, enc_hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "        \n",
    "    def fine_tuning_embeddings(self, fine_tune=False):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "        \n",
    "    def forward(self, input_word_index, h_state, c_state, enc_outputs):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            input_word_index: Tensor[batch_size,]\n",
    "            h_state: Tensor[1, batch_size, hidden_size]\n",
    "            c_state: Tensor[1, batch_size, hidden_size]\n",
    "            enc_outputs: Tensor[batch_size, num_pixels, enc_hidden_size]\n",
    "        :return\n",
    "            logit: Tensor[batch_size, vocab_size]\n",
    "            h_state: Tensor[1, batch_size, hidden_size]\n",
    "            c_state: Tensor[1, batch_size, hidden_size]\n",
    "            attention_weights: Tensor[batch_size, num_pixels]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_word_index)  # [batch_size, embedding_size]\n",
    "        context_vector, attention_weights = self.attention(enc_outputs, h_state.squeeze(0))\n",
    "        # context_vector: Tensor[batch_size, enc_hidden_size]\n",
    "        # attention_weights: Tensor[batch_size, num_pixels]\n",
    "        \n",
    "        gate = torch.sigmoid(self.f_beta(h_state))  # [1, batch_size, enc_hidden_size], Gating scalar\n",
    "        context_vector = gate.squeeze(0) * context_vector # [batch_size, enc_hidden_size]\n",
    "        \n",
    "        x = torch.cat((embedded, context_vector), dim=1) # [batch_size, embedding_size + enc_hidden_size]\n",
    "        output, (h_state, c_state) = self.lstm(x.unsqueeze(0), (h_state, c_state))\n",
    "        # output: [1, batch_size, hidden_size]\n",
    "        # h_state: [1, batch_size, hidden_size]\n",
    "        # c_state: [1, batch_size, hidden_size]\n",
    "        \n",
    "        logit = self.fc(self.dropout(output)) # [1, batch_size, vocab_size]\n",
    "        logit = logit.squeeze(0) # [batch_size, vocab_size]\n",
    "        return logit, h_state, c_state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1hWqJPYnZX4E"
   },
   "outputs": [],
   "source": [
    "def test_decoder():\n",
    "    decoder = DecoderWithBahdanauAttention(enc_hidden_size=2048,\n",
    "                                           attn_hidden_size=512,\n",
    "                                           hidden_size=512,\n",
    "                                           embedding_size=512,\n",
    "                                           vocab_size=1000,\n",
    "                                           dropout=0.5)\n",
    "    logit, h_state, c_state, attention_weights = decoder(torch.randint(low=0, high=1000, size=(10,)),\n",
    "                                                         torch.rand((1, 10, 512)),\n",
    "                                                         torch.rand((1, 10, 512)),\n",
    "                                                         torch.rand((10, 14*14, 2048)))\n",
    "    assert logit.size() == torch.Size([10, 1000])\n",
    "    assert h_state.size() == torch.Size([1, 10, 512])\n",
    "    assert c_state.size() == torch.Size([1, 10, 512])\n",
    "    assert attention_weights.size() == torch.Size([10, 14*14])\n",
    "    \n",
    "test_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZHfTeBkUZX4P"
   },
   "source": [
    "## 4.4. Putting all together - AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlZGDdL9ZX4Q"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.init_h0 = nn.Linear(encoder.hidden_size, decoder.hidden_size)\n",
    "        self.init_c0 = nn.Linear(encoder.hidden_size, decoder.hidden_size)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, images, target_sequences, sequence_lengths, tf_ratio):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            images: Tensor[batch_size, 3, img_size, img_size]\n",
    "            target_sequences: Tensor[batch_size, seq_len]\n",
    "            sequence_lengths: Tensor[batch_size,]\n",
    "            tf_ratio: float\n",
    "        :return\n",
    "            logits: Tensor[max(decode_lengths), batch_size, vocab_size]\n",
    "            logits: Tensor[batch_size, max(decode_lengths), num_pixels]\n",
    "            sorted_target_sequences: Tensor[seq_len, batch_size]\n",
    "            sorted_decode_lengths: list[seq_len]\n",
    "            sorted_indices: list[batch_size]\n",
    "        \"\"\"\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Encoding\n",
    "        image_features = self.encoder(images) # [batch_size, 14, 14, hidden_size]\n",
    "        image_features = image_features.view(batch_size, -1, self.encoder.hidden_size) # [batch_size, num_pixels, enc_hidden_size]\n",
    "        num_pixels = image_features.size(1)\n",
    "        \n",
    "        # Sort the batch by decreasing lengths\n",
    "        sorted_sequence_lengths, sorted_indices = torch.sort(sequence_lengths, dim=0, descending=True)\n",
    "        sorted_image_features = image_features[sorted_indices] # [batch_size, num_pixels, enc_hidden_size]\n",
    "        sorted_target_sequences = target_sequences[sorted_indices] # [seq_len, batch_size]\n",
    "        \n",
    "        # Init hidden and memory states\n",
    "        mean_image_features = sorted_image_features.mean(dim=1) # [batch_size, enc_hidden_size]\n",
    "        h_state, c_state = self.init_h0(mean_image_features), self.init_c0(mean_image_features) # [batch_size, dec_hidden_size]\n",
    "        h_state, c_state = h_state.unsqueeze(0), c_state.unsqueeze(0) # [1, batch_size, dec_hidden_size]\n",
    "        \n",
    "        # We won't decode at the <eos> position, since we've finished generating as soon as we generate <eos>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        sorted_decode_lengths = (sorted_sequence_lengths - 1).tolist()\n",
    "        \n",
    "        # Decoding\n",
    "        logits = torch.zeros(max(sorted_decode_lengths), batch_size, self.decoder.vocab_size).to(self.device)\n",
    "        alphas = torch.zeros(batch_size, max(sorted_decode_lengths), num_pixels).to(self.device)\n",
    "        last = None\n",
    "        for t in range(max(sorted_decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in sorted_decode_lengths])\n",
    "            \n",
    "            if last is not None:\n",
    "                if random.random() < tf_ratio:\n",
    "                    in_ = last[:batch_size_t]\n",
    "                else:\n",
    "                    in_ = sorted_target_sequences[:batch_size_t, t]\n",
    "            else:\n",
    "                in_ = sorted_target_sequences[:batch_size_t, t]\n",
    "            \n",
    "            logit, h_state, c_state, attention_weights = self.decoder(in_,\n",
    "                                                                      h_state[:, :batch_size_t, :],\n",
    "                                                                      c_state[:, :batch_size_t, :],\n",
    "                                                                      sorted_image_features[:batch_size_t, :, :])\n",
    "            logits[t, :batch_size_t, :] = logit\n",
    "            alphas[:batch_size_t, t, :] = attention_weights\n",
    "            last = torch.argmax(F.softmax(logit, dim=1), dim=1) # [batch_size,]\n",
    "        \n",
    "        return logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xL8sy2MsZX4o"
   },
   "outputs": [],
   "source": [
    "def test_autoencoder():\n",
    "    encoder = ResNetEncoder()\n",
    "    decoder = DecoderWithBahdanauAttention(enc_hidden_size=2048,\n",
    "                                           attn_hidden_size=512,\n",
    "                                           hidden_size=512,\n",
    "                                           embedding_size=512,\n",
    "                                           vocab_size=1000,\n",
    "                                           dropout=0.5)\n",
    "    autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, device='cpu')\n",
    "    logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = \\\n",
    "        autoencoder(torch.rand((10, 3, 256, 256)),\n",
    "                    torch.randint(low=0, high=1000, size=(10, 25)),\n",
    "                    torch.randint(low=5, high=26, size=(10,)), 0.5)\n",
    "    assert logits.size() == torch.Size([max(sorted_decode_lengths), 10, 1000])\n",
    "    assert alphas.size() == torch.Size([10, max(sorted_decode_lengths), 8*8])\n",
    "    assert len(sorted_decode_lengths) == 10\n",
    "    assert sorted_target_sequences.size() == torch.Size([10, 25])\n",
    "    assert len(sorted_indices) == 10\n",
    "    \n",
    "test_autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-Ph4v-GZX43"
   },
   "source": [
    "# 5. Training\n",
    "\n",
    "## 5.1. Training routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Twx29pVuZX46"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def init_embeddings(embeddings):\n",
    "    bias = np.sqrt(3.0 / embeddings.size(1))\n",
    "    torch.nn.init.uniform_(embeddings, -bias, bias)\n",
    "\n",
    "def load_embeddings(nlp, field):\n",
    "    embeddings = torch.FloatTensor(len(field.vocab), 300)\n",
    "    init_embeddings(embeddings)\n",
    "    for token, index in tqdm.tqdm(field.vocab.stoi.items()):\n",
    "        token = nlp(token)\n",
    "        if token.has_vector:\n",
    "            embeddings[index] = torch.tensor(token.vector, dtype=torch.float32)\n",
    "    return embeddings\n",
    "\n",
    "def clip_gradient(optimizer, grad_clip):\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "\n",
    "def save_checkpoint(model, optimizer, data_name, epoch, last_improv, bleu4, is_best):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'bleu-4': bleu4,\n",
    "        'last_improv': last_improv,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    if not os.path.exists('./checkpoint'):\n",
    "        !mkdir ./checkpoint\n",
    "    torch.save(state, './checkpoint/' + data_name + '.pt')\n",
    "    if is_best:\n",
    "        torch.save(state, './checkpoint/' + 'BEST_' + data_name + '.pt')\n",
    "\n",
    "        \n",
    "class AvgMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def adjust_lr(optimizer, shrink_factor):\n",
    "    print(\"\\nDecaying learning rate.\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
    "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
    "    \n",
    "def accuracy(outputs, target_sequences, k=5):\n",
    "    batch_size = outputs.size(1)\n",
    "    _, indices = outputs.topk(k, dim=1, largest=True, sorted=True)\n",
    "    correct = indices.eq(target_sequences.view(-1, 1).expand_as(indices))\n",
    "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
    "    return correct_total.item() * (100.0 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gPf9q93ZX5L"
   },
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, criterion, loader, epoch, grad_clip, alpha_c, tf_ratio, device):\n",
    "    loss_tracker, acc_tracker = AvgMeter(), AvgMeter()\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "    for i, (images, (target_sequences, sequence_lengths)) in pbar:\n",
    "        images = images.to(device)\n",
    "        target_sequences = target_sequences.to(device)\n",
    "        sequence_lengths = sequence_lengths.to(device)\n",
    "        # Forward prop.\n",
    "        logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = model(images, target_sequences, sequence_lengths, tf_ratio)\n",
    "        # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
    "        sorted_target_sequences = sorted_target_sequences[:, 1:]\n",
    "        # Remove paddings\n",
    "        logits = pack_padded_sequence(logits, sorted_decode_lengths).data\n",
    "        sorted_target_sequences = pack_padded_sequence(sorted_target_sequences, sorted_decode_lengths, batch_first=True).data\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, sorted_target_sequences)\n",
    "        # Add doubly stochastic attention regularization\n",
    "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "        # Back prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Track metrics\n",
    "        loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
    "        acc_tracker.update(accuracy(logits, sorted_target_sequences, 5), sum(sorted_decode_lengths))\n",
    "        # Update progressbar description\n",
    "        pbar.set_description(f'Epoch: {epoch + 1:03d} - loss: {loss_tracker.avg:.3f} - acc: {acc_tracker.avg:.3f}%')\n",
    "    return loss_tracker.avg, acc_tracker.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npl8hB_QZX5m"
   },
   "outputs": [],
   "source": [
    "def validate(model, criterion, loader, field, epoch, alpha_c, device):\n",
    "    references, hypotheses = [], []\n",
    "    loss_tracker, acc_tracker = AvgMeter(), AvgMeter()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "        for i, (images, (target_sequences, sequence_lengths), (all_target_sequences, all_sequence_lengths)) in pbar:\n",
    "            images = images.to(device)\n",
    "            target_sequences = target_sequences.to(device)\n",
    "            sequence_lengths = sequence_lengths.to(device)\n",
    "            all_target_sequences = all_target_sequences.to(device)\n",
    "            all_sequence_lengths = all_sequence_lengths.to(device)\n",
    "            # Forward prop.\n",
    "            logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = model(images, target_sequences, sequence_lengths, 0)\n",
    "            # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
    "            sorted_target_sequences = sorted_target_sequences[:, 1:]\n",
    "            # Remove paddings\n",
    "            logits_copy = logits.clone()\n",
    "            logits = pack_padded_sequence(logits, sorted_decode_lengths).data\n",
    "            sorted_target_sequences = pack_padded_sequence(sorted_target_sequences, sorted_decode_lengths, batch_first=True).data\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, sorted_target_sequences)\n",
    "            # Add doubly stochastic attention regularization\n",
    "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "            # Track metrics\n",
    "            loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
    "            acc_tracker.update(accuracy(logits, sorted_target_sequences, 5), sum(sorted_decode_lengths))\n",
    "            # Update references\n",
    "            all_sorted_target_sequences = all_target_sequences[sorted_indices] # Because images were sorted in the decoder\n",
    "            for j in range(all_sorted_target_sequences.size(0)):\n",
    "                img_caps = all_sorted_target_sequences[j].t().tolist()\n",
    "                # Remove <sos> and <pad> tokens\n",
    "                img_caps = [*map(lambda c: [field.vocab.itos[w] for w in c\n",
    "                                            if w not in (field.vocab.stoi[field.init_token],\n",
    "                                                         field.vocab.stoi[field.pad_token])], img_caps)]\n",
    "                references.append(img_caps)\n",
    "            # Update hypotheses\n",
    "            _, preds = torch.max(logits_copy, dim=2)\n",
    "            preds, temp_preds = preds.t().tolist(), []\n",
    "            for j, p in enumerate(preds):\n",
    "                temp_preds.append([*map(lambda w: field.vocab.itos[w], preds[j][:sorted_decode_lengths[j]])]) # Remove padding\n",
    "            hypotheses.extend(temp_preds)\n",
    "            # Update progressbar description\n",
    "            pbar.set_description(f'Epoch: {epoch + 1:03d} - val_loss: {loss_tracker.avg:.3f} - val_acc: {acc_tracker.avg:.3f}%')\n",
    "        # Calculate BLEU-4 score\n",
    "        bleu4 = bleu_score(hypotheses, references, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "    return loss_tracker.avg, acc_tracker.avg, bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0N1p6-xkZX56"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, valid_loader, field, alpha_c, start_epoch, n_epochs, grad_clip, tf_ratio, device, model_name, last_improv):\n",
    "    history, best_bleu = {\n",
    "        'acc': [],\n",
    "        'loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_loss': [],\n",
    "        'bleu4': []\n",
    "    }, 0.\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        # Stop training if no improvment since last 4 epochs\n",
    "        if last_improv == 4:\n",
    "            print('Training Finished - The model has stopped improving since last 4 epochs')\n",
    "            break\n",
    "        # Decay LR if no improvment\n",
    "        if last_improv > 0:\n",
    "            adjust_lr(optimizer, 0.8)\n",
    "        # Train step\n",
    "        loss, acc = train_step(model=model, optimizer=optimizer, criterion=criterion,\n",
    "                               loader=train_loader, epoch=epoch, grad_clip=grad_clip,\n",
    "                               alpha_c=alpha_c, tf_ratio=tf_ratio, device=device)\n",
    "        # Validation step\n",
    "        val_loss, val_acc, bleu4 = validate(model=model, criterion=criterion, loader=valid_loader,\n",
    "                                            field=field, epoch=epoch, alpha_c=alpha_c, device=device)\n",
    "        # Update history dict\n",
    "        history['acc'].append(acc)\n",
    "        history['loss'].append(loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['bleu4'].append(bleu4)\n",
    "        # Print BLEU score\n",
    "        text = f'BLEU-4: {bleu4*100:.3f}%'\n",
    "        if best_bleu > bleu4:\n",
    "            last_improv += 1\n",
    "            text += f' - Last improvement since {last_improv} epoch(s)'\n",
    "        else:\n",
    "            best_bleu, last_improv = bleu4, 0\n",
    "        print(text)\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(model=model, optimizer=optimizer, data_name=model_name, epoch=epoch, last_improv=last_improv, bleu4=bleu4, is_best=bleu4 >= best_bleu)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nN_kRRgjZX6B"
   },
   "source": [
    "## 5.2. Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQD6jkdfZX6G"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'Flickr_8k'\n",
    "ENCODER_HIDDEN_SIZE = 2048\n",
    "ATTENTION_SIZE = 512\n",
    "DECODER_HIDDEN_SIZE = 512\n",
    "EMBEDDING_SIZE = 300\n",
    "DROPOUT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EzLwRaLMZX6L"
   },
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "batch_size = 32\n",
    "lr = 3e-5\n",
    "grad_clip = 5.\n",
    "alpha_c = 1.\n",
    "tf_ratio = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tpI9IT-PZX6a",
    "outputId": "c3ddeaa7-3250-43e9-b8d2-64b65a4e70c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2548/2548 [00:18<00:00, 139.23it/s]\n"
     ]
    }
   ],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_lg')\n",
    "embeddings = load_embeddings(nlp=spacy_nlp, field=EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WM1wCpP6ZX6i",
    "outputId": "44943ebb-e57f-40d2-cf36-e20679545a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of the model: 136,587,749\n"
     ]
    }
   ],
   "source": [
    "encoder = ResNetEncoder()\n",
    "encoder.fine_tuning_resnet(fine_tune=True)\n",
    "decoder = DecoderWithBahdanauAttention(enc_hidden_size=ENCODER_HIDDEN_SIZE,\n",
    "                                       attn_hidden_size=ATTENTION_SIZE,\n",
    "                                       hidden_size=DECODER_HIDDEN_SIZE,\n",
    "                                       embedding_size=EMBEDDING_SIZE,\n",
    "                                       vocab_size=len(EN.vocab),\n",
    "                                       dropout=DROPOUT)\n",
    "decoder.load_pretrained_embeddings(embeddings)\n",
    "decoder.fine_tuning_embeddings(fine_tune=True)\n",
    "autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, device=DEVICE).to(DEVICE)\n",
    "optimizer = optim.RMSprop(params=autoencoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(f'Number of parameters of the model: {count_parameters(autoencoder):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QjnxhtJ2ZX6t"
   },
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           num_workers=N_WORKERS,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                           num_workers=N_WORKERS,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True, \n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "IeYO4y0EZX65",
    "outputId": "7eb11200-8d72-4b05-eaad-c97afa8e6a71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 001 - loss: 5.664 - acc: 6.237%: 100%|██████████| 188/188 [02:44<00:00,  1.14it/s]\n",
      "Epoch: 001 - val_loss: 5.257 - val_acc: 6.780%: 100%|██████████| 32/32 [00:11<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 0.000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 002 - loss: 5.201 - acc: 6.952%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 002 - val_loss: 5.102 - val_acc: 7.093%: 100%|██████████| 32/32 [00:09<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 4.716%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 003 - loss: 5.009 - acc: 7.332%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 003 - val_loss: 4.949 - val_acc: 7.363%: 100%|██████████| 32/32 [00:09<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 5.145%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 004 - loss: 4.836 - acc: 7.731%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 004 - val_loss: 4.810 - val_acc: 7.681%: 100%|██████████| 32/32 [00:09<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 6.213%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 005 - loss: 4.698 - acc: 8.032%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 005 - val_loss: 4.729 - val_acc: 7.902%: 100%|██████████| 32/32 [00:09<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 6.816%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 006 - loss: 4.572 - acc: 8.279%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 006 - val_loss: 4.651 - val_acc: 8.205%: 100%|██████████| 32/32 [00:09<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 7.418%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 007 - loss: 4.467 - acc: 8.590%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 007 - val_loss: 4.575 - val_acc: 8.379%: 100%|██████████| 32/32 [00:09<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 7.790%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 008 - loss: 4.376 - acc: 8.841%:  61%|██████    | 114/188 [01:37<01:03,  1.17it/s]"
     ]
    }
   ],
   "source": [
    "history = train(model=autoencoder,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                train_loader=train_loader,\n",
    "                valid_loader=valid_loader,\n",
    "                field=EN,\n",
    "                alpha_c=alpha_c,\n",
    "                start_epoch=0,\n",
    "                n_epochs=n_epochs,\n",
    "                grad_clip=grad_clip,\n",
    "                tf_ratio=tf_ratio,\n",
    "                device=DEVICE,\n",
    "                model_name=MODEL_NAME,\n",
    "                last_improv=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "3bebBmWmZX7A",
    "outputId": "9bcf2d8b-1f0d-41b7-fd81-d289562473a2"
   },
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(history['loss'], label='train')\n",
    "axes[0].plot(history['val_loss'], label='valid')\n",
    "axes[0].set_title('Loss history')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True)\n",
    "axes[1].plot(history['acc'], label='train')\n",
    "axes[1].plot(history['val_acc'], label='valid')\n",
    "axes[1].plot(np.array(history['bleu4']) * 100., label='BLEU-4')\n",
    "axes[1].set_title('Accuracy history')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8s2Dp7RPZX7i"
   },
   "source": [
    "# 6. Evaluation - BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wC3H7uoPZX8J",
    "outputId": "a614a5c7-08f6-41de-a658-6bd7a0a53f84"
   },
   "outputs": [],
   "source": [
    "autoencoder.load_state_dict(torch.load('./checkpoint/BEST_Flickr_8k.pt').get('model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULooViEOZX8e"
   },
   "outputs": [],
   "source": [
    "test_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
    "                                   img_captions=test_img_captions,\n",
    "                                   split_set='TEST',\n",
    "                                   img_transform=img_transform,\n",
    "                                   caption_transform=caption_transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          num_workers=N_WORKERS,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True, \n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2DZ6d03twtu"
   },
   "outputs": [],
   "source": [
    "# class BeamNode:\n",
    "#     __index = 0\n",
    "\n",
    "#     def __init__(self, ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kXqLgQHZX8j"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, beam_size, field, max_len, device):\n",
    "    references, hypotheses = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "        for i, (images, (target_sequences, sequence_lengths), (all_target_sequences, all_sequence_lengths)) in pbar:\n",
    "            images = images.to(device)\n",
    "            target_sequences = target_sequences.to(device)\n",
    "            sequence_lengths = sequence_lengths.to(device)\n",
    "            all_target_sequences = all_target_sequences.to(device)\n",
    "            all_sequence_lengths = all_sequence_lengths.to(device)\n",
    "            \n",
    "            k = beam_size\n",
    "            \n",
    "            # Encoding\n",
    "            image_features = model.encoder(images) # [1, 14, 14, hidden_size]\n",
    "            image_features = image_features.view(1, -1, model.encoder.hidden_size) # [1, num_pixels, enc_hidden_size]\n",
    "            \n",
    "            # Init hidden and memory states\n",
    "            mean_image_features = image_features.mean(dim=1) # [1, enc_hidden_size]\n",
    "            h_state, c_state = model.init_h0(mean_image_features), model.init_c0(mean_image_features) # [1, dec_hidden_size]\n",
    "\n",
    "            # Decoding\n",
    "            step = 1\n",
    "            while True:\n",
    "                embedded = model.decoder.embedding(topk_prev_tokens.squeeze(1))  # [k, embedding_size]\n",
    "                context_vector, _ = model.decoder.attention(image_features, h_state)\n",
    "                # context_vector: Tensor[k, enc_hidden_size]\n",
    "                # _: Tensor[k, num_pixels]\n",
    "                gate = torch.sigmoid(model.decoder.f_beta(h_state))  # [k, enc_hidden_size], Gating scalar\n",
    "                context_vector = gate * context_vector # [k, enc_hidden_size]\n",
    "                x = torch.cat((embedded, context_vector), dim=1) # [k, embedding_size + enc_hidden_size]\n",
    "                h_state, c_state = h_state.unsqueeze(0).contiguous(), c_state.unsqueeze(0).contiguous()\n",
    "                output, (h_state, c_state) = model.decoder.lstm(x.unsqueeze(0), (h_state, c_state))\n",
    "                # output: [1, k, hidden_size]\n",
    "                # h_state: [1, k, hidden_size]\n",
    "                # c_state: [1, k, hidden_size]\n",
    "                logit = model.decoder.fc(output) # [1, k, vocab_size]\n",
    "                logp = F.log_softmax(logit.squeeze(0), dim=1) # [k, vocab_size]\n",
    "\n",
    "                topk_logp = torch.topk(logp, k, dim=1) # [k, k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZMIDy2mZX81"
   },
   "source": [
    "# 7. Inference - Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gzgW20ekZX81"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D55g1AHNZX8_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Neural Image Caption generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
