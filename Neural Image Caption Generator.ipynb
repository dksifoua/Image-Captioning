{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dksifoua/Neural-Image-Caption-Generator/blob/master/Neural%20Image%20Caption%20Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKrQqejCZXyE"
   },
   "source": [
    "# 1. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "S8bMWPjTZXyI",
    "outputId": "ae99572f-89d5-4860-dd16-b5bf7e9c8da3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun  4 20:43:02 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 35%   51C    P5     9W / 180W |      1MiB / 16278MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlsnqBPSZXyg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import functools\n",
    "import collections\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import torchvision\n",
    "from torchtext.data import Example, Field, Dataset\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "AO8NwIodZXy5",
    "outputId": "0781ac6b-6e94-47db-d697-3d5c902bd41b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Number of CPUs: 8\n"
     ]
    }
   ],
   "source": [
    "SEED = 781\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "N_WORKERS = multiprocessing.cpu_count()\n",
    "print(f'Number of CPUs: {N_WORKERS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2StBu8PtZXzG"
   },
   "source": [
    "# 2. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v0emNzBNZXzK",
    "outputId": "e836beaf-c0c3-44ce-8955-3ac2294114a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 µs, sys: 18 µs, total: 48 µs\n",
      "Wall time: 40.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists('./data'):\n",
    "    !mkdir ./data\n",
    "    \n",
    "    !wget --no-check-certificate \\\n",
    "        https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip \\\n",
    "        -O ./data/Flickr8k_Dataset.zip\n",
    "    !unzip -q ./data/Flickr8k_Dataset.zip -d ./data\n",
    "    !rm -r ./data/Flickr8k_Dataset.zip\n",
    "\n",
    "    !wget --no-check-certificate \\\n",
    "        https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip \\\n",
    "        -O ./data/Flickr8k_text.zip\n",
    "    !unzip -q ./data/Flickr8k_text.zip -d ./data\n",
    "    !rm -r ./data/Flickr8k_text.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xd62CBG3ZXzV"
   },
   "source": [
    "# 3. Data Preparation\n",
    "\n",
    "## 3.1. Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "Ud-2n9boZXzX",
    "outputId": "8b1b73ab-ea01-4677-cd59-a619c411e432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use this corpus / data:\n",
      "\n",
      "Please cite: M. Hodosh, P. Young and J. Hockenmaier (2013) \"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics\", Journal of Artifical Intellegence Research, Volume 47, pages 853-899\n",
      "http://www.jair.org/papers/paper3994.html\n",
      "\n",
      "\n",
      "Captions, Dataset Splits, and Human Annotations :\n",
      "\n",
      "\n",
      "Flickr8k.token.txt - the raw captions of the Flickr8k Dataset . The first column is the ID of the caption which is \"image address # caption number\"\n",
      "\n",
      "Flickr8k.lemma.txt - the lemmatized version of the above captions \n",
      "\n",
      "Flickr_8k.trainImages.txt - The training images used in our experiments\n",
      "Flickr_8k.devImages.txt - The development/validation images used in our experiments\n",
      "Flickr_8k.testImages.txt - The test images used in our experiments\n",
      "\n",
      "\n",
      "ExpertAnnotations.txt is the expert judgments.  The first two columns are the image and caption IDs.  Caption IDs are <image file name>#<0-4>.  The next three columns are the expert judgments for that image-caption pair.  Scores range from 1 to 4, with a 1 indicating that the caption does not describe the image at all, a 2 indicating the caption describes minor aspects of the image but does not describe the image, a 3 indicating that the caption almost describes the image with minor mistakes, and a 4 indicating that the caption describes the image.\n",
      "\n",
      "\n",
      "CrowdFlowerAnnotations.txt contains the CrowdFlower judgments.  The first two columns are the image and caption IDs.  The third column is the percent of Yeses, the fourth column is the total number of Yeses, the fifth column is the total number of Noes.  A Yes means that the caption describes the image (possibly with minor mistakes), while a No means that the caption does not describe the image.  Each image-caption pair has a minimum of three judgments, but some may have more.\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "F3RCqRt7ZXzn",
    "outputId": "a37ebf9b-c63c-4d8c-d05d-bc810a66985e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 8,092\n",
      "Number of train images: 6,000\n",
      "Number of valid images: 1,000\n",
      "Number of test images: 1,000\n"
     ]
    }
   ],
   "source": [
    "train_img_fn = [*map(str.strip, open('./data/Flickr_8k.trainImages.txt').readlines())]\n",
    "valid_img_fn = [*map(str.strip, open('./data/Flickr_8k.devImages.txt').readlines())]\n",
    "test_img_fn = [*map(str.strip, open('./data/Flickr_8k.testImages.txt').readlines())]\n",
    "\n",
    "img_captions = collections.defaultdict(lambda: [])\n",
    "with open('./data/Flickr8k.token.txt') as file:\n",
    "    for line in file.readlines():\n",
    "        img_fn, caption = line.strip().split('\\t')\n",
    "        img_captions[img_fn[:-2]].append(caption)\n",
    "        \n",
    "train_img_captions = dict(filter(lambda x: x[0] in train_img_fn, img_captions.items()))\n",
    "valid_img_captions = dict(filter(lambda x: x[0] in valid_img_fn, img_captions.items()))\n",
    "test_img_captions = dict(filter(lambda x: x[0] in test_img_fn, img_captions.items()))\n",
    "    \n",
    "print(f'Number of images: {len(img_captions):,}')\n",
    "print(f'Number of train images: {len(train_img_captions):,}')\n",
    "print(f'Number of valid images: {len(valid_img_captions):,}')\n",
    "print(f'Number of test images: {len(test_img_captions):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4c1mm4rIZXzv"
   },
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "PAD_TOKEN = '<pad>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "\n",
    "def clean(caption):\n",
    "    # Remove non-alphabetical character\n",
    "    caption = re.sub(r'[^a-zA-Z]', r' ', caption)\n",
    "    # Remove one word character\n",
    "    caption = re.sub(r'\\b[a-zA-Z]\\b', r' ', caption)\n",
    "    # Remove multiple spaces\n",
    "    caption = re.sub(r'\\s+', r' ', caption)\n",
    "    return caption.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "ZEgswwOSZX0R",
    "outputId": "9af07ee3-7d89-4c57-a469-9bc1a7574aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train image caption length: 38\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean captions\n",
    "# train_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), train_img_captions.items()))\n",
    "# valid_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), valid_img_captions.items()))\n",
    "# test_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), test_img_captions.items()))\n",
    "\n",
    "# Get the length of the longest caption\n",
    "all_train_captions = [*functools.reduce(lambda x, y: x + y, train_img_captions.values())]\n",
    "print('Max train image caption length:', max(map(len, map(str.split, all_train_captions))))\n",
    "plt.hist([*map(len, map(str.split, all_train_captions))])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "TK007mJhZX09",
    "outputId": "8b0c0d35-dc10-42ff-b60d-586e615f55a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:01<00:00, 17090.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary: 2,548\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 5\n",
    "MAX_LEN = 25\n",
    "all_train_captions = [*functools.reduce(lambda x, y: x + y, train_img_captions.values())]\n",
    "EN = Field(init_token=SOS_TOKEN,\n",
    "           eos_token=EOS_TOKEN,\n",
    "           fix_length=MAX_LEN,\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='en',\n",
    "           include_lengths=True)\n",
    "examples = [Example.fromlist(data=[caption], fields=[('caption', EN)])\n",
    "            for caption in tqdm.tqdm(all_train_captions)]\n",
    "captions_data = Dataset(examples, fields={'caption': EN})\n",
    "EN.build_vocab(captions_data,\n",
    "               min_freq=MIN_COUNT,\n",
    "               specials=[SOS_TOKEN, UNK_TOKEN, EOS_TOKEN, PAD_TOKEN])\n",
    "print(f'Length of vocabulary: {len(EN.vocab):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eEgNGFvJZX1I"
   },
   "outputs": [],
   "source": [
    "def caption_transform(caption):\n",
    "    if isinstance(caption, str):\n",
    "        return EN.process([EN.preprocess(caption)])\n",
    "    elif isinstance(caption, list):\n",
    "        return EN.process([*map(EN.preprocess, caption)])\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4DMbwJDZX1d"
   },
   "source": [
    "## 3.2. Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDBS9jH2ZX1g"
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (256, 256)\n",
    "\n",
    "img_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Lambda(lambda x: x / 255.),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bc7y35ZvZX1z"
   },
   "source": [
    "## 3.3. Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xx9cTSYwZX11"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, img_captions, split_set, img_transform, caption_transform, img_shape=(256, 256)):\n",
    "        assert split_set in {'TRAIN', 'VALID', 'TEST'}\n",
    "        self.data_path = data_path\n",
    "        self.img_captions = img_captions\n",
    "        self.split_set = split_set\n",
    "        self.img_transform = img_transform\n",
    "        self.caption_transform = caption_transform\n",
    "        self.img_shape = img_shape\n",
    "        self.ids = list(sorted(self.img_captions.keys()))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "\n",
    "        img = Image.open(os.path.join(self.data_path, img_id)).convert('RGB')\n",
    "        img = self.img_transform(img.resize(self.img_shape))\n",
    "\n",
    "        targets = self.img_captions[img_id]\n",
    "        targets = self.caption_transform(targets)\n",
    "        \n",
    "        idx = np.random.randint(len(targets))\n",
    "        target = (targets[0][:, idx], targets[1][idx])\n",
    "        \n",
    "        if self.split_set is 'TRAIN':\n",
    "            return img, target\n",
    "        else:\n",
    "            return img, target, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1lMLIU4ZX2X"
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
    "                                    img_captions=train_img_captions,\n",
    "                                    split_set='TRAIN',\n",
    "                                    img_transform=img_transform,\n",
    "                                    caption_transform=caption_transform)\n",
    "valid_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
    "                                    img_captions=valid_img_captions,\n",
    "                                    split_set='VALID',\n",
    "                                    img_transform=img_transform,\n",
    "                                    caption_transform=caption_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7AIJjCbZX20"
   },
   "source": [
    "# 4. Modeling\n",
    "\n",
    "## 4.1. Generate image features - ResNet Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fbvd220ZX22"
   },
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size=2048):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        resnet = torchvision.models.wide_resnet101_2(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "    def fine_tuning_resnet(self, fine_tune):\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            images: Tensor[batch_size, 3, img_size, img_size]\n",
    "        :return\n",
    "            out: Tensor[batch_size, 8, 8, hidden_size]\n",
    "        \"\"\"\n",
    "        out = self.resnet(images)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFUilrnBZX3C"
   },
   "outputs": [],
   "source": [
    "def test_encoder():\n",
    "    encoder = ResNetEncoder()\n",
    "    latent = encoder(torch.rand((10, 3, 256, 256)))\n",
    "    assert latent.size() == torch.Size([10, 8, 8, 2048]), latent.size() \n",
    "    \n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LExSos0FZX3U"
   },
   "source": [
    "## 4.2. Badhanau Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2tAPwvLZX3X"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = nn.Linear(enc_hidden_size, hidden_size)\n",
    "        self.W2 = nn.Linear(dec_hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, features, h_state):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            features:  Tensor[batch_size, num_pixels, enc_hidden_size]\n",
    "            h_state: Tensor[batch_size, dec_hidden_size]\n",
    "        :return\n",
    "            context_vector: Tensor[batch_size, enc_hidden_size]\n",
    "            attention_weights: Tensor[batch_size, num_pixels]\n",
    "        \"\"\"\n",
    "        h_state = h_state.unsqueeze(1) # [batch_size, 1, dec_hidden_size]\n",
    "        score = F.elu(self.W1(features) + self.W2(h_state)) # [batch_size, num_pixels, hidden_size]\n",
    "        attention_weights = F.softmax(self.V(score), dim=1) # [batch_size, num_pixels, 1]\n",
    "        context_vector = attention_weights * features # [batch_size, num_pixels, enc_hidden_size]\n",
    "        context_vector = torch.sum(context_vector, dim=1) # [batch_size, enc_hidden_size]\n",
    "        return context_vector, attention_weights.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWzAiYL3ZX3o"
   },
   "outputs": [],
   "source": [
    "def test_attention():\n",
    "    attention = BahdanauAttention(enc_hidden_size=2048, dec_hidden_size=512, hidden_size=512)\n",
    "    context_vector, attention_weights = attention(torch.rand((10, 8*8, 2048)), torch.rand((10, 512)))\n",
    "    assert context_vector.size() == torch.Size([10, 2048])\n",
    "    assert attention_weights.size() == torch.Size([10, 8*8])\n",
    "    \n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R62bBm1OZX32"
   },
   "source": [
    "## 4.3. Generate captions - LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdFH5HsPZX34"
   },
   "outputs": [],
   "source": [
    "class DecoderWithBahdanauAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_hidden_size, attn_hidden_size, hidden_size, embedding_size, vocab_size, dropout):\n",
    "        super(DecoderWithBahdanauAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size + enc_hidden_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(enc_hidden_size, hidden_size, attn_hidden_size)\n",
    "        self.f_beta = nn.Linear(hidden_size, enc_hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "        \n",
    "    def fine_tuning_embeddings(self, fine_tune=False):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "        \n",
    "    def forward(self, input_word_index, h_state, c_state, enc_outputs):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            input_word_index: Tensor[batch_size,]\n",
    "            h_state: Tensor[1, batch_size, hidden_size]\n",
    "            c_state: Tensor[1, batch_size, hidden_size]\n",
    "            enc_outputs: Tensor[batch_size, num_pixels, enc_hidden_size]\n",
    "        :return\n",
    "            logit: Tensor[batch_size, vocab_size]\n",
    "            h_state: Tensor[1, batch_size, hidden_size]\n",
    "            c_state: Tensor[1, batch_size, hidden_size]\n",
    "            attention_weights: Tensor[batch_size, num_pixels]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_word_index)  # [batch_size, embedding_size]\n",
    "        context_vector, attention_weights = self.attention(enc_outputs, h_state.squeeze(0))\n",
    "        # context_vector: Tensor[batch_size, enc_hidden_size]\n",
    "        # attention_weights: Tensor[batch_size, num_pixels]\n",
    "        \n",
    "        gate = torch.sigmoid(self.f_beta(h_state))  # [1, batch_size, enc_hidden_size], Gating scalar\n",
    "        context_vector = gate.squeeze(0) * context_vector # [batch_size, enc_hidden_size]\n",
    "        \n",
    "        x = torch.cat((embedded, context_vector), dim=1) # [batch_size, embedding_size + enc_hidden_size]\n",
    "        output, (h_state, c_state) = self.lstm(x.unsqueeze(0), (h_state, c_state))\n",
    "        # output: [1, batch_size, hidden_size]\n",
    "        # h_state: [1, batch_size, hidden_size]\n",
    "        # c_state: [1, batch_size, hidden_size]\n",
    "        \n",
    "        logit = self.fc(self.dropout(output)) # [1, batch_size, vocab_size]\n",
    "        logit = logit.squeeze(0) # [batch_size, vocab_size]\n",
    "        return logit, h_state, c_state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1hWqJPYnZX4E"
   },
   "outputs": [],
   "source": [
    "def test_decoder():\n",
    "    decoder = DecoderWithBahdanauAttention(enc_hidden_size=2048,\n",
    "                                           attn_hidden_size=512,\n",
    "                                           hidden_size=512,\n",
    "                                           embedding_size=512,\n",
    "                                           vocab_size=1000,\n",
    "                                           dropout=0.5)\n",
    "    logit, h_state, c_state, attention_weights = decoder(torch.randint(low=0, high=1000, size=(10,)),\n",
    "                                                         torch.rand((1, 10, 512)),\n",
    "                                                         torch.rand((1, 10, 512)),\n",
    "                                                         torch.rand((10, 14*14, 2048)))\n",
    "    assert logit.size() == torch.Size([10, 1000])\n",
    "    assert h_state.size() == torch.Size([1, 10, 512])\n",
    "    assert c_state.size() == torch.Size([1, 10, 512])\n",
    "    assert attention_weights.size() == torch.Size([10, 14*14])\n",
    "    \n",
    "test_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZHfTeBkUZX4P"
   },
   "source": [
    "## 4.4. Putting all together - AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlZGDdL9ZX4Q"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.init_h0 = nn.Linear(encoder.hidden_size, decoder.hidden_size)\n",
    "        self.init_c0 = nn.Linear(encoder.hidden_size, decoder.hidden_size)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, images, target_sequences, sequence_lengths, tf_ratio):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            images: Tensor[batch_size, 3, img_size, img_size]\n",
    "            target_sequences: Tensor[batch_size, seq_len]\n",
    "            sequence_lengths: Tensor[batch_size,]\n",
    "            tf_ratio: float\n",
    "        :return\n",
    "            logits: Tensor[max(decode_lengths), batch_size, vocab_size]\n",
    "            logits: Tensor[batch_size, max(decode_lengths), num_pixels]\n",
    "            sorted_target_sequences: Tensor[seq_len, batch_size]\n",
    "            sorted_decode_lengths: list[seq_len]\n",
    "            sorted_indices: list[batch_size]\n",
    "        \"\"\"\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Encoding\n",
    "        image_features = self.encoder(images) # [batch_size, 14, 14, hidden_size]\n",
    "        image_features = image_features.view(batch_size, -1, self.encoder.hidden_size) # [batch_size, num_pixels, enc_hidden_size]\n",
    "        num_pixels = image_features.size(1)\n",
    "        \n",
    "        # Sort the batch by decreasing lengths\n",
    "        sorted_sequence_lengths, sorted_indices = torch.sort(sequence_lengths, dim=0, descending=True)\n",
    "        sorted_image_features = image_features[sorted_indices] # [batch_size, num_pixels, enc_hidden_size]\n",
    "        sorted_target_sequences = target_sequences[sorted_indices] # [seq_len, batch_size]\n",
    "        \n",
    "        # Init hidden and memory states\n",
    "        mean_image_features = sorted_image_features.mean(dim=1) # [batch_size, enc_hidden_size]\n",
    "        h_state, c_state = self.init_h0(mean_image_features), self.init_c0(mean_image_features) # [batch_size, dec_hidden_size]\n",
    "        h_state, c_state = h_state.unsqueeze(0), c_state.unsqueeze(0) # [1, batch_size, dec_hidden_size]\n",
    "        \n",
    "        # We won't decode at the <eos> position, since we've finished generating as soon as we generate <eos>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        sorted_decode_lengths = (sorted_sequence_lengths - 1).tolist()\n",
    "        \n",
    "        # Decoding\n",
    "        logits = torch.zeros(max(sorted_decode_lengths), batch_size, self.decoder.vocab_size).to(self.device)\n",
    "        alphas = torch.zeros(batch_size, max(sorted_decode_lengths), num_pixels).to(self.device)\n",
    "        last = None\n",
    "        for t in range(max(sorted_decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in sorted_decode_lengths])\n",
    "            \n",
    "            if last is not None:\n",
    "                if random.random() < tf_ratio:\n",
    "                    in_ = last[:batch_size_t]\n",
    "                else:\n",
    "                    in_ = sorted_target_sequences[:batch_size_t, t]\n",
    "            else:\n",
    "                in_ = sorted_target_sequences[:batch_size_t, t]\n",
    "            \n",
    "            logit, h_state, c_state, attention_weights = self.decoder(in_,\n",
    "                                                                      h_state[:, :batch_size_t, :],\n",
    "                                                                      c_state[:, :batch_size_t, :],\n",
    "                                                                      sorted_image_features[:batch_size_t, :, :])\n",
    "            logits[t, :batch_size_t, :] = logit\n",
    "            alphas[:batch_size_t, t, :] = attention_weights\n",
    "            last = torch.argmax(F.softmax(logit, dim=1), dim=1) # [batch_size,]\n",
    "        \n",
    "        return logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xL8sy2MsZX4o"
   },
   "outputs": [],
   "source": [
    "def test_autoencoder():\n",
    "    encoder = ResNetEncoder()\n",
    "    decoder = DecoderWithBahdanauAttention(enc_hidden_size=2048,\n",
    "                                           attn_hidden_size=512,\n",
    "                                           hidden_size=512,\n",
    "                                           embedding_size=512,\n",
    "                                           vocab_size=1000,\n",
    "                                           dropout=0.5)\n",
    "    autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, device='cpu')\n",
    "    logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = \\\n",
    "        autoencoder(torch.rand((10, 3, 256, 256)),\n",
    "                    torch.randint(low=0, high=1000, size=(10, 25)),\n",
    "                    torch.randint(low=5, high=26, size=(10,)), 0.5)\n",
    "    assert logits.size() == torch.Size([max(sorted_decode_lengths), 10, 1000])\n",
    "    assert alphas.size() == torch.Size([10, max(sorted_decode_lengths), 8*8])\n",
    "    assert len(sorted_decode_lengths) == 10\n",
    "    assert sorted_target_sequences.size() == torch.Size([10, 25])\n",
    "    assert len(sorted_indices) == 10\n",
    "    \n",
    "test_autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-Ph4v-GZX43"
   },
   "source": [
    "# 5. Training\n",
    "\n",
    "## 5.1. Training routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Twx29pVuZX46"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def init_embeddings(embeddings):\n",
    "    bias = np.sqrt(3.0 / embeddings.size(1))\n",
    "    torch.nn.init.uniform_(embeddings, -bias, bias)\n",
    "\n",
    "def load_embeddings(nlp, field):\n",
    "    embeddings = torch.FloatTensor(len(field.vocab), 300)\n",
    "    init_embeddings(embeddings)\n",
    "    for token, index in tqdm.tqdm(field.vocab.stoi.items()):\n",
    "        token = nlp(token)\n",
    "        if token.has_vector:\n",
    "            embeddings[index] = torch.tensor(token.vector, dtype=torch.float32)\n",
    "    return embeddings\n",
    "\n",
    "def clip_gradient(optimizer, grad_clip):\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "\n",
    "def save_checkpoint(model, optimizer, data_name, epoch, last_improv, bleu4, is_best):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'bleu-4': bleu4,\n",
    "        'last_improv': last_improv,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    if not os.path.exists('./checkpoint'):\n",
    "        !mkdir ./checkpoint\n",
    "    torch.save(state, './checkpoint/' + data_name + '.pt')\n",
    "    if is_best:\n",
    "        torch.save(state, './checkpoint/' + 'BEST_' + data_name + '.pt')\n",
    "\n",
    "        \n",
    "class AvgMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def adjust_lr(optimizer, shrink_factor):\n",
    "    print(\"\\nDecaying learning rate.\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
    "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
    "    \n",
    "def accuracy(outputs, target_sequences, k=5):\n",
    "    batch_size = outputs.size(1)\n",
    "    _, indices = outputs.topk(k, dim=1, largest=True, sorted=True)\n",
    "    correct = indices.eq(target_sequences.view(-1, 1).expand_as(indices))\n",
    "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
    "    return correct_total.item() * (100.0 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gPf9q93ZX5L"
   },
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, criterion, loader, epoch, grad_clip, alpha_c, tf_ratio, device):\n",
    "    loss_tracker, acc_tracker = AvgMeter(), AvgMeter()\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "    for i, (images, (target_sequences, sequence_lengths)) in pbar:\n",
    "        images = images.to(device)\n",
    "        target_sequences = target_sequences.to(device)\n",
    "        sequence_lengths = sequence_lengths.to(device)\n",
    "        # Forward prop.\n",
    "        logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = model(images, target_sequences, sequence_lengths, tf_ratio)\n",
    "        # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
    "        sorted_target_sequences = sorted_target_sequences[:, 1:]\n",
    "        # Remove paddings\n",
    "        logits = pack_padded_sequence(logits, sorted_decode_lengths).data\n",
    "        sorted_target_sequences = pack_padded_sequence(sorted_target_sequences, sorted_decode_lengths, batch_first=True).data\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, sorted_target_sequences)\n",
    "        # Add doubly stochastic attention regularization\n",
    "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "        # Back prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Track metrics\n",
    "        loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
    "        acc_tracker.update(accuracy(logits, sorted_target_sequences, 5), sum(sorted_decode_lengths))\n",
    "        # Update progressbar description\n",
    "        pbar.set_description(f'Epoch: {epoch + 1:03d} - loss: {loss_tracker.avg:.3f} - acc: {acc_tracker.avg:.3f}%')\n",
    "    return loss_tracker.avg, acc_tracker.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npl8hB_QZX5m"
   },
   "outputs": [],
   "source": [
    "def validate(model, criterion, loader, field, epoch, alpha_c, device):\n",
    "    references, hypotheses = [], []\n",
    "    loss_tracker, acc_tracker = AvgMeter(), AvgMeter()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "        for i, (images, (target_sequences, sequence_lengths), (all_target_sequences, all_sequence_lengths)) in pbar:\n",
    "            images = images.to(device)\n",
    "            target_sequences = target_sequences.to(device)\n",
    "            sequence_lengths = sequence_lengths.to(device)\n",
    "            all_target_sequences = all_target_sequences.to(device)\n",
    "            all_sequence_lengths = all_sequence_lengths.to(device)\n",
    "            # Forward prop.\n",
    "            logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = model(images, target_sequences, sequence_lengths, 0)\n",
    "            # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
    "            sorted_target_sequences = sorted_target_sequences[:, 1:]\n",
    "            # Remove paddings\n",
    "            logits_copy = logits.clone()\n",
    "            logits = pack_padded_sequence(logits, sorted_decode_lengths).data\n",
    "            sorted_target_sequences = pack_padded_sequence(sorted_target_sequences, sorted_decode_lengths, batch_first=True).data\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, sorted_target_sequences)\n",
    "            # Add doubly stochastic attention regularization\n",
    "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "            # Track metrics\n",
    "            loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
    "            acc_tracker.update(accuracy(logits, sorted_target_sequences, 5), sum(sorted_decode_lengths))\n",
    "            # Update references\n",
    "            all_sorted_target_sequences = all_target_sequences[sorted_indices] # Because images were sorted in the decoder\n",
    "            for j in range(all_sorted_target_sequences.size(0)):\n",
    "                img_caps = all_sorted_target_sequences[j].t().tolist()\n",
    "                # Remove <sos> and <pad> tokens\n",
    "                img_caps = [*map(lambda c: [field.vocab.itos[w] for w in c\n",
    "                                            if w not in (field.vocab.stoi[field.init_token],\n",
    "                                                         field.vocab.stoi[field.pad_token])], img_caps)]\n",
    "                references.append(img_caps)\n",
    "            # Update hypotheses\n",
    "            _, preds = torch.max(logits_copy, dim=2)\n",
    "            preds, temp_preds = preds.t().tolist(), []\n",
    "            for j, p in enumerate(preds):\n",
    "                temp_preds.append([*map(lambda w: field.vocab.itos[w], preds[j][:sorted_decode_lengths[j]])]) # Remove padding\n",
    "            hypotheses.extend(temp_preds)\n",
    "            # Update progressbar description\n",
    "            pbar.set_description(f'Epoch: {epoch + 1:03d} - val_loss: {loss_tracker.avg:.3f} - val_acc: {acc_tracker.avg:.3f}%')\n",
    "        # Calculate BLEU-4 score\n",
    "        bleu4 = bleu_score(hypotheses, references, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "    return loss_tracker.avg, acc_tracker.avg, bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0N1p6-xkZX56"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, valid_loader, field, alpha_c, start_epoch, n_epochs, grad_clip, tf_ratio, device, model_name, last_improv):\n",
    "    history, best_bleu = {\n",
    "        'acc': [],\n",
    "        'loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_loss': [],\n",
    "        'bleu4': []\n",
    "    }, 0.\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        # Stop training if no improvment since last 4 epochs\n",
    "        if last_improv == 4:\n",
    "            print('Training Finished - The model has stopped improving since last 4 epochs')\n",
    "            break\n",
    "        # Decay LR if no improvment\n",
    "        if last_improv > 0:\n",
    "            adjust_lr(optimizer, 0.8)\n",
    "        # Train step\n",
    "        loss, acc = train_step(model=model, optimizer=optimizer, criterion=criterion,\n",
    "                               loader=train_loader, epoch=epoch, grad_clip=grad_clip,\n",
    "                               alpha_c=alpha_c, tf_ratio=tf_ratio, device=device)\n",
    "        # Validation step\n",
    "        val_loss, val_acc, bleu4 = validate(model=model, criterion=criterion, loader=valid_loader,\n",
    "                                            field=field, epoch=epoch, alpha_c=alpha_c, device=device)\n",
    "        # Update history dict\n",
    "        history['acc'].append(acc)\n",
    "        history['loss'].append(loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['bleu4'].append(bleu4)\n",
    "        # Print BLEU score\n",
    "        text = f'BLEU-4: {bleu4*100:.3f}%'\n",
    "        if best_bleu > bleu4:\n",
    "            last_improv += 1\n",
    "            text += f' - Last improvement since {last_improv} epoch(s)'\n",
    "        else:\n",
    "            best_bleu, last_improv = bleu4, 0\n",
    "        print(text)\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(model=model, optimizer=optimizer, data_name=model_name, epoch=epoch, last_improv=last_improv, bleu4=bleu4, is_best=bleu4 >= best_bleu)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nN_kRRgjZX6B"
   },
   "source": [
    "## 5.2. Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQD6jkdfZX6G"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'Flickr_8k'\n",
    "ENCODER_HIDDEN_SIZE = 2048\n",
    "ATTENTION_SIZE = 512\n",
    "DECODER_HIDDEN_SIZE = 512\n",
    "EMBEDDING_SIZE = 300\n",
    "DROPOUT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EzLwRaLMZX6L"
   },
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "batch_size = 32\n",
    "lr = 3e-5\n",
    "grad_clip = 5.\n",
    "alpha_c = 1.\n",
    "tf_ratio = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tpI9IT-PZX6a",
    "outputId": "c3ddeaa7-3250-43e9-b8d2-64b65a4e70c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2548/2548 [00:18<00:00, 137.15it/s]\n"
     ]
    }
   ],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_lg')\n",
    "embeddings = load_embeddings(nlp=spacy_nlp, field=EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WM1wCpP6ZX6i",
    "outputId": "44943ebb-e57f-40d2-cf36-e20679545a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of the model: 136,587,749\n"
     ]
    }
   ],
   "source": [
    "encoder = ResNetEncoder()\n",
    "encoder.fine_tuning_resnet(fine_tune=True)\n",
    "decoder = DecoderWithBahdanauAttention(enc_hidden_size=ENCODER_HIDDEN_SIZE,\n",
    "                                       attn_hidden_size=ATTENTION_SIZE,\n",
    "                                       hidden_size=DECODER_HIDDEN_SIZE,\n",
    "                                       embedding_size=EMBEDDING_SIZE,\n",
    "                                       vocab_size=len(EN.vocab),\n",
    "                                       dropout=DROPOUT)\n",
    "decoder.load_pretrained_embeddings(embeddings)\n",
    "decoder.fine_tuning_embeddings(fine_tune=True)\n",
    "autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, device=DEVICE).to(DEVICE)\n",
    "optimizer = optim.RMSprop(params=autoencoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(f'Number of parameters of the model: {count_parameters(autoencoder):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QjnxhtJ2ZX6t"
   },
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           num_workers=N_WORKERS,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                           num_workers=N_WORKERS,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True, \n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "IeYO4y0EZX65",
    "outputId": "7eb11200-8d72-4b05-eaad-c97afa8e6a71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 001 - loss: 5.664 - acc: 6.237%: 100%|██████████| 188/188 [02:44<00:00,  1.14it/s]\n",
      "Epoch: 001 - val_loss: 5.257 - val_acc: 6.780%: 100%|██████████| 32/32 [00:11<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 0.000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 002 - loss: 5.201 - acc: 6.952%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 002 - val_loss: 5.102 - val_acc: 7.093%: 100%|██████████| 32/32 [00:09<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 4.716%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 003 - loss: 5.009 - acc: 7.332%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 003 - val_loss: 4.949 - val_acc: 7.363%: 100%|██████████| 32/32 [00:09<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 5.145%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 004 - loss: 4.836 - acc: 7.731%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 004 - val_loss: 4.810 - val_acc: 7.681%: 100%|██████████| 32/32 [00:09<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 6.213%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 005 - loss: 4.698 - acc: 8.032%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 005 - val_loss: 4.729 - val_acc: 7.902%: 100%|██████████| 32/32 [00:09<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 6.816%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 006 - loss: 4.572 - acc: 8.279%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 006 - val_loss: 4.651 - val_acc: 8.205%: 100%|██████████| 32/32 [00:09<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 7.418%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 007 - loss: 4.467 - acc: 8.590%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 007 - val_loss: 4.575 - val_acc: 8.379%: 100%|██████████| 32/32 [00:09<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 7.790%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 008 - loss: 4.364 - acc: 8.879%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 008 - val_loss: 4.496 - val_acc: 8.597%: 100%|██████████| 32/32 [00:09<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 8.528%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 009 - loss: 4.250 - acc: 9.087%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 009 - val_loss: 4.465 - val_acc: 8.684%: 100%|██████████| 32/32 [00:09<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 9.629%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 010 - loss: 4.174 - acc: 9.312%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 010 - val_loss: 4.409 - val_acc: 8.936%: 100%|██████████| 32/32 [00:09<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 10.207%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 011 - loss: 4.100 - acc: 9.521%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 011 - val_loss: 4.378 - val_acc: 8.866%: 100%|██████████| 32/32 [00:09<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 10.787%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 012 - loss: 4.011 - acc: 9.671%: 100%|██████████| 188/188 [02:40<00:00,  1.17it/s]\n",
      "Epoch: 012 - val_loss: 4.316 - val_acc: 9.214%: 100%|██████████| 32/32 [00:10<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 11.212%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 013 - loss: 3.932 - acc: 9.941%: 100%|██████████| 188/188 [02:42<00:00,  1.16it/s]\n",
      "Epoch: 013 - val_loss: 4.296 - val_acc: 9.139%: 100%|██████████| 32/32 [00:10<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 11.055% - Last improvement since 1 epoch(s)\n",
      "\n",
      "Decaying learning rate.\n",
      "The new learning rate is 0.000024\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 014 - loss: 3.868 - acc: 10.078%: 100%|██████████| 188/188 [02:39<00:00,  1.18it/s]\n",
      "Epoch: 014 - val_loss: 4.276 - val_acc: 9.220%: 100%|██████████| 32/32 [00:09<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 10.506% - Last improvement since 2 epoch(s)\n",
      "\n",
      "Decaying learning rate.\n",
      "The new learning rate is 0.000019\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 015 - loss: 3.808 - acc: 10.254%: 100%|██████████| 188/188 [02:39<00:00,  1.18it/s]\n",
      "Epoch: 015 - val_loss: 4.291 - val_acc: 9.133%: 100%|██████████| 32/32 [00:09<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 10.549% - Last improvement since 3 epoch(s)\n"
     ]
    }
   ],
   "source": [
    "history = train(model=autoencoder,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                train_loader=train_loader,\n",
    "                valid_loader=valid_loader,\n",
    "                field=EN,\n",
    "                alpha_c=alpha_c,\n",
    "                start_epoch=0,\n",
    "                n_epochs=n_epochs,\n",
    "                grad_clip=grad_clip,\n",
    "                tf_ratio=tf_ratio,\n",
    "                device=DEVICE,\n",
    "                model_name=MODEL_NAME,\n",
    "                last_improv=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "3bebBmWmZX7A",
    "outputId": "9bcf2d8b-1f0d-41b7-fd81-d289562473a2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(history['loss'], label='train')\n",
    "axes[0].plot(history['val_loss'], label='valid')\n",
    "axes[0].set_title('Loss history')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True)\n",
    "axes[1].plot(history['acc'], label='train')\n",
    "axes[1].plot(history['val_acc'], label='valid')\n",
    "axes[1].plot(np.array(history['bleu4']) * 100., label='BLEU-4')\n",
    "axes[1].set_title('Accuracy & BLEU-4 history')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy & BLEU-4 (%)')\n",
    "axes[1].grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8s2Dp7RPZX7i"
   },
   "source": [
    "# 6. Evaluation - BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kXqLgQHZX8j"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, beam_size, field, max_len, device):\n",
    "    references, hypotheses = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "        for i, (images, (target_sequences, sequence_lengths), (all_target_sequences, all_sequence_lengths)) in pbar:\n",
    "            images = images.to(device)\n",
    "            target_sequences = target_sequences.to(device)\n",
    "            sequence_lengths = sequence_lengths.to(device)\n",
    "            all_target_sequences = all_target_sequences.to(device)\n",
    "            all_sequence_lengths = all_sequence_lengths.to(device)\n",
    "            \n",
    "            k = beam_size\n",
    "            \n",
    "            # Top k previous token indices at each step\n",
    "            topk_prev_tokens = torch.LongTensor([[field.vocab.stoi[field.init_token]]] * k).to(device)  # [k, 1]\n",
    "\n",
    "            # Top k sequences\n",
    "            topk_sequences = topk_prev_tokens  # [k, 1]\n",
    "\n",
    "            # Top k sequences' logps\n",
    "            topk_logps = torch.zeros(k, 1).to(device)  # [k, 1]\n",
    "            \n",
    "            # Complete sequences and logps\n",
    "            complete_sequences, complete_sequence_logps = [], []\n",
    "            \n",
    "            # Encoding\n",
    "            image_features = model.encoder(images) # [1, 14, 14, hidden_size]\n",
    "            image_features = image_features.view(1, -1, model.encoder.hidden_size) # [1, num_pixels, enc_hidden_size]\n",
    "            image_features = image_features.expand(k, -1, -1) # [k, num_pixels, enc_hidden_size]\n",
    "            \n",
    "            # Init hidden and memory states\n",
    "            mean_image_features = image_features.mean(dim=1) # [k, enc_hidden_size]\n",
    "            h_state, c_state = model.init_h0(mean_image_features), model.init_c0(mean_image_features) # [k, dec_hidden_size]\n",
    "\n",
    "            # Decoding\n",
    "            step = 1\n",
    "            while True:\n",
    "                if len(h_state.shape) < 3:\n",
    "                    h_state, c_state = h_state.unsqueeze(0), c_state.unsqueeze(0) # [1, k, dec_hidden_size]\n",
    "                    \n",
    "                logit, h_state, c_state, _ = model.decoder(topk_prev_tokens.squeeze(1), h_state, c_state, image_features)\n",
    "                \n",
    "                # Get scores\n",
    "                logp = F.log_softmax(logit, dim=1) # [k, vocab_size]\n",
    "                \n",
    "                # Extend\n",
    "                logp = topk_logps.expand_as(logp) + logp  # [k, vocab_size]\n",
    "                \n",
    "                # At the 1st step, the score is 0\n",
    "                if step == 1:\n",
    "                    topk_logps, topk_tokens = logp[0].topk(k, 0, True, True)  # [k,]\n",
    "                else:\n",
    "                    # Unroll and find top logp, and their unrolled indices\n",
    "                    topk_logps, topk_tokens = logp.view(-1).topk(k, 0, True, True)  # [k,]\n",
    "                    \n",
    "                # Convert unrolled indices to actual indices of logp\n",
    "                prev_tokens = topk_tokens // model.decoder.vocab_size  # [k,]\n",
    "                next_tokens = topk_tokens % model.decoder.vocab_size  # [k,]\n",
    "                \n",
    "                # Add new indices to topk_sequences\n",
    "                topk_sequences = torch.cat((topk_sequences[prev_tokens], next_tokens.unsqueeze(1)), dim=1) # [k, step + 1]\n",
    "                \n",
    "                # Get the complete and incomplete sequences\n",
    "                incomplete_indices = [indice for indice, next_token in enumerate(next_tokens) if next_token != field.vocab.stoi[field.eos_token]]\n",
    "                complete_indices = list(set(range(len(next_tokens))) - set(incomplete_indices))\n",
    "                \n",
    "                # Set aside complete sequences\n",
    "                if len(complete_indices) > 0:\n",
    "                    complete_sequences.extend(topk_sequences[complete_indices].tolist())\n",
    "                    complete_sequence_logps.extend(topk_logps[complete_indices])\n",
    "                    \n",
    "                # Reduce beam length accordingly\n",
    "                k -= len(complete_indices) \n",
    "                \n",
    "                # Proceed with incomplete sequences\n",
    "                if k == 0:\n",
    "                    break\n",
    "                    \n",
    "                topk_sequences = topk_sequences[incomplete_indices]\n",
    "                h_state = h_state[:, prev_tokens[incomplete_indices], :]\n",
    "                c_state = c_state[:, prev_tokens[incomplete_indices], :]\n",
    "                image_features = image_features[prev_tokens[incomplete_indices]]\n",
    "                topk_logps = topk_logps[incomplete_indices].unsqueeze(1)\n",
    "                topk_prev_tokens = next_tokens[incomplete_indices].unsqueeze(1)\n",
    "                \n",
    "                # Break if things have been going on too long\n",
    "                if step > max_len:\n",
    "                    if len(complete_indices) == 0:\n",
    "                        complete_sequences.extend(topk_sequences.tolist())\n",
    "                        complete_sequence_logps.extend(topk_logps[incomplete_indices])\n",
    "                    break\n",
    "                    \n",
    "                # Update step\n",
    "                step += 1\n",
    "            \n",
    "            i = complete_sequence_logps.index(max(complete_sequence_logps))\n",
    "            sequence = complete_sequences[i]\n",
    "            \n",
    "            # Update references\n",
    "            for j in range(all_target_sequences.size(0)):\n",
    "                img_caps = all_target_sequences[j].t().tolist()\n",
    "                img_caps = [*map(lambda caption: [field.vocab.itos[token] for token in caption\n",
    "                                                  if token not in (field.vocab.stoi[field.init_token],\n",
    "                                                                   field.vocab.stoi[field.pad_token])], img_caps)]\n",
    "                references.append(img_caps)\n",
    "                \n",
    "            # Update hypotheses\n",
    "            hypotheses.append([field.vocab.itos[token] for token in sequence if token not in \n",
    "                               {field.vocab.stoi[field.init_token],\n",
    "                                field.vocab.stoi[field.eos_token],\n",
    "                                field.vocab.stoi[field.pad_token]}])\n",
    "\n",
    "            assert len(references) == len(hypotheses)\n",
    "        \n",
    "        # Calculate BLEU-4 score\n",
    "        bleu4 = bleu_score(hypotheses, references, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "    \n",
    "    return hypotheses, references, bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULooViEOZX8e"
   },
   "outputs": [],
   "source": [
    "test_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
    "                                   img_captions=test_img_captions,\n",
    "                                   split_set='TEST',\n",
    "                                   img_transform=img_transform,\n",
    "                                   caption_transform=caption_transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          num_workers=N_WORKERS,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True, \n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wC3H7uoPZX8J",
    "outputId": "a614a5c7-08f6-41de-a658-6bd7a0a53f84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.load_state_dict(torch.load('./checkpoint/BEST_Flickr_8k.pt').get('model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:37<00:00, 26.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 13.288% with beam_size=1\n",
      "CPU times: user 36.8 s, sys: 3.83 s, total: 40.6 s\n",
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_, _, bleu4 = evaluate(autoencoder.to(DEVICE),\n",
    "                       loader=test_loader,\n",
    "                       beam_size=1,\n",
    "                       field=EN,\n",
    "                       max_len=MAX_LEN,\n",
    "                       device=DEVICE)\n",
    "print(f'BLEU-4: {bleu4*100:.3f}% with beam_size=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 15.134% with beam_size=3\n",
      "CPU times: user 41.1 s, sys: 3.57 s, total: 44.7 s\n",
      "Wall time: 42.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_, _, bleu4 = evaluate(autoencoder.to(DEVICE),\n",
    "                       loader=test_loader,\n",
    "                       beam_size=3,\n",
    "                       field=EN,\n",
    "                       max_len=MAX_LEN,\n",
    "                       device=DEVICE)\n",
    "print(f'BLEU-4: {bleu4*100:.3f}% with beam_size=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:43<00:00, 23.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 15.028% with beam_size=5\n",
      "CPU times: user 42.7 s, sys: 3.63 s, total: 46.4 s\n",
      "Wall time: 44.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_, _, bleu4 = evaluate(autoencoder.to(DEVICE),\n",
    "                       loader=test_loader,\n",
    "                       beam_size=5,\n",
    "                       field=EN,\n",
    "                       max_len=MAX_LEN,\n",
    "                       device=DEVICE)\n",
    "print(f'BLEU-4: {bleu4*100:.3f}% with beam_size=5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZMIDy2mZX81"
   },
   "source": [
    "# 7. Inference - Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Neural Image Caption generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
