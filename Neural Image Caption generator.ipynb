{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Neural Image Caption generator.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dksifoua/Neural-Image-Caption-Generator/blob/master/Neural%20Image%20Caption%20generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKrQqejCZXyE",
        "colab_type": "text"
      },
      "source": [
        "# 1. Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8bMWPjTZXyI",
        "colab_type": "code",
        "outputId": "ae99572f-89d5-4860-dd16-b5bf7e9c8da3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jun  3 23:29:12 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdNqWsXXZxPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install --upgrade torchtext spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlsnqBPSZXyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import tqdm\n",
        "import time\n",
        "import random\n",
        "import functools\n",
        "import collections\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import multiprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import spacy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "import torchvision\n",
        "from torchtext.data import Example, Field, Dataset\n",
        "from torchtext.data.metrics import bleu_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO8NwIodZXy5",
        "colab_type": "code",
        "outputId": "0781ac6b-6e94-47db-d697-3d5c902bd41b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "SEED = 781\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {DEVICE}')\n",
        "\n",
        "N_WORKERS = multiprocessing.cpu_count()\n",
        "print(f'Number of CPUs: {N_WORKERS}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Number of CPUs: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2StBu8PtZXzG",
        "colab_type": "text"
      },
      "source": [
        "# 2. Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0emNzBNZXzK",
        "colab_type": "code",
        "outputId": "e836beaf-c0c3-44ce-8955-3ac2294114a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "if not os.path.exists('./data'):\n",
        "    !mkdir ./data\n",
        "    \n",
        "    !wget --no-check-certificate \\\n",
        "        https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip \\\n",
        "        -O ./data/Flickr8k_Dataset.zip\n",
        "    !unzip -q ./data/Flickr8k_Dataset.zip -d ./data\n",
        "    !rm -r ./data/Flickr8k_Dataset.zip\n",
        "\n",
        "    !wget --no-check-certificate \\\n",
        "        https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip \\\n",
        "        -O ./data/Flickr8k_text.zip\n",
        "    !unzip -q ./data/Flickr8k_text.zip -d ./data\n",
        "    !rm -r ./data/Flickr8k_text.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 27 µs, sys: 6 µs, total: 33 µs\n",
            "Wall time: 37.7 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd62CBG3ZXzV",
        "colab_type": "text"
      },
      "source": [
        "# 3. Data Preparation\n",
        "\n",
        "## 3.1. Text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud-2n9boZXzX",
        "colab_type": "code",
        "outputId": "8b1b73ab-ea01-4677-cd59-a619c411e432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!cat ./data/readme.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "If you use this corpus / data:\n",
            "\n",
            "Please cite: M. Hodosh, P. Young and J. Hockenmaier (2013) \"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics\", Journal of Artifical Intellegence Research, Volume 47, pages 853-899\n",
            "http://www.jair.org/papers/paper3994.html\n",
            "\n",
            "\n",
            "Captions, Dataset Splits, and Human Annotations :\n",
            "\n",
            "\n",
            "Flickr8k.token.txt - the raw captions of the Flickr8k Dataset . The first column is the ID of the caption which is \"image address # caption number\"\n",
            "\n",
            "Flickr8k.lemma.txt - the lemmatized version of the above captions \n",
            "\n",
            "Flickr_8k.trainImages.txt - The training images used in our experiments\n",
            "Flickr_8k.devImages.txt - The development/validation images used in our experiments\n",
            "Flickr_8k.testImages.txt - The test images used in our experiments\n",
            "\n",
            "\n",
            "ExpertAnnotations.txt is the expert judgments.  The first two columns are the image and caption IDs.  Caption IDs are <image file name>#<0-4>.  The next three columns are the expert judgments for that image-caption pair.  Scores range from 1 to 4, with a 1 indicating that the caption does not describe the image at all, a 2 indicating the caption describes minor aspects of the image but does not describe the image, a 3 indicating that the caption almost describes the image with minor mistakes, and a 4 indicating that the caption describes the image.\n",
            "\n",
            "\n",
            "CrowdFlowerAnnotations.txt contains the CrowdFlower judgments.  The first two columns are the image and caption IDs.  The third column is the percent of Yeses, the fourth column is the total number of Yeses, the fifth column is the total number of Noes.  A Yes means that the caption describes the image (possibly with minor mistakes), while a No means that the caption does not describe the image.  Each image-caption pair has a minimum of three judgments, but some may have more.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3RCqRt7ZXzn",
        "colab_type": "code",
        "outputId": "a37ebf9b-c63c-4d8c-d05d-bc810a66985e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_img_fn = [*map(str.strip, open('./data/Flickr_8k.trainImages.txt').readlines())]\n",
        "valid_img_fn = [*map(str.strip, open('./data/Flickr_8k.devImages.txt').readlines())]\n",
        "test_img_fn = [*map(str.strip, open('./data/Flickr_8k.testImages.txt').readlines())]\n",
        "\n",
        "img_captions = collections.defaultdict(lambda: [])\n",
        "with open('./data/Flickr8k.token.txt') as file:\n",
        "    for line in file.readlines():\n",
        "        img_fn, caption = line.strip().split('\\t')\n",
        "        img_captions[img_fn[:-2]].append(caption)\n",
        "        \n",
        "train_img_captions = dict(filter(lambda x: x[0] in train_img_fn, img_captions.items()))\n",
        "valid_img_captions = dict(filter(lambda x: x[0] in valid_img_fn, img_captions.items()))\n",
        "test_img_captions = dict(filter(lambda x: x[0] in test_img_fn, img_captions.items()))\n",
        "    \n",
        "print(f'Number of images: {len(img_captions):,}')\n",
        "print(f'Number of train images: {len(train_img_captions):,}')\n",
        "print(f'Number of valid images: {len(valid_img_captions):,}')\n",
        "print(f'Number of test images: {len(test_img_captions):,}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images: 8,092\n",
            "Number of train images: 6,000\n",
            "Number of valid images: 1,000\n",
            "Number of test images: 1,000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c1mm4rIZXzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Special tokens\n",
        "PAD_TOKEN = '<pad>'\n",
        "SOS_TOKEN = '<sos>'\n",
        "EOS_TOKEN = '<eos>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "\n",
        "def clean(caption):\n",
        "    # Remove non-alphabetical character\n",
        "    caption = re.sub(r'[^a-zA-Z]', r' ', caption)\n",
        "    # Remove one word character\n",
        "    caption = re.sub(r'\\b[a-zA-Z]\\b', r' ', caption)\n",
        "    # Remove multiple spaces\n",
        "    caption = re.sub(r'\\s+', r' ', caption)\n",
        "    return caption.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEgswwOSZX0R",
        "colab_type": "code",
        "outputId": "9af07ee3-7d89-4c57-a469-9bc1a7574aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Clean captions\n",
        "# train_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), train_img_captions.items()))\n",
        "# valid_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), valid_img_captions.items()))\n",
        "# test_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), test_img_captions.items()))\n",
        "\n",
        "# Get the length of the longest caption\n",
        "all_train_captions = [*functools.reduce(lambda x, y: x + y, train_img_captions.values())]\n",
        "print('Max train image caption length:', max(map(len, map(str.split, all_train_captions))))\n",
        "plt.hist([*map(len, map(str.split, all_train_captions))])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max train image caption length: 38\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK007mJhZX09",
        "colab_type": "code",
        "outputId": "8b0c0d35-dc10-42ff-b60d-586e615f55a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "MIN_COUNT = 5\n",
        "MAX_LEN = 25\n",
        "all_train_captions = [*functools.reduce(lambda x, y: x + y, train_img_captions.values())]\n",
        "EN = Field(init_token=SOS_TOKEN,\n",
        "           eos_token=EOS_TOKEN,\n",
        "           fix_length=MAX_LEN,\n",
        "           lower=True,\n",
        "           tokenize='spacy',\n",
        "           tokenizer_language='en',\n",
        "           include_lengths=True)\n",
        "examples = [Example.fromlist(data=[caption], fields=[('caption', EN)])\n",
        "            for caption in tqdm.tqdm(all_train_captions)]\n",
        "captions_data = Dataset(examples, fields={'caption': EN})\n",
        "EN.build_vocab(captions_data,\n",
        "               min_freq=MIN_COUNT,\n",
        "               specials=[SOS_TOKEN, UNK_TOKEN, EOS_TOKEN, PAD_TOKEN])\n",
        "print(f'Length of vocabulary: {len(EN.vocab):,}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [00:02<00:00, 14383.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of vocabulary: 2,548\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEgNGFvJZX1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def caption_transform(caption):\n",
        "    if isinstance(caption, str):\n",
        "        return EN.process([EN.preprocess(caption)])\n",
        "    elif isinstance(caption, list):\n",
        "        return EN.process([*map(EN.preprocess, caption)])\n",
        "    else:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4DMbwJDZX1d",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Image data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDBS9jH2ZX1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SHAPE = (256, 256)\n",
        "\n",
        "img_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Lambda(lambda x: x / 255.),\n",
        "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc7y35ZvZX1z",
        "colab_type": "text"
      },
      "source": [
        "## 3.3. Build datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx9cTSYwZX11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, data_path, img_captions, split_set, img_transform, caption_transform, img_shape=(256, 256)):\n",
        "        assert split_set in {'TRAIN', 'VALID', 'TEST'}\n",
        "        self.data_path = data_path\n",
        "        self.img_captions = img_captions\n",
        "        self.split_set = split_set\n",
        "        self.img_transform = img_transform\n",
        "        self.caption_transform = caption_transform\n",
        "        self.img_shape = img_shape\n",
        "        self.ids = list(sorted(self.img_captions.keys()))\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.ids[index]\n",
        "\n",
        "        img = Image.open(os.path.join(self.data_path, img_id)).convert('RGB')\n",
        "        img = self.img_transform(img.resize(self.img_shape))\n",
        "\n",
        "        targets = self.img_captions[img_id]\n",
        "        targets = self.caption_transform(targets)\n",
        "        \n",
        "        idx = np.random.randint(len(targets))\n",
        "        target = (targets[0][:, idx], targets[1][idx])\n",
        "        \n",
        "        if self.split_set is 'TRAIN':\n",
        "            return img, target\n",
        "        else:\n",
        "            return img, target, targets\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1lMLIU4ZX2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
        "                                    img_captions=train_img_captions,\n",
        "                                    split_set='TRAIN',\n",
        "                                    img_transform=img_transform,\n",
        "                                    caption_transform=caption_transform)\n",
        "valid_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
        "                                    img_captions=valid_img_captions,\n",
        "                                    split_set='VALID',\n",
        "                                    img_transform=img_transform,\n",
        "                                    caption_transform=caption_transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7AIJjCbZX20",
        "colab_type": "text"
      },
      "source": [
        "# 4. Modeling\n",
        "\n",
        "## 4.1. Generate image features - ResNet Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fbvd220ZX22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNetEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_size=2048):\n",
        "        super(ResNetEncoder, self).__init__()\n",
        "        resnet = torchvision.models.wide_resnet101_2(pretrained=True)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.hidden_size = hidden_size\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        \n",
        "    def fine_tuning_resnet(self, fine_tune):\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune\n",
        "        \n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        :param\n",
        "            images: Tensor[batch_size, 3, img_size, img_size]\n",
        "        :return\n",
        "            out: Tensor[batch_size, 8, 8, hidden_size]\n",
        "        \"\"\"\n",
        "        out = self.resnet(images)\n",
        "        out = out.permute(0, 2, 3, 1)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFUilrnBZX3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_encoder():\n",
        "    encoder = ResNetEncoder()\n",
        "    latent = encoder(torch.rand((10, 3, 256, 256)))\n",
        "    assert latent.size() == torch.Size([10, 8, 8, 2048]), latent.size() \n",
        "    \n",
        "test_encoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LExSos0FZX3U",
        "colab_type": "text"
      },
      "source": [
        "## 4.2. Badhanau Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2tAPwvLZX3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, enc_hidden_size, dec_hidden_size, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = nn.Linear(enc_hidden_size, hidden_size)\n",
        "        self.W2 = nn.Linear(dec_hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, features, h_state):\n",
        "        \"\"\"\n",
        "        :param\n",
        "            features:  Tensor[batch_size, num_pixels, enc_hidden_size]\n",
        "            h_state: Tensor[batch_size, dec_hidden_size]\n",
        "        :return\n",
        "            context_vector: Tensor[batch_size, enc_hidden_size]\n",
        "            attention_weights: Tensor[batch_size, num_pixels]\n",
        "        \"\"\"\n",
        "        h_state = h_state.unsqueeze(1) # [batch_size, 1, dec_hidden_size]\n",
        "        score = F.elu(self.W1(features) + self.W2(h_state)) # [batch_size, num_pixels, hidden_size]\n",
        "        attention_weights = F.softmax(self.V(score), dim=1) # [batch_size, num_pixels, 1]\n",
        "        context_vector = attention_weights * features # [batch_size, num_pixels, enc_hidden_size]\n",
        "        context_vector = torch.sum(context_vector, dim=1) # [batch_size, enc_hidden_size]\n",
        "        return context_vector, attention_weights.squeeze(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWzAiYL3ZX3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_attention():\n",
        "    attention = BahdanauAttention(enc_hidden_size=2048, dec_hidden_size=512, hidden_size=512)\n",
        "    context_vector, attention_weights = attention(torch.rand((10, 8*8, 2048)), torch.rand((10, 512)))\n",
        "    assert context_vector.size() == torch.Size([10, 2048])\n",
        "    assert attention_weights.size() == torch.Size([10, 8*8])\n",
        "    \n",
        "test_attention()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R62bBm1OZX32",
        "colab_type": "text"
      },
      "source": [
        "## 4.3. Generate captions - LSTM Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdFH5HsPZX34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderWithBahdanauAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, enc_hidden_size, attn_hidden_size, hidden_size, embedding_size, vocab_size, dropout):\n",
        "        super(DecoderWithBahdanauAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.lstm = nn.LSTM(embedding_size + enc_hidden_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(enc_hidden_size, hidden_size, attn_hidden_size)\n",
        "        self.f_beta = nn.Linear(hidden_size, enc_hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "        \n",
        "    def fine_tuning_embeddings(self, fine_tune=False):\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "        \n",
        "    def forward(self, input_word_index, h_state, c_state, enc_outputs):\n",
        "        \"\"\"\n",
        "        :param\n",
        "            input_word_index: Tensor[batch_size,]\n",
        "            h_state: Tensor[1, batch_size, hidden_size]\n",
        "            c_state: Tensor[1, batch_size, hidden_size]\n",
        "            enc_outputs: Tensor[batch_size, num_pixels, enc_hidden_size]\n",
        "        :return\n",
        "            logit: Tensor[batch_size, vocab_size]\n",
        "            h_state: Tensor[1, batch_size, hidden_size]\n",
        "            c_state: Tensor[1, batch_size, hidden_size]\n",
        "            attention_weights: Tensor[batch_size, num_pixels]\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_word_index)  # [batch_size, embedding_size]\n",
        "        context_vector, attention_weights = self.attention(enc_outputs, h_state.squeeze(0))\n",
        "        # context_vector: Tensor[batch_size, enc_hidden_size]\n",
        "        # attention_weights: Tensor[batch_size, num_pixels]\n",
        "        \n",
        "        gate = torch.sigmoid(self.f_beta(h_state))  # [1, batch_size, enc_hidden_size], Gating scalar\n",
        "        context_vector = gate.squeeze(0) * context_vector # [batch_size, enc_hidden_size]\n",
        "        \n",
        "        x = torch.cat((embedded, context_vector), dim=1) # [batch_size, embedding_size + enc_hidden_size]\n",
        "        output, (h_state, c_state) = self.lstm(x.unsqueeze(0), (h_state, c_state))\n",
        "        # output: [1, batch_size, hidden_size]\n",
        "        # h_state: [1, batch_size, hidden_size]\n",
        "        # c_state: [1, batch_size, hidden_size]\n",
        "        \n",
        "        logit = self.fc(self.dropout(output)) # [1, batch_size, vocab_size]\n",
        "        logit = logit.squeeze(0) # [batch_size, vocab_size]\n",
        "        return logit, h_state, c_state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hWqJPYnZX4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_decoder():\n",
        "    decoder = DecoderWithBahdanauAttention(enc_hidden_size=2048,\n",
        "                                           attn_hidden_size=512,\n",
        "                                           hidden_size=512,\n",
        "                                           embedding_size=512,\n",
        "                                           vocab_size=1000,\n",
        "                                           dropout=0.5)\n",
        "    logit, h_state, c_state, attention_weights = decoder(torch.randint(low=0, high=1000, size=(10,)),\n",
        "                                                         torch.rand((1, 10, 512)),\n",
        "                                                         torch.rand((1, 10, 512)),\n",
        "                                                         torch.rand((10, 14*14, 2048)))\n",
        "    assert logit.size() == torch.Size([10, 1000])\n",
        "    assert h_state.size() == torch.Size([1, 10, 512])\n",
        "    assert c_state.size() == torch.Size([1, 10, 512])\n",
        "    assert attention_weights.size() == torch.Size([10, 14*14])\n",
        "    \n",
        "test_decoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHfTeBkUZX4P",
        "colab_type": "text"
      },
      "source": [
        "## 4.4. Putting all together - AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlZGDdL9ZX4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.init_h0 = nn.Linear(encoder.hidden_size, decoder.hidden_size)\n",
        "        self.init_c0 = nn.Linear(encoder.hidden_size, decoder.hidden_size)\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, images, target_sequences, sequence_lengths, tf_ratio):\n",
        "        \"\"\"\n",
        "        :param\n",
        "            images: Tensor[batch_size, 3, img_size, img_size]\n",
        "            target_sequences: Tensor[batch_size, seq_len]\n",
        "            sequence_lengths: Tensor[batch_size,]\n",
        "            tf_ratio: float\n",
        "        :return\n",
        "            logits: Tensor[max(decode_lengths), batch_size, vocab_size]\n",
        "            logits: Tensor[batch_size, max(decode_lengths), num_pixels]\n",
        "            sorted_target_sequences: Tensor[seq_len, batch_size]\n",
        "            sorted_decode_lengths: list[seq_len]\n",
        "            sorted_indices: list[batch_size]\n",
        "        \"\"\"\n",
        "        batch_size = images.size(0)\n",
        "        \n",
        "        # Encoding\n",
        "        image_features = self.encoder(images) # [batch_size, 14, 14, hidden_size]\n",
        "        image_features = image_features.view(batch_size, -1, self.encoder.hidden_size) # [batch_size, num_pixels, enc_hidden_size]\n",
        "        num_pixels = image_features.size(1)\n",
        "        \n",
        "        # Sort the batch by decreasing lengths\n",
        "        sorted_sequence_lengths, sorted_indices = torch.sort(sequence_lengths, dim=0, descending=True)\n",
        "        sorted_image_features = image_features[sorted_indices] # [batch_size, num_pixels, enc_hidden_size]\n",
        "        sorted_target_sequences = target_sequences[sorted_indices] # [seq_len, batch_size]\n",
        "        \n",
        "        # Init hidden and memory states\n",
        "        mean_image_features = sorted_image_features.mean(dim=1) # [batch_size, enc_hidden_size]\n",
        "        h_state, c_state = self.init_h0(mean_image_features), self.init_c0(mean_image_features) # [batch_size, dec_hidden_size]\n",
        "        h_state, c_state = h_state.unsqueeze(0), c_state.unsqueeze(0) # [1, batch_size, dec_hidden_size]\n",
        "        \n",
        "        # We won't decode at the <eos> position, since we've finished generating as soon as we generate <eos>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        sorted_decode_lengths = (sorted_sequence_lengths - 1).tolist()\n",
        "        \n",
        "        # Decoding\n",
        "        logits = torch.zeros(max(sorted_decode_lengths), batch_size, self.decoder.vocab_size).to(self.device)\n",
        "        alphas = torch.zeros(batch_size, max(sorted_decode_lengths), num_pixels).to(self.device)\n",
        "        last = None\n",
        "        for t in range(max(sorted_decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in sorted_decode_lengths])\n",
        "            \n",
        "            if last is not None:\n",
        "                if random.random() < tf_ratio:\n",
        "                    in_ = last[:batch_size_t]\n",
        "                else:\n",
        "                    in_ = sorted_target_sequences[:batch_size_t, t]\n",
        "            else:\n",
        "                in_ = sorted_target_sequences[:batch_size_t, t]\n",
        "            \n",
        "            logit, h_state, c_state, attention_weights = self.decoder(in_,\n",
        "                                                                      h_state[:, :batch_size_t, :],\n",
        "                                                                      c_state[:, :batch_size_t, :],\n",
        "                                                                      sorted_image_features[:batch_size_t, :, :])\n",
        "            logits[t, :batch_size_t, :] = logit\n",
        "            alphas[:batch_size_t, t, :] = attention_weights\n",
        "            last = torch.argmax(F.softmax(logit, dim=1), dim=1) # [batch_size,]\n",
        "        \n",
        "        return logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL8sy2MsZX4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_autoencoder():\n",
        "    encoder = ResNetEncoder()\n",
        "    decoder = DecoderWithBahdanauAttention(enc_hidden_size=2048,\n",
        "                                           attn_hidden_size=512,\n",
        "                                           hidden_size=512,\n",
        "                                           embedding_size=512,\n",
        "                                           vocab_size=1000,\n",
        "                                           dropout=0.5)\n",
        "    autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, device='cpu')\n",
        "    logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = \\\n",
        "        autoencoder(torch.rand((10, 3, 256, 256)),\n",
        "                    torch.randint(low=0, high=1000, size=(10, 25)),\n",
        "                    torch.randint(low=5, high=26, size=(10,)), 0.5)\n",
        "    assert logits.size() == torch.Size([max(sorted_decode_lengths), 10, 1000])\n",
        "    assert alphas.size() == torch.Size([10, max(sorted_decode_lengths), 8*8])\n",
        "    assert len(sorted_decode_lengths) == 10\n",
        "    assert sorted_target_sequences.size() == torch.Size([10, 25])\n",
        "    assert len(sorted_indices) == 10\n",
        "    \n",
        "test_autoencoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-Ph4v-GZX43",
        "colab_type": "text"
      },
      "source": [
        "# 5. Training\n",
        "\n",
        "## 5.1. Training routines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twx29pVuZX46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def init_embeddings(embeddings):\n",
        "    bias = np.sqrt(3.0 / embeddings.size(1))\n",
        "    torch.nn.init.uniform_(embeddings, -bias, bias)\n",
        "\n",
        "def load_embeddings(nlp, field):\n",
        "    embeddings = torch.FloatTensor(len(field.vocab), 300)\n",
        "    init_embeddings(embeddings)\n",
        "    for token, index in tqdm.tqdm(field.vocab.stoi.items()):\n",
        "        token = nlp(token)\n",
        "        if token.has_vector:\n",
        "            embeddings[index] = torch.tensor(token.vector, dtype=torch.float32)\n",
        "    return embeddings\n",
        "\n",
        "def clip_gradient(optimizer, grad_clip):\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "\n",
        "def save_checkpoint(model, optimizer, data_name, epoch, last_improv, bleu4, is_best):\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'bleu-4': bleu4,\n",
        "        'last_improv': last_improv,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }\n",
        "    if not os.path.exists('./checkpoint'):\n",
        "        !mkdir ./checkpoint\n",
        "    torch.save(state, './checkpoint/' + data_name + '.pt')\n",
        "    if is_best:\n",
        "        torch.save(state, './checkpoint/' + 'BEST_' + data_name + '.pt')\n",
        "\n",
        "        \n",
        "class AvgMeter(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        \n",
        "def adjust_lr(optimizer, shrink_factor):\n",
        "    print(\"\\nDecaying learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "    \n",
        "def accuracy(outputs, target_sequences, k=5):\n",
        "    batch_size = outputs.size(1)\n",
        "    _, indices = outputs.topk(k, dim=1, largest=True, sorted=True)\n",
        "    correct = indices.eq(target_sequences.view(-1, 1).expand_as(indices))\n",
        "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
        "    return correct_total.item() * (100.0 / batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gPf9q93ZX5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(model, optimizer, criterion, loader, epoch, grad_clip, alpha_c, tf_ratio, device):\n",
        "    loss_tracker, acc_tracker = AvgMeter(), AvgMeter()\n",
        "    model.train()\n",
        "    pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "    for i, (images, (target_sequences, sequence_lengths)) in pbar:\n",
        "        images = images.to(device)\n",
        "        target_sequences = target_sequences.to(device)\n",
        "        sequence_lengths = sequence_lengths.to(device)\n",
        "        # Forward prop.\n",
        "        logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = model(images, target_sequences, sequence_lengths, tf_ratio)\n",
        "        # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
        "        sorted_target_sequences = sorted_target_sequences[:, 1:]\n",
        "        # Remove paddings\n",
        "        logits = pack_padded_sequence(logits, sorted_decode_lengths).data\n",
        "        sorted_target_sequences = pack_padded_sequence(sorted_target_sequences, sorted_decode_lengths, batch_first=True).data\n",
        "        # Calculate loss\n",
        "        loss = criterion(logits, sorted_target_sequences)\n",
        "        # Add doubly stochastic attention regularization\n",
        "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "        # Back prop.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # Clip gradients\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(optimizer, grad_clip)\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        # Track metrics\n",
        "        loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
        "        acc_tracker.update(accuracy(logits, sorted_target_sequences, 5), sum(sorted_decode_lengths))\n",
        "        # Update progressbar description\n",
        "        pbar.set_description(f'Epoch: {epoch + 1:03d} - loss: {loss_tracker.avg:.3f} - acc: {acc_tracker.avg:.3f}%')\n",
        "    return loss_tracker.avg, acc_tracker.avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npl8hB_QZX5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, criterion, loader, field, epoch, alpha_c, device):\n",
        "    references, hypotheses = [], []\n",
        "    loss_tracker, acc_tracker = AvgMeter(), AvgMeter()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "        for i, (images, (target_sequences, sequence_lengths), (all_target_sequences, all_sequence_lengths)) in pbar:\n",
        "            images = images.to(device)\n",
        "            target_sequences = target_sequences.to(device)\n",
        "            sequence_lengths = sequence_lengths.to(device)\n",
        "            all_target_sequences = all_target_sequences.to(device)\n",
        "            all_sequence_lengths = all_sequence_lengths.to(device)\n",
        "            # Forward prop.\n",
        "            logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = model(images, target_sequences, sequence_lengths, 0)\n",
        "            # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
        "            sorted_target_sequences = sorted_target_sequences[:, 1:]\n",
        "            # Remove paddings\n",
        "            logits_copy = logits.clone()\n",
        "            logits = pack_padded_sequence(logits, sorted_decode_lengths).data\n",
        "            sorted_target_sequences = pack_padded_sequence(sorted_target_sequences, sorted_decode_lengths, batch_first=True).data\n",
        "            # Calculate loss\n",
        "            loss = criterion(logits, sorted_target_sequences)\n",
        "            # Add doubly stochastic attention regularization\n",
        "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "            # Track metrics\n",
        "            loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
        "            acc_tracker.update(accuracy(logits, sorted_target_sequences, 5), sum(sorted_decode_lengths))\n",
        "            # Update references\n",
        "            all_sorted_target_sequences = all_target_sequences[sorted_indices] # Because images were sorted in the decoder\n",
        "            for j in range(all_sorted_target_sequences.size(0)):\n",
        "                img_caps = all_sorted_target_sequences[j].t().tolist()\n",
        "                # Remove <sos> and <pad> tokens\n",
        "                img_caps = [*map(lambda c: [field.vocab.itos[w] for w in c\n",
        "                                            if w not in (field.vocab.stoi[field.init_token],\n",
        "                                                         field.vocab.stoi[field.pad_token])], img_caps)]\n",
        "                references.append(img_caps)\n",
        "            # Update hypotheses\n",
        "            _, preds = torch.max(logits_copy, dim=2)\n",
        "            preds, temp_preds = preds.t().tolist(), []\n",
        "            for j, p in enumerate(preds):\n",
        "                temp_preds.append([*map(lambda w: field.vocab.itos[w], preds[j][:sorted_decode_lengths[j]])]) # Remove padding\n",
        "            hypotheses.extend(temp_preds)\n",
        "            # Update progressbar description\n",
        "            pbar.set_description(f'Epoch: {epoch + 1:03d} - val_loss: {loss_tracker.avg:.3f} - val_acc: {acc_tracker.avg:.3f}%')\n",
        "        # Calculate BLEU-4 score\n",
        "        bleu4 = bleu_score(hypotheses, references, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
        "    return loss_tracker.avg, acc_tracker.avg, bleu4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N1p6-xkZX56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, criterion, train_loader, valid_loader, field, alpha_c, start_epoch, n_epochs, grad_clip, tf_ratio, device, model_name, last_improv):\n",
        "    history, best_bleu = {\n",
        "        'acc': [],\n",
        "        'loss': [],\n",
        "        'val_acc': [],\n",
        "        'val_loss': [],\n",
        "        'bleu4': []\n",
        "    }, 0.\n",
        "    for epoch in range(start_epoch, n_epochs):\n",
        "        # Stop training if no improvment since last 4 epochs\n",
        "        if last_improv == 4:\n",
        "            print('Training Finished - The model has stopped improving since last 4 epochs')\n",
        "            break\n",
        "        # Decay LR if no improvment\n",
        "        if last_improv > 0:\n",
        "            adjust_lr(optimizer, 0.8)\n",
        "        # Train step\n",
        "        loss, acc = train_step(model=model, optimizer=optimizer, criterion=criterion,\n",
        "                               loader=train_loader, epoch=epoch, grad_clip=grad_clip,\n",
        "                               alpha_c=alpha_c, tf_ratio=tf_ratio, device=device)\n",
        "        # Validation step\n",
        "        val_loss, val_acc, bleu4 = validate(model=model, criterion=criterion, loader=valid_loader,\n",
        "                                            field=field, epoch=epoch, alpha_c=alpha_c, device=device)\n",
        "        # Update history dict\n",
        "        history['acc'].append(acc)\n",
        "        history['loss'].append(loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['bleu4'].append(bleu4)\n",
        "        # Print BLEU score\n",
        "        text = f'BLEU-4: {bleu4*100:.3f}%'\n",
        "        if best_bleu > bleu4:\n",
        "            last_improv += 1\n",
        "            text += f' - Last improvement since {last_improv} epoch(s)'\n",
        "        else:\n",
        "            best_bleu, last_improv = bleu4, 0\n",
        "        print(text)\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(model=model, optimizer=optimizer, data_name=model_name, epoch=epoch, last_improv=last_improv, bleu4=bleu4, is_best=bleu4 >= best_bleu)\n",
        "    return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN_kRRgjZX6B",
        "colab_type": "text"
      },
      "source": [
        "## 5.2. Training models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQD6jkdfZX6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_NAME = 'Flickr_8k'\n",
        "ENCODER_HIDDEN_SIZE = 2048\n",
        "ATTENTION_SIZE = 512\n",
        "DECODER_HIDDEN_SIZE = 512\n",
        "EMBEDDING_SIZE = 300\n",
        "DROPOUT = 0."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzLwRaLMZX6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 10\n",
        "batch_size = 32\n",
        "lr = 3e-5\n",
        "grad_clip = 5.\n",
        "alpha_c = 1.\n",
        "tf_ratio = 0."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpI9IT-PZX6a",
        "colab_type": "code",
        "outputId": "c3ddeaa7-3250-43e9-b8d2-64b65a4e70c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "spacy_nlp = spacy.load('en_core_web_lg')\n",
        "embeddings = load_embeddings(nlp=spacy_nlp, field=EN)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2548/2548 [00:22<00:00, 114.39it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM1wCpP6ZX6i",
        "colab_type": "code",
        "outputId": "44943ebb-e57f-40d2-cf36-e20679545a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "encoder = ResNetEncoder()\n",
        "encoder.fine_tuning_resnet(fine_tune=True)\n",
        "decoder = DecoderWithBahdanauAttention(enc_hidden_size=ENCODER_HIDDEN_SIZE,\n",
        "                                       attn_hidden_size=ATTENTION_SIZE,\n",
        "                                       hidden_size=DECODER_HIDDEN_SIZE,\n",
        "                                       embedding_size=EMBEDDING_SIZE,\n",
        "                                       vocab_size=len(EN.vocab),\n",
        "                                       dropout=DROPOUT)\n",
        "decoder.load_pretrained_embeddings(embeddings)\n",
        "decoder.fine_tuning_embeddings(fine_tune=True)\n",
        "autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, device=DEVICE).to(DEVICE)\n",
        "optimizer = optim.RMSprop(params=autoencoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(f'Number of parameters of the model: {count_parameters(autoencoder):,}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters of the model: 136,587,749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjnxhtJ2ZX6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           num_workers=N_WORKERS,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True,\n",
        "                                           pin_memory=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                           num_workers=N_WORKERS,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True, \n",
        "                                           pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeYO4y0EZX65",
        "colab_type": "code",
        "outputId": "7eb11200-8d72-4b05-eaad-c97afa8e6a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "history = train(model=autoencoder,\n",
        "                optimizer=optimizer,\n",
        "                criterion=criterion,\n",
        "                train_loader=train_loader,\n",
        "                valid_loader=valid_loader,\n",
        "                field=EN,\n",
        "                alpha_c=alpha_c,\n",
        "                start_epoch=0,\n",
        "                n_epochs=n_epochs,\n",
        "                grad_clip=grad_clip,\n",
        "                tf_ratio=tf_ratio,\n",
        "                device=DEVICE,\n",
        "                model_name=MODEL_NAME,\n",
        "                last_improv=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001 - loss: 5.636 - acc: 6.325%: 100%|██████████| 188/188 [02:19<00:00,  1.35it/s]\n",
            "Epoch: 001 - val_loss: 5.280 - val_acc: 6.652%: 100%|██████████| 32/32 [00:13<00:00,  2.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-4: 4.154%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 002 - loss: 5.176 - acc: 6.982%: 100%|██████████| 188/188 [02:20<00:00,  1.34it/s]\n",
            "Epoch: 002 - val_loss: 5.095 - val_acc: 6.963%: 100%|██████████| 32/32 [00:11<00:00,  2.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-4: 4.897%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 003 - loss: 4.990 - acc: 7.407%: 100%|██████████| 188/188 [02:15<00:00,  1.39it/s]\n",
            "Epoch: 003 - val_loss: 4.934 - val_acc: 7.446%: 100%|██████████| 32/32 [00:12<00:00,  2.65it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-4: 5.187%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 004 - loss: 4.827 - acc: 7.708%: 100%|██████████| 188/188 [02:15<00:00,  1.39it/s]\n",
            "Epoch: 004 - val_loss: 4.785 - val_acc: 7.754%: 100%|██████████| 32/32 [00:11<00:00,  2.71it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-4: 6.327%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 005 - loss: 4.679 - acc: 8.040%: 100%|██████████| 188/188 [02:15<00:00,  1.39it/s]\n",
            "Epoch: 005 - val_loss: 4.696 - val_acc: 8.009%: 100%|██████████| 32/32 [00:11<00:00,  2.68it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-4: 6.955%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 006 - loss: 4.541 - acc: 8.394%: 100%|██████████| 188/188 [02:15<00:00,  1.39it/s]\n",
            "Epoch: 006 - val_loss: 4.614 - val_acc: 8.251%: 100%|██████████| 32/32 [00:11<00:00,  2.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-4: 7.721%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 007 - loss: 4.417 - acc: 8.706%: 100%|██████████| 188/188 [02:15<00:00,  1.38it/s]\n",
            "Epoch: 007 - val_loss: 4.543 - val_acc: 8.473%: 100%|██████████| 32/32 [00:11<00:00,  2.73it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-4: 8.431%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 008 - loss: 4.306 - acc: 8.979%: 100%|██████████| 188/188 [02:16<00:00,  1.38it/s]\n",
            "Epoch: 008 - val_loss: 4.474 - val_acc: 8.700%: 100%|██████████| 32/32 [00:12<00:00,  2.63it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-4: 8.498%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 009 - loss: 4.201 - acc: 9.190%: 100%|██████████| 188/188 [02:16<00:00,  1.38it/s]\n",
            "Epoch: 009 - val_loss: 4.415 - val_acc: 8.689%: 100%|██████████| 32/32 [00:12<00:00,  2.64it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-4: 10.503%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 010 - loss: 4.118 - acc: 9.463%: 100%|██████████| 188/188 [02:16<00:00,  1.37it/s]\n",
            "Epoch: 010 - val_loss: 4.392 - val_acc: 8.940%: 100%|██████████| 32/32 [00:11<00:00,  2.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-4: 10.417% - Last improvement since 1 epoch(s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bebBmWmZX7A",
        "colab_type": "code",
        "outputId": "9bcf2d8b-1f0d-41b7-fd81-d289562473a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "_, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "axes[0].plot(history['loss'], label='train')\n",
        "axes[0].plot(history['val_loss'], label='valid')\n",
        "axes[0].set_title('Loss history')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].grid(True)\n",
        "axes[1].plot(history['acc'], label='train')\n",
        "axes[1].plot(history['val_acc'], label='valid')\n",
        "axes[1].plot(np.array(history['bleu4']) * 100., label='BLEU-4')\n",
        "axes[1].set_title('Accuracy history')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s2Dp7RPZX7i",
        "colab_type": "text"
      },
      "source": [
        "# 6. Evaluation - BLEU Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC3H7uoPZX8J",
        "colab_type": "code",
        "outputId": "a614a5c7-08f6-41de-a658-6bd7a0a53f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "autoencoder.load_state_dict(torch.load('./checkpoint/BEST_Flickr_8k.pt').get('model'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULooViEOZX8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
        "                                   img_captions=test_img_captions,\n",
        "                                   split_set='TEST',\n",
        "                                   img_transform=img_transform,\n",
        "                                   caption_transform=caption_transform)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                          num_workers=N_WORKERS,\n",
        "                                          batch_size=1,\n",
        "                                          shuffle=True, \n",
        "                                          pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2DZ6d03twtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BeamNode:\n",
        "    __index = 0\n",
        "\n",
        "    def __init__(self, ):"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kXqLgQHZX8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, loader, beam_size, field, max_len, device):\n",
        "    references, hypotheses = [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
        "        for i, (images, (target_sequences, sequence_lengths), (all_target_sequences, all_sequence_lengths)) in pbar:\n",
        "            images = images.to(device)\n",
        "            target_sequences = target_sequences.to(device)\n",
        "            sequence_lengths = sequence_lengths.to(device)\n",
        "            all_target_sequences = all_target_sequences.to(device)\n",
        "            all_sequence_lengths = all_sequence_lengths.to(device)\n",
        "            \n",
        "            k = beam_size\n",
        "            \n",
        "            # Encoding\n",
        "            image_features = model.encoder(images) # [1, 14, 14, hidden_size]\n",
        "            image_features = image_features.view(1, -1, model.encoder.hidden_size) # [1, num_pixels, enc_hidden_size]\n",
        "            \n",
        "            # Init hidden and memory states\n",
        "            mean_image_features = image_features.mean(dim=1) # [1, enc_hidden_size]\n",
        "            h_state, c_state = model.init_h0(mean_image_features), model.init_c0(mean_image_features) # [1, dec_hidden_size]\n",
        "\n",
        "            # Decoding\n",
        "            step = 1\n",
        "            while True:\n",
        "                embedded = model.decoder.embedding(topk_prev_tokens.squeeze(1))  # [k, embedding_size]\n",
        "                context_vector, _ = model.decoder.attention(image_features, h_state)\n",
        "                # context_vector: Tensor[k, enc_hidden_size]\n",
        "                # _: Tensor[k, num_pixels]\n",
        "                gate = torch.sigmoid(model.decoder.f_beta(h_state))  # [k, enc_hidden_size], Gating scalar\n",
        "                context_vector = gate * context_vector # [k, enc_hidden_size]\n",
        "                x = torch.cat((embedded, context_vector), dim=1) # [k, embedding_size + enc_hidden_size]\n",
        "                h_state, c_state = h_state.unsqueeze(0).contiguous(), c_state.unsqueeze(0).contiguous()\n",
        "                output, (h_state, c_state) = model.decoder.lstm(x.unsqueeze(0), (h_state, c_state))\n",
        "                # output: [1, k, hidden_size]\n",
        "                # h_state: [1, k, hidden_size]\n",
        "                # c_state: [1, k, hidden_size]\n",
        "                logit = model.decoder.fc(output) # [1, k, vocab_size]\n",
        "                logp = F.log_softmax(logit.squeeze(0), dim=1) # [k, vocab_size]\n",
        "\n",
        "                topk_logp = torch.topk(logp, k, dim=1) # [k, k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZMIDy2mZX81",
        "colab_type": "text"
      },
      "source": [
        "# 7. Inference - Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzgW20ekZX81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D55g1AHNZX8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}