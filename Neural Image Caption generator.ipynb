{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  2 04:48:03 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 26%   33C    P0    39W / 180W |      1MiB / 16278MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import functools\n",
    "import collections\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import torchvision\n",
    "from torchtext.data import Example, Field, Dataset\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Number of CPUs: 8\n"
     ]
    }
   ],
   "source": [
    "SEED = 781\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "N_WORKERS = multiprocessing.cpu_count()\n",
    "print(f'Number of CPUs: {N_WORKERS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 µs, sys: 8 µs, total: 21 µs\n",
      "Wall time: 25.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not os.path.exists('./data'):\n",
    "    !mkdir ./data\n",
    "    \n",
    "    !wget --no-check-certificate \\\n",
    "        https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip \\\n",
    "        -O ./data/Flickr8k_Dataset.zip\n",
    "    !unzip -q ./data/Flickr8k_Dataset.zip -d ./data\n",
    "    !rm -r ./data/Flickr8k_Dataset.zip\n",
    "\n",
    "    !wget --no-check-certificate \\\n",
    "        https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip \\\n",
    "        -O ./data/Flickr8k_text.zip\n",
    "    !unzip -q ./data/Flickr8k_text.zip -d ./data\n",
    "    !rm -r ./data/Flickr8k_text.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation\n",
    "\n",
    "## 3.1. Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use this corpus / data:\n",
      "\n",
      "Please cite: M. Hodosh, P. Young and J. Hockenmaier (2013) \"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics\", Journal of Artifical Intellegence Research, Volume 47, pages 853-899\n",
      "http://www.jair.org/papers/paper3994.html\n",
      "\n",
      "\n",
      "Captions, Dataset Splits, and Human Annotations :\n",
      "\n",
      "\n",
      "Flickr8k.token.txt - the raw captions of the Flickr8k Dataset . The first column is the ID of the caption which is \"image address # caption number\"\n",
      "\n",
      "Flickr8k.lemma.txt - the lemmatized version of the above captions \n",
      "\n",
      "Flickr_8k.trainImages.txt - The training images used in our experiments\n",
      "Flickr_8k.devImages.txt - The development/validation images used in our experiments\n",
      "Flickr_8k.testImages.txt - The test images used in our experiments\n",
      "\n",
      "\n",
      "ExpertAnnotations.txt is the expert judgments.  The first two columns are the image and caption IDs.  Caption IDs are <image file name>#<0-4>.  The next three columns are the expert judgments for that image-caption pair.  Scores range from 1 to 4, with a 1 indicating that the caption does not describe the image at all, a 2 indicating the caption describes minor aspects of the image but does not describe the image, a 3 indicating that the caption almost describes the image with minor mistakes, and a 4 indicating that the caption describes the image.\n",
      "\n",
      "\n",
      "CrowdFlowerAnnotations.txt contains the CrowdFlower judgments.  The first two columns are the image and caption IDs.  The third column is the percent of Yeses, the fourth column is the total number of Yeses, the fifth column is the total number of Noes.  A Yes means that the caption describes the image (possibly with minor mistakes), while a No means that the caption does not describe the image.  Each image-caption pair has a minimum of three judgments, but some may have more.\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 8,092\n",
      "Number of train images: 6,000\n",
      "Number of valid images: 1,000\n",
      "Number of test images: 1,000\n"
     ]
    }
   ],
   "source": [
    "train_img_fn = [*map(str.strip, open('./data/Flickr_8k.trainImages.txt').readlines())]\n",
    "valid_img_fn = [*map(str.strip, open('./data/Flickr_8k.devImages.txt').readlines())]\n",
    "test_img_fn = [*map(str.strip, open('./data/Flickr_8k.testImages.txt').readlines())]\n",
    "\n",
    "img_captions = collections.defaultdict(lambda: [])\n",
    "with open('./data/Flickr8k.token.txt') as file:\n",
    "    for line in file.readlines():\n",
    "        img_fn, caption = line.strip().split('\\t')\n",
    "        img_captions[img_fn[:-2]].append(caption)\n",
    "        \n",
    "train_img_captions = dict(filter(lambda x: x[0] in train_img_fn, img_captions.items()))\n",
    "valid_img_captions = dict(filter(lambda x: x[0] in valid_img_fn, img_captions.items()))\n",
    "test_img_captions = dict(filter(lambda x: x[0] in test_img_fn, img_captions.items()))\n",
    "    \n",
    "print(f'Number of images: {len(img_captions):,}')\n",
    "print(f'Number of train images: {len(train_img_captions):,}')\n",
    "print(f'Number of valid images: {len(valid_img_captions):,}')\n",
    "print(f'Number of test images: {len(test_img_captions):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "PAD_TOKEN = '<pad>'\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "\n",
    "def clean(caption):\n",
    "    # Remove non-alphabetical character\n",
    "    caption = re.sub(r'[^a-zA-Z]', r' ', caption)\n",
    "    # Remove one word character\n",
    "    caption = re.sub(r'\\b[a-zA-Z]\\b', r' ', caption)\n",
    "    # Remove multiple spaces\n",
    "    caption = re.sub(r'\\s+', r' ', caption)\n",
    "    return caption.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train image caption length: 33\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean captions\n",
    "train_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), train_img_captions.items()))\n",
    "valid_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), valid_img_captions.items()))\n",
    "test_img_captions = dict(map(lambda x: (x[0], [*map(clean, x[1])]), test_img_captions.items()))\n",
    "\n",
    "# Get the length of the longest caption\n",
    "all_train_captions = [*functools.reduce(lambda x, y: x + y, train_img_captions.values())]\n",
    "print('Max train image caption length:', max(map(len, map(str.split, all_train_captions))))\n",
    "plt.hist([*map(len, map(str.split, all_train_captions))])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:01<00:00, 21136.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary: 2,527\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 5\n",
    "MAX_LEN = 25\n",
    "all_train_captions = [*functools.reduce(lambda x, y: x + y, train_img_captions.values())]\n",
    "EN = Field(init_token=SOS_TOKEN,\n",
    "           eos_token=EOS_TOKEN,\n",
    "           fix_length=MAX_LEN,\n",
    "           lower=True,\n",
    "           tokenize='spacy',\n",
    "           tokenizer_language='en',\n",
    "           include_lengths=True)\n",
    "examples = [Example.fromlist(data=[caption], fields=[('caption', EN)])\n",
    "            for caption in tqdm.tqdm(all_train_captions)]\n",
    "captions_data = Dataset(examples, fields={'caption': EN})\n",
    "EN.build_vocab(captions_data,\n",
    "               min_freq=MIN_COUNT,\n",
    "               specials=[SOS_TOKEN, UNK_TOKEN, EOS_TOKEN, PAD_TOKEN])\n",
    "print(f'Length of vocabulary: {len(EN.vocab):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_transform(caption):\n",
    "    if isinstance(caption, str):\n",
    "        return EN.process([EN.preprocess(caption)])\n",
    "    elif isinstance(caption, list):\n",
    "        return EN.process([*map(EN.preprocess, caption)])\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (256, 256)\n",
    "\n",
    "img_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Lambda(lambda x: x / 255.),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, img_captions, split_set, img_transform, caption_transform, img_shape=(256, 256)):\n",
    "        assert split_set in {'TRAIN', 'VALID', 'TEST'}\n",
    "        self.data_path = data_path\n",
    "        self.img_captions = img_captions\n",
    "        self.split_set = split_set\n",
    "        self.img_transform = img_transform\n",
    "        self.caption_transform = caption_transform\n",
    "        self.img_shape = img_shape\n",
    "        self.ids = list(sorted(self.img_captions.keys()))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "\n",
    "        img = Image.open(os.path.join(self.data_path, img_id)).convert('RGB')\n",
    "        img = self.img_transform(img.resize(self.img_shape))\n",
    "\n",
    "        targets = self.img_captions[img_id]\n",
    "        targets = self.caption_transform(targets)\n",
    "        \n",
    "        idx = np.random.randint(len(targets))\n",
    "        target = (targets[0][:, idx], targets[1][idx])\n",
    "        \n",
    "        if self.split_set is 'TRAIN':\n",
    "            return img, target\n",
    "        else:\n",
    "            return img, target, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
    "                                    img_captions=train_img_captions,\n",
    "                                    split_set='TRAIN',\n",
    "                                    img_transform=img_transform,\n",
    "                                    caption_transform=caption_transform)\n",
    "valid_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
    "                                    img_captions=valid_img_captions,\n",
    "                                    split_set='VALID',\n",
    "                                    img_transform=img_transform,\n",
    "                                    caption_transform=caption_transform)\n",
    "test_dataset = ImageCaptionDataset(data_path='./data/Flicker8k_Dataset/',\n",
    "                                   img_captions=test_img_captions,\n",
    "                                   split_set='TEST',\n",
    "                                   img_transform=img_transform,\n",
    "                                   caption_transform=caption_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling\n",
    "\n",
    "## 4.1. Generate image features - ResNet Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size=2048):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        resnet = torchvision.models.resnet101(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "    def fine_tuning_resnet(self, fine_tune):\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            images: Tensor[batch_size, 3, img_size, img_size]\n",
    "        :return\n",
    "            out: Tensor[batch_size, 8, 8, hidden_size]\n",
    "        \"\"\"\n",
    "        out = self.resnet(images)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encoder():\n",
    "    encoder = ResNetEncoder()\n",
    "    latent = encoder(torch.rand((10, 3, 256, 256)))\n",
    "    assert latent.size() == torch.Size([10, 8, 8, 2048])\n",
    "    \n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Badhanau Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = nn.Linear(enc_hidden_size, hidden_size)\n",
    "        self.W2 = nn.Linear(dec_hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, features, h_state):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            features:  Tensor[batch_size, num_pixels, enc_hidden_size]\n",
    "            h_state: Tensor[batch_size, dec_hidden_size]\n",
    "        :return\n",
    "            context_vector: Tensor[batch_size, enc_hidden_size]\n",
    "            attention_weights: Tensor[batch_size, num_pixels]\n",
    "        \"\"\"\n",
    "        h_state = h_state.unsqueeze(1) # [batch_size, 1, dec_hidden_size]\n",
    "        score = torch.tanh(self.W1(features) + self.W2(h_state)) # [batch_size, num_pixels, hidden_size]\n",
    "        attention_weights = F.softmax(self.V(score), dim=1) # [batch_size, num_pixels, 1]\n",
    "        context_vector = attention_weights * features # [batch_size, num_pixels, enc_hidden_size]\n",
    "        context_vector = torch.sum(context_vector, dim=1) # [batch_size, enc_hidden_size]\n",
    "        return context_vector, attention_weights.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attention():\n",
    "    attention = BahdanauAttention(enc_hidden_size=2048, dec_hidden_size=512, hidden_size=512)\n",
    "    context_vector, attention_weights = attention(torch.rand((10, 8*8, 2048)), torch.rand((10, 512)))\n",
    "    assert context_vector.size() == torch.Size([10, 2048])\n",
    "    assert attention_weights.size() == torch.Size([10, 8*8])\n",
    "    \n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Generate captions - LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithBahdanauAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_hidden_size, attn_hidden_size, hidden_size, embedding_size, vocab_size, dropout):\n",
    "        super(DecoderWithBahdanauAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size + enc_hidden_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(enc_hidden_size, hidden_size, attn_hidden_size)\n",
    "        self.f_beta = nn.Linear(hidden_size, enc_hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "        \n",
    "    def fine_tuning_embeddings(self, fine_tune=False):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "        \n",
    "    def forward(self, input_word_index, h_state, c_state, enc_outputs):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            input_word_index: Tensor[batch_size,]\n",
    "            h_state: Tensor[1, batch_size, hidden_size]\n",
    "            c_state: Tensor[1, batch_size, hidden_size]\n",
    "            enc_outputs: Tensor[batch_size, num_pixels, enc_hidden_size]\n",
    "        :return\n",
    "            logit: Tensor[batch_size, vocab_size]\n",
    "            h_state: Tensor[1, batch_size, hidden_size]\n",
    "            c_state: Tensor[1, batch_size, hidden_size]\n",
    "            attention_weights: Tensor[batch_size, num_pixels]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_word_index)  # [batch_size, embedding_size]\n",
    "        context_vector, attention_weights = self.attention(enc_outputs, h_state.squeeze(0))\n",
    "        # context_vector: Tensor[batch_size, enc_hidden_size]\n",
    "        # attention_weights: Tensor[batch_size, num_pixels]\n",
    "        \n",
    "        gate = torch.sigmoid(self.f_beta(h_state))  # [1, batch_size, enc_hidden_size], Gating scalar\n",
    "        context_vector = gate.squeeze(0) * context_vector # [batch_size, enc_hidden_size]\n",
    "        \n",
    "        x = torch.cat((embedded, context_vector), dim=1) # [batch_size, embedding_size + enc_hidden_size]\n",
    "        output, (h_state, c_state) = self.lstm(x.unsqueeze(0), (h_state, c_state))\n",
    "        # output: [1, batch_size, hidden_size]\n",
    "        # h_state: [1, batch_size, hidden_size]\n",
    "        # c_state: [1, batch_size, hidden_size]\n",
    "        \n",
    "        logit = self.fc(self.dropout(output)) # [1, batch_size, vocab_size]\n",
    "        logit = logit.squeeze(0) # [batch_size, vocab_size]\n",
    "        return logit, h_state, c_state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decoder():\n",
    "    decoder = DecoderWithBahdanauAttention(enc_hidden_size=2048,\n",
    "                                           attn_hidden_size=512,\n",
    "                                           hidden_size=512,\n",
    "                                           embedding_size=512,\n",
    "                                           vocab_size=1000,\n",
    "                                           dropout=0.5)\n",
    "    logit, h_state, c_state, attention_weights = decoder(torch.randint(low=0, high=1000, size=(10,)),\n",
    "                                                         torch.rand((1, 10, 512)),\n",
    "                                                         torch.rand((1, 10, 512)),\n",
    "                                                         torch.rand((10, 14*14, 2048)))\n",
    "    assert logit.size() == torch.Size([10, 1000])\n",
    "    assert h_state.size() == torch.Size([1, 10, 512])\n",
    "    assert c_state.size() == torch.Size([1, 10, 512])\n",
    "    assert attention_weights.size() == torch.Size([10, 14*14])\n",
    "    \n",
    "test_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Putting all together - AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.init_h0 = nn.Linear(encoder.hidden_size, decoder.hidden_size)\n",
    "        self.init_c0 = nn.Linear(encoder.hidden_size, decoder.hidden_size)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, images, target_sequences, sequence_lengths, tf_ratio):\n",
    "        \"\"\"\n",
    "        :param\n",
    "            images: Tensor[batch_size, 3, img_size, img_size]\n",
    "            target_sequences: Tensor[batch_size, seq_len]\n",
    "            sequence_lengths: Tensor[batch_size,]\n",
    "            tf_ratio: float\n",
    "        :return\n",
    "            logits: Tensor[max(decode_lengths), batch_size, vocab_size]\n",
    "            logits: Tensor[batch_size, max(decode_lengths), num_pixels]\n",
    "            sorted_target_sequences: Tensor[seq_len, batch_size]\n",
    "            sorted_decode_lengths: list[seq_len]\n",
    "            sorted_indices: list[batch_size]\n",
    "        \"\"\"\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Encoding\n",
    "        image_features = self.encoder(images) # [batch_size, 14, 14, hidden_size]\n",
    "        image_features = image_features.view(batch_size, -1, self.encoder.hidden_size) # [batch_size, num_pixels, enc_hidden_size]\n",
    "        num_pixels = image_features.size(1)\n",
    "        \n",
    "        # Sort the batch by decreasing lengths\n",
    "        sorted_sequence_lengths, sorted_indices = torch.sort(sequence_lengths, dim=0, descending=True)\n",
    "        sorted_image_features = image_features[sorted_indices] # [batch_size, num_pixels, enc_hidden_size]\n",
    "        sorted_target_sequences = target_sequences[sorted_indices] # [seq_len, batch_size]\n",
    "        \n",
    "        # Init hidden and memory states\n",
    "        mean_image_features = sorted_image_features.mean(dim=1) # [batch_size, enc_hidden_size]\n",
    "        h_state, c_state = self.init_h0(mean_image_features), self.init_c0(mean_image_features) # [batch_size, dec_hidden_size]\n",
    "        h_state, c_state = h_state.unsqueeze(0), c_state.unsqueeze(0) # [1, batch_size, dec_hidden_size]\n",
    "        \n",
    "        # We won't decode at the <eos> position, since we've finished generating as soon as we generate <eos>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        sorted_decode_lengths = (sorted_sequence_lengths - 1).tolist()\n",
    "        \n",
    "        # Decoding\n",
    "        logits = torch.zeros(max(sorted_decode_lengths), batch_size, self.decoder.vocab_size).to(self.device)\n",
    "        alphas = torch.zeros(batch_size, max(sorted_decode_lengths), num_pixels).to(self.device)\n",
    "        last = None\n",
    "        for t in range(max(sorted_decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in sorted_decode_lengths])\n",
    "            \n",
    "            if last is not None:\n",
    "                if random.random() < tf_ratio:\n",
    "                    in_ = last[:batch_size_t]\n",
    "                else:\n",
    "                    in_ = sorted_target_sequences[:batch_size_t, t]\n",
    "            else:\n",
    "                in_ = sorted_target_sequences[:batch_size_t, t]\n",
    "            \n",
    "            logit, h_state, c_state, attention_weights = self.decoder(in_,\n",
    "                                                                      h_state[:, :batch_size_t, :],\n",
    "                                                                      c_state[:, :batch_size_t, :],\n",
    "                                                                      sorted_image_features[:batch_size_t, :, :])\n",
    "            logits[t, :batch_size_t, :] = logit\n",
    "            alphas[:batch_size_t, t, :] = attention_weights\n",
    "            last = torch.argmax(F.softmax(logit, dim=1), dim=1) # [batch_size,]\n",
    "        \n",
    "        return logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_autoencoder():\n",
    "    encoder = ResNetEncoder()\n",
    "    decoder = DecoderWithBahdanauAttention(enc_hidden_size=2048,\n",
    "                                           attn_hidden_size=512,\n",
    "                                           hidden_size=512,\n",
    "                                           embedding_size=512,\n",
    "                                           vocab_size=1000,\n",
    "                                           dropout=0.5)\n",
    "    autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, device='cpu')\n",
    "    logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = \\\n",
    "        autoencoder(torch.rand((10, 3, 256, 256)),\n",
    "                    torch.randint(low=0, high=1000, size=(10, 25)),\n",
    "                    torch.randint(low=5, high=26, size=(10,)), 0.5)\n",
    "    assert logits.size() == torch.Size([max(sorted_decode_lengths), 10, 1000])\n",
    "    assert alphas.size() == torch.Size([10, max(sorted_decode_lengths), 8*8])\n",
    "    assert len(sorted_decode_lengths) == 10\n",
    "    assert sorted_target_sequences.size() == torch.Size([10, 25])\n",
    "    assert len(sorted_indices) == 10\n",
    "    \n",
    "test_autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training\n",
    "\n",
    "## 5.1. Training routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def init_embeddings(embeddings):\n",
    "    bias = np.sqrt(3.0 / embeddings.size(1))\n",
    "    torch.nn.init.uniform_(embeddings, -bias, bias)\n",
    "\n",
    "def load_embeddings(nlp, field):\n",
    "    embeddings = torch.FloatTensor(len(field.vocab), 300)\n",
    "    init_embeddings(embeddings)\n",
    "    for token, index in tqdm.tqdm(field.vocab.stoi.items()):\n",
    "        token = nlp(token)\n",
    "        if token.has_vector:\n",
    "            embeddings[index] = torch.tensor(token.vector, dtype=torch.float32)\n",
    "    return embeddings\n",
    "\n",
    "def clip_gradient(optimizer, grad_clip):\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "\n",
    "def save_checkpoint(model, optimizer, data_name, epoch, last_improv, bleu4, is_best):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'bleu-4': bleu4,\n",
    "        'last_improv': last_improv,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    if not os.path.exists('./checkpoint'):\n",
    "        !mkdir ./checkpoint\n",
    "    torch.save(state, './checkpoint/' + data_name + '.pt')\n",
    "    if is_best:\n",
    "        torch.save(state, './checkpoint/' + 'BEST_' + data_name + '.pt')\n",
    "\n",
    "        \n",
    "class AvgMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def adjust_lr(optimizer, shrink_factor):\n",
    "    print(\"\\nDecaying learning rate.\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
    "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
    "    \n",
    "def accuracy(outputs, target_sequences, k=5):\n",
    "    batch_size = outputs.size(1)\n",
    "    _, indices = outputs.topk(k, dim=1, largest=True, sorted=True)\n",
    "    correct = indices.eq(target_sequences.view(-1, 1).expand_as(indices))\n",
    "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
    "    return correct_total.item() * (100.0 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, criterion, loader, epoch, grad_clip, alpha_c, tf_ratio, device):\n",
    "    loss_tracker, acc_tracker = AvgMeter(), AvgMeter()\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "    for i, (images, (target_sequences, sequence_lengths)) in pbar:\n",
    "        images = images.to(device)\n",
    "        target_sequences = target_sequences.to(device)\n",
    "        sequence_lengths = sequence_lengths.to(device)\n",
    "        # Forward prop.\n",
    "        logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = model(images, target_sequences, sequence_lengths, tf_ratio)\n",
    "        # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
    "        sorted_target_sequences = sorted_target_sequences[:, 1:]\n",
    "        # Remove paddings\n",
    "        logits = pack_padded_sequence(logits, sorted_decode_lengths).data\n",
    "        sorted_target_sequences = pack_padded_sequence(sorted_target_sequences, sorted_decode_lengths, batch_first=True).data\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, sorted_target_sequences)\n",
    "        # Add doubly stochastic attention regularization\n",
    "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "        # Back prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Track metrics\n",
    "        loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
    "        acc_tracker.update(accuracy(logits, sorted_target_sequences, 5), sum(sorted_decode_lengths))\n",
    "        # Update progressbar description\n",
    "        pbar.set_description(f'Epoch: {epoch + 1:03d} - loss: {loss_tracker.avg:.3f} - acc: {acc_tracker.avg:.3f}%')\n",
    "    return loss_tracker.avg, acc_tracker.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, loader, field, epoch, alpha_c, device):\n",
    "    references, hypotheses = [], []\n",
    "    loss_tracker, acc_tracker = AvgMeter(), AvgMeter()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "        for i, (images, (target_sequences, sequence_lengths), (all_target_sequences, all_sequence_lengths)) in pbar:\n",
    "            images = images.to(device)\n",
    "            target_sequences = target_sequences.to(device)\n",
    "            sequence_lengths = sequence_lengths.to(device)\n",
    "            all_target_sequences = all_target_sequences.to(device)\n",
    "            all_sequence_lengths = all_sequence_lengths.to(device)\n",
    "            # Forward prop.\n",
    "            logits, alphas, sorted_target_sequences, sorted_decode_lengths, sorted_indices = model(images, target_sequences, sequence_lengths, 0)\n",
    "            # Since we decoded starting with <sos>, the targets are all words after <sos>, up to <eos>\n",
    "            sorted_target_sequences = sorted_target_sequences[:, 1:]\n",
    "            # Remove paddings\n",
    "            logits_copy = logits.clone()\n",
    "            logits = pack_padded_sequence(logits, sorted_decode_lengths).data\n",
    "            sorted_target_sequences = pack_padded_sequence(sorted_target_sequences, sorted_decode_lengths, batch_first=True).data\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, sorted_target_sequences)\n",
    "            # Add doubly stochastic attention regularization\n",
    "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "            # Track metrics\n",
    "            loss_tracker.update(loss.item(), sum(sorted_decode_lengths))\n",
    "            acc_tracker.update(accuracy(logits, sorted_target_sequences, 5), sum(sorted_decode_lengths))\n",
    "            # Update references\n",
    "            all_sorted_target_sequences = all_target_sequences[sorted_indices] # Because images were sorted in the decoder\n",
    "            for j in range(all_sorted_target_sequences.size(0)):\n",
    "                img_caps = all_sorted_target_sequences[j].t().tolist()\n",
    "                # Remove <sos> and <pad> tokens\n",
    "                img_caps = [*map(lambda c: [field.vocab.itos[w] for w in c\n",
    "                                            if w not in (field.vocab.stoi[field.init_token],\n",
    "                                                         field.vocab.stoi[field.pad_token])], img_caps)]\n",
    "                references.append(img_caps)\n",
    "            # Update hypotheses\n",
    "            _, preds = torch.max(logits_copy, dim=2)\n",
    "            preds, temp_preds = preds.t().tolist(), []\n",
    "            for j, p in enumerate(preds):\n",
    "                temp_preds.append([*map(lambda w: field.vocab.itos[w], preds[j][:sorted_decode_lengths[j]])]) # Remove padding\n",
    "            hypotheses.extend(temp_preds)\n",
    "            # Update progressbar description\n",
    "            pbar.set_description(f'Epoch: {epoch + 1:03d} - val_loss: {loss_tracker.avg:.3f} - val_acc: {acc_tracker.avg:.3f}%')\n",
    "        # Calculate BLEU-4 score\n",
    "        bleu4 = bleu_score(hypotheses, references, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "    return loss_tracker.avg, acc_tracker.avg, bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, valid_loader, field, alpha_c, start_epoch, n_epochs, grad_clip, tf_ratio, device, model_name, last_improv):\n",
    "    history, best_bleu = {\n",
    "        'acc': [],\n",
    "        'loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_loss': [],\n",
    "        'bleu4': []\n",
    "    }, 0.\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        # Stop training if no improvment since last 4 epochs\n",
    "        if last_improv == 4:\n",
    "            print('Training Finished - The model has stopped improving since last 4 epochs')\n",
    "            break\n",
    "        # Decay LR if no improvment\n",
    "        if last_improv > 0:\n",
    "            adjust_lr(optimizer, 0.9)\n",
    "        # Train step\n",
    "        loss, acc = train_step(model=model, optimizer=optimizer, criterion=criterion,\n",
    "                               loader=train_loader, epoch=epoch, grad_clip=grad_clip,\n",
    "                               alpha_c=alpha_c, tf_ratio=tf_ratio, device=device)\n",
    "        # Validation step\n",
    "        val_loss, val_acc, bleu4 = validate(model=model, criterion=criterion, loader=valid_loader,\n",
    "                                            field=field, epoch=epoch, alpha_c=alpha_c, device=device)\n",
    "        # Update history dict\n",
    "        history['acc'].append(acc)\n",
    "        history['loss'].append(loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['bleu4'].append(bleu4)\n",
    "        # Print BLEU score\n",
    "        text = f'BLEU-4: {bleu4*100:.3f}%'\n",
    "        if best_bleu > bleu4:\n",
    "            last_improv += 1\n",
    "            text += f' - Last improvement since {last_improv} epoch(s)'\n",
    "        else:\n",
    "            best_bleu, last_improv = bleu4, 0\n",
    "        print(text)\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(model=model, optimizer=optimizer, data_name=model_name, epoch=epoch, last_improv=last_improv, bleu4=bleu4, is_best=bleu4 >= best_bleu)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'Flickr_8k'\n",
    "ENCODER_HIDDEN_SIZE = 2048\n",
    "ATTENTION_SIZE = 512\n",
    "DECODER_HIDDEN_SIZE = 512\n",
    "EMBEDDING_SIZE = 300\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 32\n",
    "lr = 4e-4\n",
    "grad_clip = 5.\n",
    "alpha_c = 1.\n",
    "tf_ratio = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2527/2527 [00:18<00:00, 136.70it/s]\n"
     ]
    }
   ],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_lg')\n",
    "embeddings = load_embeddings(nlp=spacy_nlp, field=EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of the model: 54,651,700\n"
     ]
    }
   ],
   "source": [
    "encoder = ResNetEncoder()\n",
    "encoder.fine_tuning_resnet(fine_tune=True)\n",
    "decoder = DecoderWithBahdanauAttention(enc_hidden_size=ENCODER_HIDDEN_SIZE,\n",
    "                                       attn_hidden_size=ATTENTION_SIZE,\n",
    "                                       hidden_size=DECODER_HIDDEN_SIZE,\n",
    "                                       embedding_size=EMBEDDING_SIZE,\n",
    "                                       vocab_size=len(EN.vocab),\n",
    "                                       dropout=DROPOUT)\n",
    "decoder.load_pretrained_embeddings(embeddings)\n",
    "decoder.fine_tuning_embeddings(fine_tune=True)\n",
    "autoencoder = AutoEncoder(encoder=encoder, decoder=decoder, device=DEVICE).to(DEVICE)\n",
    "optimizer = optim.Adam(params=autoencoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(f'Number of parameters of the model: {count_parameters(autoencoder):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           num_workers=N_WORKERS,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                           num_workers=N_WORKERS,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True, \n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 001 - loss: 5.930 - acc: 4.238%: 100%|██████████| 188/188 [01:33<00:00,  2.01it/s]\n",
      "Epoch: 001 - val_loss: 5.373 - val_acc: 5.273%: 100%|██████████| 32/32 [00:06<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 2.926%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 002 - loss: 5.180 - acc: 5.621%: 100%|██████████| 188/188 [01:31<00:00,  2.05it/s]\n",
      "Epoch: 002 - val_loss: 4.964 - val_acc: 6.031%: 100%|██████████| 32/32 [00:06<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 5.178%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 003 - loss: 4.872 - acc: 6.090%: 100%|██████████| 188/188 [01:32<00:00,  2.04it/s]\n",
      "Epoch: 003 - val_loss: 4.723 - val_acc: 6.418%: 100%|██████████| 32/32 [00:06<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 5.855%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 004 - loss: 4.677 - acc: 6.441%: 100%|██████████| 188/188 [01:32<00:00,  2.03it/s]\n",
      "Epoch: 004 - val_loss: 4.618 - val_acc: 6.575%: 100%|██████████| 32/32 [00:06<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 5.812% - Last improvement since 1 epoch(s)\n",
      "\n",
      "Decaying learning rate.\n",
      "The new learning rate is 0.000360\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 005 - loss: 4.531 - acc: 6.691%: 100%|██████████| 188/188 [01:32<00:00,  2.03it/s]\n",
      "Epoch: 005 - val_loss: 4.472 - val_acc: 6.877%: 100%|██████████| 32/32 [00:06<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 6.801%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 006 - loss: 4.430 - acc: 6.833%: 100%|██████████| 188/188 [01:32<00:00,  2.03it/s]\n",
      "Epoch: 006 - val_loss: 4.451 - val_acc: 6.856%: 100%|██████████| 32/32 [00:06<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 6.367% - Last improvement since 1 epoch(s)\n",
      "\n",
      "Decaying learning rate.\n",
      "The new learning rate is 0.000324\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 007 - loss: 4.316 - acc: 7.018%: 100%|██████████| 188/188 [01:32<00:00,  2.03it/s]\n",
      "Epoch: 007 - val_loss: 4.354 - val_acc: 7.119%: 100%|██████████| 32/32 [00:06<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 7.147%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 008 - loss: 4.273 - acc: 7.108%: 100%|██████████| 188/188 [01:32<00:00,  2.03it/s]\n",
      "Epoch: 008 - val_loss: 4.288 - val_acc: 7.085%: 100%|██████████| 32/32 [00:06<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 7.729%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 009 - loss: 4.183 - acc: 7.280%: 100%|██████████| 188/188 [01:32<00:00,  2.03it/s]\n",
      "Epoch: 009 - val_loss: 4.263 - val_acc: 7.263%: 100%|██████████| 32/32 [00:06<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 7.319% - Last improvement since 1 epoch(s)\n",
      "\n",
      "Decaying learning rate.\n",
      "The new learning rate is 0.000292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 010 - loss: 4.114 - acc: 7.348%: 100%|██████████| 188/188 [01:32<00:00,  2.03it/s]\n",
      "Epoch: 010 - val_loss: 4.283 - val_acc: 7.127%: 100%|██████████| 32/32 [00:06<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 7.049% - Last improvement since 2 epoch(s)\n",
      "\n",
      "Decaying learning rate.\n",
      "The new learning rate is 0.000262\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 011 - loss: 4.051 - acc: 7.500%: 100%|██████████| 188/188 [01:32<00:00,  2.03it/s]\n",
      "Epoch: 011 - val_loss: 4.182 - val_acc: 7.373%: 100%|██████████| 32/32 [00:06<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 7.352% - Last improvement since 3 epoch(s)\n",
      "\n",
      "Decaying learning rate.\n",
      "The new learning rate is 0.000236\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 012 - loss: 4.005 - acc: 7.563%: 100%|██████████| 188/188 [01:32<00:00,  2.03it/s]\n",
      "Epoch: 012 - val_loss: 4.176 - val_acc: 7.391%: 100%|██████████| 32/32 [00:06<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 7.613% - Last improvement since 4 epoch(s)\n",
      "Training Finished - The model has stopped improving since last 4 epochs\n"
     ]
    }
   ],
   "source": [
    "history = train(model=autoencoder,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                train_loader=train_loader,\n",
    "                valid_loader=valid_loader,\n",
    "                field=EN,\n",
    "                alpha_c=alpha_c,\n",
    "                start_epoch=0,\n",
    "                n_epochs=n_epochs,\n",
    "                grad_clip=grad_clip,\n",
    "                tf_ratio=tf_ratio,\n",
    "                device=DEVICE,\n",
    "                model_name=MODEL_NAME,\n",
    "                last_improv=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(history['loss'], label='train')\n",
    "axes[0].plot(history['val_loss'], label='valid')\n",
    "axes[0].set_title('Loss history')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True)\n",
    "axes[1].plot(history['acc'], label='train')\n",
    "axes[1].plot(history['val_acc'], label='valid')\n",
    "axes[1].plot(np.array(history['bleu4']) * 100., label='BLEU-4')\n",
    "axes[1].set_title('Accuracy history')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluation - BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.load_state_dict(torch.load('./checkpoint/BEST_Flickr_8k.pt').get('model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          num_workers=N_WORKERS,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True, \n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, beam_size, field, max_len, device):\n",
    "    references, hypotheses = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm.tqdm(enumerate(loader), total=len(loader))\n",
    "        for i, (images, (target_sequences, sequence_lengths), (all_target_sequences, all_sequence_lengths)) in pbar:\n",
    "            images = images.to(device)\n",
    "            target_sequences = target_sequences.to(device)\n",
    "            sequence_lengths = sequence_lengths.to(device)\n",
    "            all_target_sequences = all_target_sequences.to(device)\n",
    "            all_sequence_lengths = all_sequence_lengths.to(device)\n",
    "            \n",
    "            k = beam_size\n",
    "            \n",
    "            # Encoding\n",
    "            image_features = model.encoder(images) # [1, 14, 14, hidden_size]\n",
    "            image_features = image_features.view(1, -1, model.encoder.hidden_size) # [1, num_pixels, enc_hidden_size]\n",
    "            \n",
    "            # Init hidden and memory states\n",
    "            mean_image_features = image_features.mean(dim=1) # [1, enc_hidden_size]\n",
    "            h_state, c_state = model.init_h0(mean_image_features), model.init_c0(mean_image_features) # [1, dec_hidden_size]\n",
    "            \n",
    "            # Extend image features, hidden and memory states\n",
    "            image_features = image_features.expand(k, -1, -1)  # [k, num_pixels, enc_hidden_size]\n",
    "            h_state = h_state.expand(k, -1) # [k, dec_hidden_size]\n",
    "            c_state = c_state.expand(k, -1) # [k, dec_hidden_size]\n",
    "            \n",
    "            # Top k previous words at each step; now they're just <sos>\n",
    "            topk_prev_tokens = torch.LongTensor([[field.vocab.stoi[field.init_token]]] * k).to(device)  # [k, 1]\n",
    "            \n",
    "            # Top k sequences; now they're just <sos>\n",
    "            topk_sequences = topk_prev_tokens  # [k, 1]\n",
    "            \n",
    "            # Top k sequences' log proba; now they're just 0\n",
    "            topk_logps = torch.zeros(k, 1).to(device)  # [k, 1]\n",
    "            \n",
    "            # Lists to store completed sequences and logps\n",
    "            complete_sequences, complete_sequence_logps = [], []\n",
    "            \n",
    "            # Decoding\n",
    "            step = 1\n",
    "            while True:\n",
    "                embedded = model.decoder.embedding(topk_prev_tokens.squeeze(1))  # [k, embedding_size]\n",
    "                context_vector, _ = model.decoder.attention(image_features, h_state)\n",
    "                # context_vector: Tensor[k, enc_hidden_size]\n",
    "                # _: Tensor[k, num_pixels]\n",
    "                gate = torch.sigmoid(model.decoder.f_beta(h_state))  # [k, enc_hidden_size], Gating scalar\n",
    "                context_vector = gate * context_vector # [k, enc_hidden_size]\n",
    "                x = torch.cat((embedded, context_vector), dim=1) # [k, embedding_size + enc_hidden_size]\n",
    "                h_state, c_state = h_state.unsqueeze(0).contiguous(), c_state.unsqueeze(0).contiguous()\n",
    "                output, (h_state, c_state) = model.decoder.lstm(x.unsqueeze(0), (h_state, c_state))\n",
    "                # output: [1, k, hidden_size]\n",
    "                # h_state: [1, k, hidden_size]\n",
    "                # c_state: [1, k, hidden_size]\n",
    "                logit = model.decoder.fc(output) # [1, k, vocab_size]\n",
    "                logps = F.log_softmax(logit.squeeze(0), dim=1) # [k, vocab_size]\n",
    "                \n",
    "                # Extend logp\n",
    "                logp = topk_logps.expand_as(logps) + logps  # [k, vocab_size]\n",
    "                \n",
    "                if step == 1:\n",
    "                    topk_logps, topk_tokens = logps[0].topk(k, 0, True, True)  # [k,]\n",
    "                else:\n",
    "                    # Unroll and find top scores, and their unrolled indices\n",
    "                    topk_logps, topk_tokens = logps.view(-1).topk(k, 0, True, True)  # [k,]\n",
    "                    \n",
    "                # Convert unrolled indices to actual indices of logps\n",
    "                prev_token_indices = topk_tokens / model.decoder.vocab_size  # [k,]\n",
    "                next_token_indices = topk_tokens % model.decoder.vocab_size  # [k,]\n",
    "                \n",
    "                # Add new tokens to top k sequences\n",
    "                topk_sequences = torch.cat((topk_sequences[prev_token_indices], next_token_indices.unsqueeze(1)), dim=1)  # [k, step + 1]\n",
    "                \n",
    "                # Get complete and incomplete sequences (didn't reach <eos>)?\n",
    "                incomplete_indices = [indice for indice, next_token in enumerate(next_token_indices) if next_token != field.vocab.stoi[field.eos_token]]\n",
    "                complete_indices = [*set(range(len(next_token_indices))) - set(incomplete_indices)]\n",
    "                \n",
    "                # Set aside complete sequences\n",
    "                if len(complete_indices) > 0:\n",
    "                    complete_sequences.extend(topk_sequences[complete_indices].tolist())\n",
    "                    complete_sequence_logps.extend(topk_logps[complete_indices])\n",
    "                \n",
    "                # Reduce beam size accordingly\n",
    "                k -= len(complete_indices)\n",
    "                \n",
    "                # Break if all sequences are complete\n",
    "                if k == 0:\n",
    "                    break\n",
    "                \n",
    "                # Proceed with incomplete sequences\n",
    "                topk_sequences = topk_sequences[incomplete_indices]\n",
    "                h_state = h_state[:, prev_token_indices[incomplete_indices]].squeeze(0).contiguous()\n",
    "                c_state = c_state[:, prev_token_indices[incomplete_indices]].squeeze(0).contiguous()\n",
    "                image_features = image_features[prev_token_indices[incomplete_indices]]\n",
    "                topk_logps = topk_logps[incomplete_indices].unsqueeze(1)\n",
    "                topk_prev_tokens = next_token_indices[incomplete_indices].unsqueeze(1)\n",
    "                \n",
    "                # Break if things have been going on too long\n",
    "                if step > max_len:\n",
    "                    complete_sequences.extend(topk_sequences[incomplete_indices].tolist())\n",
    "                    complete_sequence_logps.extend(topk_logps[incomplete_indices])\n",
    "                    break\n",
    "                    \n",
    "                # Increment step\n",
    "                step += 1\n",
    "                \n",
    "            i = complete_sequence_logps.index(max(complete_sequence_logps))\n",
    "            topk_sequences = complete_sequences[i]\n",
    "            \n",
    "            # Hypotheses\n",
    "            hypotheses.append([field.vocab.itos[w] for w in topk_sequences if w not in {field.vocab.stoi[field.init_token],\n",
    "                                                                                        field.vocab.stoi[field.eos_token],\n",
    "                                                                                        field.vocab.stoi[field.pad_token]}])\n",
    "            # Update references\n",
    "            for j in range(all_target_sequences.size(0)):\n",
    "                img_caps = all_target_sequences[j].t().tolist()\n",
    "                # Remove <sos> and <pad> tokens\n",
    "                img_caps = [*map(lambda c: [field.vocab.itos[w] for w in c if w not in {field.vocab.stoi[field.init_token],\n",
    "                                                                                        field.vocab.stoi[field.eos_token],\n",
    "                                                                                        field.vocab.stoi[field.pad_token]}], img_caps)]\n",
    "                references.append(img_caps)\n",
    "        \n",
    "        # Calculate BLEU-4 score\n",
    "        bleu4 = bleu_score(hypotheses, references, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "        # print(' '.join([field.vocab.itos[w] for w in topk_sequences]))\n",
    "        \n",
    "    return references, hypotheses, bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:32<00:00, 30.85it/s]\n"
     ]
    }
   ],
   "source": [
    "references, hypotheses, bleu4 = evaluate(model=autoencoder.to(DEVICE),\n",
    "                                         loader=test_loader,\n",
    "                                         beam_size=1,\n",
    "                                         field=EN,\n",
    "                                         max_len=MAX_LEN,\n",
    "                                         device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4: 6.674%\n"
     ]
    }
   ],
   "source": [
    "print(f'BLEU-4: {bleu4*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual =>  child on the beach while woman takes pictures nearby\n",
      "\tlittle girl in red at the beach while adult points camera\n",
      "\tman and girl are at the shoreline while the man takes picture and the girl <unk> doll\n",
      "\twoman is taking photograph on beach at the edge of the water while girl stands in the foreground\n",
      "\twoman taking pictures of the surf and child in the foreground\n",
      "Predicted =>  man in red shirt is standing in the snow\n",
      "==================================================\n",
      "Actual =>  family poses seated on concrete steps\n",
      "\tfamily sits for picture on staircase outside\n",
      "\tgroup of people are sitting on the steps outside\n",
      "\tgroup of people sit on the cement steps in the woods and smile\n",
      "\tthese five people are seated on steps outdoors\n",
      "Predicted =>  man in white shirt is standing on the sidewalk\n",
      "==================================================\n",
      "Actual =>  blonde boy in white and orange shirt is playing on swing\n",
      "\tboy in surfing shirt is swinging on swing\n",
      "\tchild is smiling while being on swing\n",
      "\tlittle boy swings at the playground\n",
      "\tyoung child swinging on swing\n",
      "Predicted =>  man in black shirt is standing in front of <unk>\n",
      "==================================================\n",
      "Actual =>  little girl is sitting on the counter dangling one foot in the <unk> whilst holding dish jet <unk>\n",
      "\tlittle girl plays with <unk>\n",
      "\tlittle girl wearing sunglasses puts her feet into the kitchen <unk>\n",
      "\tyoung barefoot girl in red sunglasses plays in <unk>\n",
      "\tyoung girl wearing red sunglasses sits near <unk> and holds the <unk>\n",
      "Predicted =>  man in red shirt is standing in front of <unk> <unk>\n",
      "==================================================\n",
      "Actual =>  football player in red and white is holding both hands up\n",
      "\tfootball players gather around the <unk>\n",
      "\tfootball players gathering to <unk> something to <unk> <unk>\n",
      "\toklahoma university football players in front of crowd of sooners fans and next to three <unk>\n",
      "\tthe red and white football team is talking with the <unk>\n",
      "Predicted =>  group of people are standing in front of <unk>\n",
      "==================================================\n",
      "Actual =>  child is playing ice hockey\n",
      "\tyoung hockey player playing in the ice rink\n",
      "\tyouth playing hockey\n",
      "\tthe hockey player is skating on the ice <unk> the puck\n",
      "\ttwo people are playing ice hockey\n",
      "Predicted =>  man in black shirt is standing on the sidewalk\n",
      "==================================================\n",
      "Actual =>  children participate in sport on green field while in uniforms\n",
      "\tthe young soccer player enjoys the game\n",
      "\ttwo girls in soccer <unk> are playing on sports field\n",
      "\ttwo girls play on fenced in field\n",
      "\ttwo girls wearing black and white uniforms run down grassy field\n",
      "Predicted =>  two boys are playing in field\n",
      "==================================================\n",
      "Actual =>  these men are smoking outside grey building\n",
      "\ttwo men are standing by railing while posing for picture\n",
      "\ttwo men in black jackets behind white handrail\n",
      "\ttwo men smoking behind railing\n",
      "\ttwo men smoking while standing by rail in front of building\n",
      "Predicted =>  man in white shirt is standing on the street\n",
      "==================================================\n",
      "Actual =>  black and blonde dog are either playing or fighting with each other\n",
      "\twhite and black dog fighting in fenced in room\n",
      "\ttwo dogs are jumping up at each other\n",
      "\ttwo dogs fighting one is black the other beige\n",
      "\ttwo dogs in <unk> on their hind legs facing one another\n",
      "Predicted =>  brown dog is running through the water\n",
      "==================================================\n",
      "Actual =>  man is sitting on the floor outside door and his head on his chin\n",
      "\tman sits against yellow wall wearing all black\n",
      "\tman wearing dark blue hat sits on the ground and leans against building\n",
      "\tman with black hat coat and pants sitting next to the door of building\n",
      "\tthe man in the black hat is sitting on the floor beside the green door\n",
      "Predicted =>  man in black shirt is standing on the sidewalk\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(10, 20):\n",
    "    print('Actual => ', '\\n\\t'.join([*map(' '.join, references[i])]))\n",
    "    print('Predicted => ', ' '.join(hypotheses[i]))\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
